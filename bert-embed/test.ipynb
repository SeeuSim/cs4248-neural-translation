{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seeusim/University/Y3/Y3S2/CS4248/Project/neural-translation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeniser\n",
    "\n",
    "We are required to:\n",
    "\n",
    "* Add special tokens to the start and end of each sentence.\n",
    "* Pad & truncate all sentences to a single constant length.\n",
    "* Explicitly differentiate real tokens from padding tokens with the “attention mask”.\n",
    "\n",
    "#### Special Tokens `[SEP]`\n",
    "\n",
    "At the end of every sentence, we need to append the special `[SEP]` token.\n",
    "\n",
    "This token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (e.g., can the answer to the question in sentence A be found in sentence B?).\n",
    "\n",
    "I am not certain yet why the token is still required when we have only single-sentence input, but it is!\n",
    "\n",
    "#### `[CLS]`\n",
    "\n",
    "For classification tasks, we must prepend the special [CLS] token to the beginning of every sentence.\n",
    "\n",
    "This token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output (but with the feature values changed, of course!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('../models/bert-embed/bert', output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture Explained\n",
    "\n",
    "#### Input \n",
    "\n",
    "As shown, it takes index mappings for the words and outputs an input layer of size 768. In this case, the tokeniser has a vocab size of 30522.\n",
    "\n",
    "(pls help)\n",
    "It also takes positional encodings of 300, meaning it can process sentences of length 300? \n",
    " \n",
    "It also takes token type embeddings of size 2 - giving 2 possible types of tokens (0 or 1). 0 means padding. In this case, our sentences are padded to a constant length of 300.\n",
    "\n",
    "#### Output\n",
    "\n",
    "It will output the token indices, which when fed to the tokeniser will return the tokens. Given that BERTTokenizer uses some form of WordPiece, the tokens with type 1 shld be joined with the previous ones, while tokens with type 0 are start tokens? May need to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(300, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"iwslt2017\", \"iwslt2017-en-zh\")\n",
    "\n",
    "train, test = dataset['train'], dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = train[0]['translation']['en']\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4067 thank\n",
      "2017 you\n",
      "2061 so\n",
      "2172 much\n",
      "1010 ,\n",
      "3782 chris\n",
      "1012 .\n",
      "1998 and\n",
      "2009 it\n",
      "1005 '\n",
      "1055 s\n",
      "5621 truly\n",
      "1037 a\n",
      "2307 great\n",
      "3932 honor\n",
      "2000 to\n",
      "2031 have\n",
      "1996 the\n",
      "4495 opportunity\n",
      "2000 to\n",
      "2272 come\n",
      "2000 to\n",
      "2023 this\n",
      "2754 stage\n",
      "3807 twice\n",
      "1025 ;\n",
      "1045 i\n",
      "1005 '\n",
      "1049 m\n",
      "5186 extremely\n",
      "8794 grateful\n",
      "1012 .\n"
     ]
    }
   ],
   "source": [
    "tokens = tokeniser.tokenize(sent)\n",
    "indices = tokeniser.convert_tokens_to_ids(tokens)\n",
    "for indice, token in zip(indices, tokens):\n",
    "    print(indice, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice_tensor = torch.tensor([indices])\n",
    "segment_tensor = torch.zeros(indice_tensor.shape) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 4067, 2017, 2061, 2172, 1010, 3782, 1012, 1998, 2009, 1005, 1055, 5621, 1037, 2307, 3932, 2000, 2031, 1996, 4495, 2000, 2272, 2000, 2023, 2754, 3807, 1025, 1045, 1005, 1049, 5186, 8794, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'special_tokens_mask': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser(sent, return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 34\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = tokeniser(sent)\n",
    "    input_ids = torch.tensor([tokens['input_ids']])\n",
    "    token_type_ids = torch.tensor([tokens['token_type_ids']])\n",
    "    attention_mask = torch.tensor([tokens['attention_mask']])\n",
    "\n",
    "    output = model(input_ids, token_type_ids, attention_mask)\n",
    "    hidden_states = output.hidden_states\n",
    "\n",
    "    print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "    layer_i = 0\n",
    "\n",
    "    print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "    batch_i = 0\n",
    "\n",
    "    print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "    token_i = 0\n",
    "\n",
    "    print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 34, 768])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 34, 768])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34, 13, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Append Last 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 34 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum Last 4 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 34 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 thank\n",
      "1 you\n",
      "2 so\n",
      "3 much\n",
      "4 ,\n",
      "5 chris\n",
      "6 .\n",
      "7 and\n",
      "8 it\n",
      "9 '\n",
      "10 s\n",
      "11 truly\n",
      "12 a\n",
      "13 great\n",
      "14 honor\n",
      "15 to\n",
      "16 have\n",
      "17 the\n",
      "18 opportunity\n",
      "19 to\n",
      "20 come\n",
      "21 to\n",
      "22 this\n",
      "23 stage\n",
      "24 twice\n",
      "25 ;\n",
      "26 i\n",
      "27 '\n",
      "28 m\n",
      "29 extremely\n",
      "30 grateful\n",
      "31 .\n"
     ]
    }
   ],
   "source": [
    "tokenised_text = tokeniser.tokenize(sent)\n",
    "\n",
    "for id, tok in enumerate(tokenised_text):\n",
    "    print(id, tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"to\".\n",
      "\n",
      "honor to have    tensor([-6.6135, -0.3495,  5.6792, -4.1039, -1.8736])\n",
      "opportunity to come   tensor([-0.7312, -4.5288,  4.5461,  0.7789, -8.9607])\n",
      "come to this    tensor([-0.5958, -5.9333,  1.3312, -7.3594, -5.6697])\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"to\".')\n",
    "print('')\n",
    "print(\"honor to have   \", str(token_vecs_sum[15][:5]))\n",
    "print(\"opportunity to come  \", str(token_vecs_sum[19][:5]))\n",
    "print(\"come to this   \", str(token_vecs_sum[21][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.48\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cosine Similarity\n",
    "\n",
    "May wish to test with other examples of ambiguous words ('river bank', 'bank robber')\n",
    "\"\"\"\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# for to\n",
    "same_to = 1 - cosine(token_vecs_sum[19], token_vecs_sum[21])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, tokeniser=None):\n",
    "        super().__init__()\n",
    "        if not tokeniser:\n",
    "            self.tokeniser = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "        else:\n",
    "            self.tokeniser = tokeniser\n",
    "        self.model = BertForMaskedLM.from_pretrained(\n",
    "            '../models/bert-embed/bert', \n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.model.eval()\n",
    "    def forward(self, sentence: str):\n",
    "        input_ids = torch.tensor([tokens['input_ids']])\n",
    "        token_type_ids = torch.tensor([tokens['token_type_ids']])\n",
    "        attention_mask = torch.tensor([tokens['attention_mask']])\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, attention_mask, token_type_ids)\n",
    "            hidden_states = output.hidden_states\n",
    "            token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "            token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "            token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "            \"\"\"\n",
    "            SENTENCE Vec\n",
    "            \"\"\"\n",
    "            token_vecs = hidden_states[-2][0]\n",
    "            # Calculate the average of all token vectors.\n",
    "            sentence_embedding_output = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "            \"\"\"\n",
    "            TOKEN SUM VEC\n",
    "            \"\"\"\n",
    "            token_vecs_sum = []\n",
    "            # `token_embeddings` is a [len x 12 x 768] tensor.\n",
    "            # For each token in the sentence...\n",
    "            for token in token_embeddings:\n",
    "                # `token` is a [12 x 768] tensor\n",
    "                # Sum the vectors from the last four layers.\n",
    "                sum_vec = torch.sum(token[-4:], dim=0)\n",
    "                # Use `sum_vec` to represent `token`.\n",
    "                token_vecs_sum.append(sum_vec)\n",
    "            token_embedding_output = torch.vstack(token_vecs_sum)\n",
    "            \n",
    "            return {\n",
    "                'Token': token_embedding_output,\n",
    "                'Sentence': sentence_embedding_output\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([34, 768]), torch.Size([768]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_bert = Embedding()\n",
    "\n",
    "output = custom_bert.forward(train[0]['translation']['en'])\n",
    "\n",
    "output['Token'].shape, output['Sentence'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16e05c8-679a-464f-b263-5562d7a2a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"iwslt2017\", \"iwslt2017-en-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21116d72-1fed-42d1-984f-5abda07c8f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We should have limited ourselves very, very strictly to the protection of the civilian population in Benghazi.\n"
     ]
    }
   ],
   "source": [
    "# To see the en text of a particular entry\n",
    "example = raw_datasets[\"train\"][123456][\"translation\"][\"en\"]\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74bce5cd-62fc-4aee-a801-86ce7e053505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator function to split dataset into batches\n",
    "def batch_iterator(batch_size=1000):\n",
    "    return (\n",
    "        [raw_datasets[\"train\"][i + j][\"translation\"][\"en\"] for j in range(batch_size) if i + j < len(raw_datasets[\"train\"])]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), batch_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73bab132-422d-4b68-b8ea-455b6c3c2ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_corpus = batch_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57498636-1a4c-4e60-bb92-bb8dad049e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise pretrained tokeniser\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "old_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c05f1c6-3030-43bf-8a1e-e5e51c3094e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train tokeniser on dataset\n",
    "tokenizer = old_tokenizer.train_new_from_iterator(training_corpus, 52000)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fef83bc7-5b7c-42b0-b015-6fe22fb5fb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gpt2_dataset_tokenizer/tokenizer_config.json',\n",
       " 'gpt2_dataset_tokenizer/special_tokens_map.json',\n",
       " 'gpt2_dataset_tokenizer/vocab.json',\n",
       " 'gpt2_dataset_tokenizer/merges.txt',\n",
       " 'gpt2_dataset_tokenizer/added_tokens.json',\n",
       " 'gpt2_dataset_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained tokeniser\n",
    "tokenizer.save_pretrained(\"gpt2_dataset_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92ee0847-fcd0-46dd-9b8c-d3eb94c68275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea89de188e741d6b2a908f5539ea8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0d1c453-2195-45ef-82ea-96a674a97424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/njcay/gpt2_dataset_tokenizer/commit/151bb176937a0de390354c0a0731d78c89717113', commit_message='Upload tokenizer', commit_description='', oid='151bb176937a0de390354c0a0731d78c89717113', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pushed to huggingface - use \"njcay/gpt2_dataset_tokenizer\"\n",
    "tokenizer.push_to_hub(\"gpt2_dataset_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f309dd21-881b-451c-9153-04b40c00236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'Ġshould', 'Ġhave', 'Ġlimited', 'Ġourselves', 'Ġvery', ',', 'Ġvery', 'Ġstrictly', 'Ġto', 'Ġthe', 'Ġprotection', 'Ġof', 'Ġthe', 'Ġcivilian', 'Ġpopulation', 'Ġin', 'ĠBenghazi', '.']\n",
      "['We', 'Ġshould', 'Ġhave', 'Ġlimited', 'Ġourselves', 'Ġvery', ',', 'Ġvery', 'Ġstrictly', 'Ġto', 'Ġthe', 'Ġprotection', 'Ġof', 'Ġthe', 'Ġcivilian', 'Ġpopulation', 'Ġin', 'ĠBen', 'ghazi', '.']\n"
     ]
    }
   ],
   "source": [
    "# Compare tokens\n",
    "old_tokens = old_tokenizer.tokenize(example)\n",
    "tokens = tokenizer.tokenize(example)\n",
    "print(old_tokens)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

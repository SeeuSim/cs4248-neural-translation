{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"iwslt2017\", \"iwslt2017-en-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "240694it [00:12, 19643.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab = set()\n",
    "for sent in tqdm(\n",
    "    chain.from_iterable([\n",
    "        dataset['train']['translation'], \n",
    "        dataset['test']['translation'], \n",
    "        dataset['validation']['translation']\n",
    "])):\n",
    "    words = set(filter(lambda tok:len(tok) > 0, word_tokenize(sent['en'])))\n",
    "    vocab |= words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab size, using NLTK\n",
    "\n",
    "We use this to determine the word index for the Tokeniser.\n",
    "\n",
    "The Tokeniser, when encoding, will output:\n",
    "  - index: int\n",
    "    The index of the word in the learned vocabulary.\n",
    "  - attention_mask: 1\n",
    "    The base attention mask (1) for the attention layer of the model.\n",
    "  - token_type: 0 | 1\n",
    "    If the token is a padding token, as all sentences are padded/truncated to a fixed context length\n",
    "\n",
    "The Tokeniser, when decoding, will take:\n",
    "  - index: int\n",
    "    The predicted token (index) that the model outputs.\n",
    "  - ...\n",
    "\n",
    "  Given these inputs, the Tokeniser should map the indices to the correct token in the learned vocabulary, and output the translated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72289\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seeusim/University/Y3/Y3S2/CS4248/Project/neural-translation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/seeusim/University/Y3/Y3S2/CS4248/Project/neural-translation/.venv/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:197: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-zh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = dataset['train']['translation'][0]['en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desired Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2547, 39, 194, 961, 2, 24873, 6, 309, 41, 23, 24, 9869, 13, 1330, 16042, 9, 55, 3, 1727, 9, 733, 9, 58, 3147, 10782, 27, 28, 23, 100, 7045, 5488, 6, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2547,    39,   194,   961,     2, 24873,     6,   309,    41,    23,\n",
       "            24,  9869,    13,  1330, 16042,     9,    55,     3,  1727,     9,\n",
       "           733,     9,    58,  3147, 10782,    27,    28,    23,   100,  7045,\n",
       "          5488,     6,     0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = tokenizer.encode(sent, return_tensors='pt')\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

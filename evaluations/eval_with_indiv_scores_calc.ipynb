{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this notebook with your predictions to generate your BLEU, CHRF, SacreBLEU, BERTScore scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import json\n",
    "import torch\n",
    "from bert_score import score\n",
    "from rouge_chinese import Rouge\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_FILE = '10k-rnn-baseline-spm-bi.json'\n",
    "REFERENCE_FILE = 'iwslt2017-en-zh-test.zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "reference = []\n",
    "\n",
    "with open(f'./predictions/{PREDICTION_FILE}', 'r', encoding='utf-8') as f:\n",
    "    raw = f.read()\n",
    "    pdict = json.loads(raw)\n",
    "    if \"predicted\" in pdict:\n",
    "        predictions = [*predictions, *pdict['predicted']]\n",
    "\n",
    "with open(f'../tokenisation/data/{REFERENCE_FILE}', 'r',encoding='utf-8') as f:\n",
    "    reference = [*reference, *(f.readlines())]\n",
    "\n",
    "assert len(predictions) == len(reference), \\\n",
    "    'Received a wrong number of predictions. ' + \\\n",
    "    'Ensure that you have generated predictions for the whole test set. \\n\\n' + \\\n",
    "    f'Predictions Length: {len(predictions)}, Expected: {len(reference)}' \n",
    "\n",
    "refs = [[r] for r in reference]\n",
    "sys = [str(p).rstrip() for p in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Score for BLEU , CHRF , CHRF++ , TER and mean for ROUGE and BERTSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('在 过 去 的 年 前 , , , , , , , , , ,   ⁇     ⁇     ⁇',\n",
       " ['几年前，在TED大会上， Peter Skillman 介绍了一个设计挑战 叫做“棉花糖挑战”\\n'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted , reference\n",
    "sys[0] , refs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU-4:\n",
      "BLEU = 20.61 73.7/33.3/11.8/6.2 (BP = 1.000 ratio = 1.000 hyp_len = 19 ref_len = 19)\n",
      "--------------------------------------------------\n",
      "CHRF\n",
      "chrF0 = 10.17\n",
      "--------------------------------------------------\n",
      "CHRF++\n",
      "chrF0++ = 12.50\n",
      "--------------------------------------------------\n",
      "TER\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TER = 55.87\n",
      "--------------------------------------------------\n",
      "Rouge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.677 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rouge-1\": {\n",
      "    \"r\": 0.07791557855036035,\n",
      "    \"p\": 0.21827346056100105,\n",
      "    \"f\": 0.10602943437603676\n",
      "  },\n",
      "  \"rouge-2\": {\n",
      "    \"r\": 0.001971258989469778,\n",
      "    \"p\": 0.0037221272248760826,\n",
      "    \"f\": 0.002327314834782995\n",
      "  },\n",
      "  \"rouge-l\": {\n",
      "    \"r\": 0.077905867277176,\n",
      "    \"p\": 0.09546040670909742,\n",
      "    \"f\": 0.08279716574836206\n",
      "  }\n",
      "}\n",
      "--------------------------------------------------\n",
      "BERT\n",
      "Precision: 0.5506570339202881 | Recall: 0.5108605027198792 | F1: 0.5291464328765869\n"
     ]
    }
   ],
   "source": [
    "# BLEU-4 (1/2/3/4)\n",
    "bleu = BLEU(smooth_method='exp', tokenize='zh', max_ngram_order=4) \n",
    "bleu = bleu.corpus_score(sys, refs)  \n",
    "print('BLEU-4:')\n",
    "print(bleu)\n",
    "print('-' * 50)\n",
    "\n",
    "# CHRF\n",
    "print(\"CHRF\")\n",
    "chrf = CHRF(word_order=0, beta=0, eps_smoothing=False)\n",
    "print(chrf.corpus_score(sys, refs))\n",
    "print(50*'-')\n",
    "\n",
    "# CHRF++\n",
    "print(\"CHRF++\")\n",
    "chrf = CHRF(word_order=2, beta=0, eps_smoothing=False)\n",
    "print(chrf.corpus_score(sys, refs))\n",
    "print(50*'-')\n",
    "\n",
    "# TER\n",
    "print(\"TER\")\n",
    "ter = TER(asian_support=True, normalized=True)\n",
    "print(ter.corpus_score(sys, refs))\n",
    "print(50*'-')\n",
    "\n",
    "# Rouge\n",
    "print(\"Rouge\")\n",
    "def get_tok(sent):\n",
    "    sent = str(sent)\n",
    "    return ' '.join(jieba.lcut(sent))\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(list(map(get_tok, predictions)), list(map(get_tok, reference)), avg=True)\n",
    "print(json.dumps(scores, indent=2))\n",
    "print(50*'-')\n",
    "\n",
    "# BERTScore\n",
    "print(\"BERT\")\n",
    "# run this if your strings got corrupted \n",
    "predictions = [str(p) for p in predictions]\n",
    "reference = [str(r) for r in reference]\n",
    "P, R, F1 = score(predictions, reference, lang='zh') # default model for zh is bert-base-chinese\n",
    "print(f'Precision: {P.mean().item()} | Recall: {R.mean().item()} | F1: {F1.mean().item()}') # Precision, Recall and F1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Individual Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [[r] for r in reference]\n",
    "sys = [str(p).rstrip() for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU\n",
    "bleu = BLEU(smooth_method='exp', tokenize='zh', max_ngram_order=4, effective_order=True)\n",
    "bleu_scores = [bleu.sentence_score(h, r).score for h, r in zip(sys, refs)]\n",
    "\n",
    "# CHRF\n",
    "chrf = CHRF(word_order=0, beta=0, eps_smoothing=False)\n",
    "chrf_scores = [chrf.sentence_score(h, r).score for h, r in zip(sys, refs)]\n",
    "\n",
    "# CHRF++\n",
    "chrf_plus = CHRF(word_order=2, beta=0, eps_smoothing=False)\n",
    "chrf_plus_scores = [chrf_plus.sentence_score(h, r).score for h, r in zip(sys, refs)]\n",
    "\n",
    "# TER\n",
    "ter = TER(asian_support=True, normalized=True)\n",
    "ter_scores = [ter.sentence_score(h, r).score for h, r in zip(sys, refs)]\n",
    "\n",
    "# ROUGE \n",
    "rouge = Rouge()\n",
    "r_scores = rouge.get_scores(list(map(get_tok, predictions)), list(map(get_tok, reference)), avg=False)\n",
    "\n",
    "# BERTSCORE \n",
    "P, R, F1 = score(predictions, reference, lang='zh')\n",
    "precision_scores = P.tolist()\n",
    "recall_scores = R.tolist()\n",
    "f1_scores = F1.tolist()\n",
    "\n",
    "\n",
    "scores = {\n",
    "    'BLEU': bleu_scores,\n",
    "    'CHRF': chrf_scores,\n",
    "    'CHRF++': chrf_plus_scores,\n",
    "    'TER': ter_scores,\n",
    "    'ROUGE':r_scores,\n",
    "    'BERTSCORE_P' : precision_scores,\n",
    "    'BERTSCORE_R' : recall_scores,\n",
    "    'BERTSCORE_F1' : f1_scores\n",
    "}\n",
    "\n",
    "with open(f'individual_scores\\{PREDICTION_FILE[:-5]}.json', 'w') as f:\n",
    "    json.dump(scores, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['几年前，在TED大会上， Peter Skillman 介绍了一个设计挑战 叫做“棉花糖挑战”\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this notebook with your predictions to generate your BLEU, CHRF, SacreBLEU, BERTScore scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import json\n",
    "import torch\n",
    "from bert_score import score\n",
    "from rouge_chinese import Rouge\n",
    "from sacrebleu.metrics import BLEU, CHRF, TER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_FILE = '10k-rnn-baseline-ss-bi.json' # 10k-rnn-baseline-spacy-jieba-bi.json\n",
    "REFERENCE_FILE = 'iwslt2017-en-zh-test.zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "reference = []\n",
    "\n",
    "with open(f'./predictions/{PREDICTION_FILE}', 'r', encoding='utf-8') as f:\n",
    "    raw = f.read()\n",
    "    pdict = json.loads(raw)\n",
    "    if \"predicted\" in pdict:\n",
    "        predictions = [*predictions, *pdict['predicted']]\n",
    "\n",
    "with open(f'../tokenisation/data/{REFERENCE_FILE}', 'r',encoding='utf-8') as f:\n",
    "    reference = [*reference, *(f.readlines())]\n",
    "\n",
    "assert len(predictions) == len(reference), \\\n",
    "    'Received a wrong number of predictions. ' + \\\n",
    "    'Ensure that you have generated predictions for the whole test set. \\n\\n' + \\\n",
    "    f'Predictions Length: {len(predictions)}, Expected: {len(reference)}' \n",
    "\n",
    "refs = [[r.rstrip()] for r in reference]\n",
    "sys = [str(p).rstrip() for p in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average on test set for BLEU , CHRF , CHRF++ , TER , ROUGE and BERTSCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>在年前，我的了一个我的一个项目，我的一个的的的的的的的的的的。',\n",
       " ['几年前，在TED大会上， Peter Skillman 介绍了一个设计挑战 叫做“棉花糖挑战”'])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted , reference\n",
    "sys[0] , refs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Individual Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [[r] for r in reference]\n",
    "sys = [str(p).rstrip() for p in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLEU\n",
    "bleu = BLEU(smooth_method='exp', tokenize='zh', max_ngram_order=4, effective_order=True)\n",
    "bleu_scores = [bleu.sentence_score(h, r).score for h, r in zip(sys, refs)]\n",
    "\n",
    "# CHRF\n",
    "chrf = CHRF(word_order=0, beta=0, eps_smoothing=False)\n",
    "chrf_scores = [chrf.sentence_score(h, r).score for h, r in zip(sys, refs)]\n",
    "\n",
    "# CHRF++\n",
    "chrf_plus = CHRF(word_order=2, beta=0, eps_smoothing=False)\n",
    "chrf_plus_scores = [chrf_plus.sentence_score(h, r).score for h, r in zip(sys, refs)]\n",
    "\n",
    "# TER\n",
    "ter = TER(asian_support=True, normalized=True)\n",
    "ter_scores = [ter.sentence_score(h, r).score for h, r in zip(sys, refs)]\n",
    "\n",
    "# ROUGE \n",
    "rouge = Rouge()\n",
    "def get_tok(sent):\n",
    "    sent = str(sent)\n",
    "    return ' '.join(jieba.lcut(sent))\n",
    "r_scores = rouge.get_scores(list(map(get_tok, predictions)), list(map(get_tok, reference)), avg=False)\n",
    "\n",
    "# BERTSCORE \n",
    "P, R, F1 = score(predictions, reference, lang='zh')\n",
    "precision_scores = P.tolist()\n",
    "recall_scores = R.tolist()\n",
    "f1_scores = F1.tolist()\n",
    "\n",
    "\n",
    "scores = {\n",
    "    'BLEU': bleu_scores,\n",
    "    'CHRF': chrf_scores,\n",
    "    'CHRF++': chrf_plus_scores,\n",
    "    'TER': ter_scores,\n",
    "    'ROUGE':r_scores,\n",
    "    'BERTSCORE_P' : precision_scores,\n",
    "    'BERTSCORE_R' : recall_scores,\n",
    "    'BERTSCORE_F1' : f1_scores\n",
    "}\n",
    "\n",
    "with open(f'individual_scores\\{PREDICTION_FILE[:-5]}.json', 'w') as f:\n",
    "    json.dump(scores, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['几年前，在TED大会上， Peter Skillman 介绍了一个设计挑战 叫做“棉花糖挑战”\\n']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU-4:\n",
      "3.449183455164379\n",
      "--------------------------------------------------\n",
      "CHRF\n",
      "3.9452189255904924\n",
      "--------------------------------------------------\n",
      "CHRF++\n",
      "3.104803878479188\n",
      "--------------------------------------------------\n",
      "TER\n",
      "107.91505345039711\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('Average BLEU-4:')\n",
    "print(sum(bleu_scores) / len(refs))\n",
    "print('-' * 50)\n",
    "\n",
    "# CHRF\n",
    "print(\"CHRF\")\n",
    "print(sum(chrf_scores) / len(refs))\n",
    "print(50*'-')\n",
    "\n",
    "# CHRF++\n",
    "print(\"CHRF++\")\n",
    "print(sum(chrf_plus_scores) / len(refs))\n",
    "print(50*'-')\n",
    "\n",
    "# TER\n",
    "print(\"TER\")\n",
    "print(sum(ter_scores) / len(refs))\n",
    "print(50*'-')\n",
    "\n",
    "# Rouge\n",
    "print(\"Rouge\")\n",
    "def get_tok(sent):\n",
    "    sent = str(sent)\n",
    "    return ' '.join(jieba.lcut(sent))\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(list(map(get_tok, predictions)), list(map(get_tok, reference)), avg=True)\n",
    "print(json.dumps(scores, indent=2))\n",
    "print(50*'-')\n",
    "\n",
    "# BERTScore\n",
    "print(\"BERT\")\n",
    "# run this if your strings got corrupted \n",
    "predictions = [str(p) for p in predictions]\n",
    "reference = [str(r) for r in reference]\n",
    "P, R, F1 = score(predictions, reference, lang='zh') # default model for zh is bert-base-chinese\n",
    "print(f'Precision: {P.mean().item()} | Recall: {R.mean().item()} | F1: {F1.mean().item()}') # Precision, Recall and F1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

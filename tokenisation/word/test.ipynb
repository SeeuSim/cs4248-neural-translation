{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stanza torch torchtext spacy[cuda]\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "stanza.download('zh', processors='tokenize')\n",
    "nlp = stanza.Pipeline(lang='zh', processors='tokenize')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Vocab and Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'en'\n",
    "TGT_LANGUAGE = 'zh'\n",
    "\n",
    "MAX_SEQ_LEN = 288\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, BOS_IDX, EOS_IDX, PAD_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<s>', '</s>', '<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}\n",
    "\n",
    "def tokenise_chinese(sent):\n",
    "    return [word.text for sentence in nlp(sent).sentences for word in sentence.words]\n",
    "\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_lg')\n",
    "token_transform[TGT_LANGUAGE] = get_tokenizer(tokenise_chinese)\n",
    "\n",
    "for lang in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
    "    vocab_transform[lang] = torch.load(f'./word-{lang}.vocab')\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform_src(token_ids: list[int]):\n",
    "    return torch.cat((\n",
    "        torch.tensor(token_ids[:MAX_SEQ_LEN]),              \n",
    "        torch.tensor([PAD_IDX] * (MAX_SEQ_LEN - len(token_ids)))\n",
    "    )).to(DEVICE)\n",
    "\n",
    "def tensor_transform_tgt_inp(token_ids: list[int]):\n",
    "    return torch.cat((\n",
    "        torch.tensor([BOS_IDX]),\n",
    "        torch.tensor(token_ids[:MAX_SEQ_LEN - 1]),\n",
    "        torch.tensor([PAD_IDX] * (MAX_SEQ_LEN - len(token_ids) - 1)),\n",
    "    )).to(DEVICE)\n",
    "\n",
    "def tensor_transform_tgt_out(token_ids: list[int]):\n",
    "    return torch.cat((\n",
    "        torch.tensor(token_ids[:MAX_SEQ_LEN - 1]),\n",
    "        torch.tensor([EOS_IDX]),\n",
    "        torch.tensor([PAD_IDX] * (MAX_SEQ_LEN - len(token_ids) - 1)),\n",
    "    )).to(DEVICE)\n",
    "\n",
    "\n",
    "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "text_transform[SRC_LANGUAGE] = sequential_transforms(\n",
    "    token_transform[SRC_LANGUAGE], #Tokenization\n",
    "    vocab_transform[SRC_LANGUAGE], #Numericalization\n",
    "    tensor_transform_src # Add BOS/EOS and create tensor\n",
    ")\n",
    "\n",
    "text_transform[TGT_LANGUAGE] = (\n",
    "    sequential_transforms(\n",
    "        token_transform[SRC_LANGUAGE], #Tokenization\n",
    "        vocab_transform[SRC_LANGUAGE], #Numericalization\n",
    "        tensor_transform_tgt_inp, # Add BOS/EOS and create tensor\n",
    "    ),\n",
    "    sequential_transforms(\n",
    "        token_transform[SRC_LANGUAGE], #Tokenization\n",
    "        vocab_transform[SRC_LANGUAGE], #Numericalization\n",
    "        tensor_transform_tgt_out, # Add BOS/EOS and create tensor\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tensors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_inp_batch, tgt_out_batch = [], [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.strip()))\n",
    "        tgt_inp_batch.append(text_transform[TGT_LANGUAGE][0](tgt_sample.strip()))\n",
    "        tgt_out_batch.append(text_transform[TGT_LANGUAGE][1](tgt_sample.strip()))\n",
    "\n",
    "    return src_batch, tgt_inp_batch, tgt_out_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Loading of data for batching during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 80\n",
    "DATA_DIR = '../data/iwslt2017-en-zh-{file}.{lang}'\n",
    "\n",
    "en_train = DATA_DIR.format(file='train', lang='en')\n",
    "zh_train = DATA_DIR.format(file='train', lang='zh')\n",
    "en_validation = DATA_DIR.format(file='validation', lang='en')\n",
    "zh_validation = DATA_DIR.format(file='validation', lang='zh')\n",
    "\n",
    "with open(en_train, 'r') as f1, open(zh_train, 'r') as f2:\n",
    "    src = f1.readlines()\n",
    "    tgt = f2.readlines()\n",
    "    train_iter = zip(src, tgt)\n",
    "with open(en_validation, 'r') as f1, open(zh_validation, 'r') as f2:\n",
    "    src = f1.readlines()\n",
    "    tgt = f2.readlines()\n",
    "    val_iter = zip(src, tgt)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, pin_memory=False)\n",
    "val_loader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn, pin_memory=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

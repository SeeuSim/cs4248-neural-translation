{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:08.006782Z","iopub.status.busy":"2024-04-14T01:49:08.006492Z","iopub.status.idle":"2024-04-14T01:49:21.070523Z","shell.execute_reply":"2024-04-14T01:49:21.069445Z","shell.execute_reply.started":"2024-04-14T01:49:08.006755Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.66.1)\n","Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.39.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.22.2)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\n","Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.28.0)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"]}],"source":["!pip install numpy sentencepiece transformers[torch] tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:21.073093Z","iopub.status.busy":"2024-04-14T01:49:21.072651Z","iopub.status.idle":"2024-04-14T01:49:27.847253Z","shell.execute_reply":"2024-04-14T01:49:27.846423Z","shell.execute_reply.started":"2024-04-14T01:49:21.073057Z"},"trusted":true},"outputs":[],"source":["import datetime\n","import math\n","import sys, os\n","\n","import numpy as np\n","import sentencepiece as spm\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","from transformers import BatchEncoding, BertForMaskedLM"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:27.848668Z","iopub.status.busy":"2024-04-14T01:49:27.848297Z","iopub.status.idle":"2024-04-14T01:49:27.909490Z","shell.execute_reply":"2024-04-14T01:49:27.908400Z","shell.execute_reply.started":"2024-04-14T01:49:27.848643Z"},"trusted":true},"outputs":[],"source":["class SentencePieceBPETokeniser(object):\n","    PAD_ID = 3  # Defined as sentencepiece custom token\n","\n","    def __init__(self, lang: str, model_file=None):\n","        self.model = spm.SentencePieceProcessor(\n","            model_file=model_file or f\"./{lang}.model\"\n","        )\n","        self.special_ids = (\n","            self.model.unk_id(),\n","            SentencePieceBPETokeniser.PAD_ID,  # self.model.pad_id(), # this is -1 and may give errors.\n","            self.model.bos_id(),\n","            self.model.eos_id(),\n","        )\n","\n","    def __len__(self):\n","        return len(self.model)\n","\n","    def encode_batch(self, sents: list[str], max_len=None):\n","        return [self.encode(sent, max_len) for sent in sents]\n","\n","    def encode(self, sent: str | list[str], max_len=None):\n","        if type(sent) == list:\n","            return self.encode_batch(sent, max_len)\n","        ids = self.model.encode(sent)\n","        return ids\n","\n","    def decode(self, ids: list[int]):\n","        return self.model.decode(\n","            list(\n","                filter(\n","                    lambda id: id >= 0\n","                    and id < len(self),\n","                    ids\n","                )\n","            )\n","        )\n","\n","    def decode_batch(self, ids: list[list[int]]):\n","        return [self.decode(id) for id in ids]\n","\n","    def get_special_ids(self):\n","        UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = self.special_ids\n","        return (UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX)\n","\n","en_tokeniser = SentencePieceBPETokeniser('en', model_file='/kaggle/input/spiecebpeunproc/other/base-spiece/1/en.model')\n","zh_tokeniser = SentencePieceBPETokeniser('zh', model_file='/kaggle/input/spiecebpeunproc/other/base-spiece/1/zh.model')"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:27.912011Z","iopub.status.busy":"2024-04-14T01:49:27.911690Z","iopub.status.idle":"2024-04-14T01:49:35.536923Z","shell.execute_reply":"2024-04-14T01:49:35.535713Z","shell.execute_reply.started":"2024-04-14T01:49:27.911986Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n","tar: Ignoring unknown extended header keyword 'LIBARCHIVE.xattr.com.apple.quarantine'\n"]}],"source":["!mkdir en\n","!mkdir zh\n","\n","!tar xf /kaggle/input/embed-brt/other/en/1/BPE-bert-en.tar -C ./en\n","!tar xf /kaggle/input/embed-brt/other/zh/1/BPE-bert-zh.tar -C ./zh"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:35.539283Z","iopub.status.busy":"2024-04-14T01:49:35.538913Z","iopub.status.idle":"2024-04-14T01:49:36.169044Z","shell.execute_reply":"2024-04-14T01:49:36.168224Z","shell.execute_reply.started":"2024-04-14T01:49:35.539252Z"},"trusted":true},"outputs":[],"source":["class BPEBertTokeniser:\n","    out_keys = [\n","        \"input_ids\",\n","        \"token_type_ids\",\n","        \"attention_mask\",\n","        \"special_tokens_mask\",\n","    ]\n","\n","    def __init__(self, lang):\n","        self.lang = lang\n","        self.pad_token_id = 3\n","\n","    def _process_id(self, input_ids):\n","        \"\"\"\n","        Called inside the model.\n","        \"\"\"\n","        # src_i: EOS, # trg_i: BOS, #trg_o: EOS\n","        # input_ids = torch.tensor(input_ids) # 288, padded\n","        token_type_ids = torch.zeros(input_ids.shape)\n","\n","        attention_mask = torch.where(input_ids == self.pad_token_id, 0, 1)\n","\n","        special_tokens_mask = torch.where(input_ids < 4, 1, 0)\n","\n","        return {\n","            \"input_ids\": input_ids,\n","            \"token_type_ids\": token_type_ids,\n","            \"attention_mask\": attention_mask,\n","            \"special_tokens_mask\": special_tokens_mask,\n","        }\n","\n","    def encode(self, row):\n","        if type(row) != torch.Tensor:\n","            row = torch.tensor(row)\n","        if len(row.shape) == 1:\n","            return self.encode_batch(row.unsqueeze(0))\n","        return self.encode_batch(row)\n","\n","    def encode_batch(self, rows):\n","        ids = list(map(lambda row: self._process_id(row), rows))\n","        return {key: torch.vstack([example[key] for example in ids]) for key in BPEBertTokeniser.out_keys}\n","\n","\n","    def __call__(self, inputs, **_kwargs):\n","        return self.encode(inputs)\n","\n","    def __len__(self):\n","        return len(self.model)\n","\n","\n","\n","DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","\n","class BPEBertEmbedder:\n","    def __init__(self, lang: str, model_file: str):\n","        # Init the model from the pretrained weights\n","        self.model = BertForMaskedLM.from_pretrained(\n","            model_file, output_hidden_states=True\n","        ).to(DEVICE)\n","        self.model.eval()\n","\n","        # Init the BPE tokeniser (padded to length 288, vocab 16384)\n","        self.tokeniser = BPEBertTokeniser(lang)\n","        pass\n","\n","    def embed(self, sent: list[int]):\n","        tokens = self.tokeniser(\n","            sent\n","        )  # input_ids, token_type_mask, attention_mask, special_tokens_mask\n","        with torch.no_grad():\n","            input_ids = tokens[\"input_ids\"].to(DEVICE)\n","            token_type_ids = tokens[\"token_type_ids\"].to(DEVICE)\n","            attention_mask = tokens[\"attention_mask\"].to(DEVICE)\n","            # Perform inference\n","            output = self.model(input_ids, token_type_ids, attention_mask)\n","            hidden_states = output.hidden_states\n","\n","            # Permutate and obtain hidden states\n","        token_embeddings = torch.stack(hidden_states, dim=0)  # 13, batch, 288, 256\n","\n","        token_embeddings = token_embeddings.permute(1, 2, 0, 3)  # (batch, 288, n_layers, 256)\n","\n","        # Take last 4 layers\n","        token_embeddings = token_embeddings[:, :, -4:, :]\n","\n","        # Take sum of last 4 layers\n","        token_embeddings = token_embeddings.sum(axis=2)  # (batch, 288, 256)\n","\n","        token_embeddings.requires_grad_(False)\n","\n","        # Matrix of embeddings of dim 256, one per word\n","        return token_embeddings\n","\n","\n","en_embedder = BPEBertEmbedder('en', '/kaggle/working/en')\n","zh_embedder = BPEBertEmbedder('zh', '/kaggle/working/zh')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:36.170541Z","iopub.status.busy":"2024-04-14T01:49:36.170194Z","iopub.status.idle":"2024-04-14T01:49:36.178514Z","shell.execute_reply":"2024-04-14T01:49:36.177409Z","shell.execute_reply.started":"2024-04-14T01:49:36.170508Z"},"trusted":true},"outputs":[],"source":["# Path or parameters for data\n","RSC_DIR = ''\n","\n","TRAIN_NAME = 'iwslt2017-en-zh-train'\n","\n","VALID_NAME = 'iwslt2017-en-zh-validation'\n","\n","# For resuming from checkpoint\n","CHKPT_NAME = 'ckpt-msk3-20.tar' # The key within the checkpoint directory\n","\n","SRC_VOCAB_SIZE = 16384\n","TGT_VOCAB_SIZE = 16384\n","OUTPUT_VOCAB_SIZE = TGT_VOCAB_SIZE\n","\n","PAD_IDX = 16384\n","\n","# Parameters for Transformer & training\n","DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","LEARNING_RATE = 1e-4\n","BATCH_SIZE = 90\n","NUM_HEADS = 8\n","NUM_LAYERS = 6\n","D_MODEL = 256\n","D_FF = 1024\n","D_K = D_MODEL // NUM_HEADS\n","SEQUENCE_LENGTH=288\n","\n","DROP_OUT_RATE = 0.1\n","NUM_EPOCHS = 20\n","BEAM_SIZE = 8\n","\n","# Resuming training from saved checkpoint\n","CHECKPOINT_DIR = f'/kaggle/input/trf-bert/other/epch-20/1'"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:36.180094Z","iopub.status.busy":"2024-04-14T01:49:36.179728Z","iopub.status.idle":"2024-04-14T01:49:36.193764Z","shell.execute_reply":"2024-04-14T01:49:36.192987Z","shell.execute_reply.started":"2024-04-14T01:49:36.180062Z"},"trusted":true},"outputs":[],"source":["def pad_or_truncate(tokenized_text, pad_idx, max_len=SEQUENCE_LENGTH):\n","    if len(tokenized_text) < max_len:\n","        left = max_len - len(tokenized_text)\n","        padding = [pad_idx] * left\n","        tokenized_text += padding\n","    else:\n","        tokenized_text = tokenized_text[:max_len]\n","\n","    return tokenized_text\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:36.195454Z","iopub.status.busy":"2024-04-14T01:49:36.195111Z","iopub.status.idle":"2024-04-14T01:49:36.239134Z","shell.execute_reply":"2024-04-14T01:49:36.238322Z","shell.execute_reply.started":"2024-04-14T01:49:36.195425Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layer_norm_1 = LayerNormalization()\n","        self.multihead_attention = MultiheadAttention()\n","        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n","\n","        self.layer_norm_2 = LayerNormalization()\n","        self.feed_forward = FeedFowardLayer()\n","        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n","\n","    def forward(self, x, e_mask):\n","        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n","        x = x + self.drop_out_1(\n","            self.multihead_attention(x_1, x_1, x_1, mask=e_mask)\n","        ) # (B, L, d_model)\n","        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n","        x = x + self.drop_out_2(self.feed_forward(x_2)) # (B, L, d_model)\n","\n","        return x # (B, L, d_model)\n","\n","\n","class DecoderLayer(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layer_norm_1 = LayerNormalization()\n","        self.masked_multihead_attention = MultiheadAttention()\n","        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n","\n","        self.layer_norm_2 = LayerNormalization()\n","        self.multihead_attention = MultiheadAttention()\n","        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n","\n","        self.layer_norm_3 = LayerNormalization()\n","        self.feed_forward = FeedFowardLayer()\n","        self.drop_out_3 = nn.Dropout(DROP_OUT_RATE)\n","\n","    def forward(self, x, e_output, e_mask,  d_mask):\n","        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n","        x = x + self.drop_out_1(\n","            self.masked_multihead_attention(x_1, x_1, x_1, mask=d_mask)\n","        ) # (B, L, d_model)\n","        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n","        x = x + self.drop_out_2(\n","            self.multihead_attention(x_2, e_output, e_output, mask=e_mask)\n","        ) # (B, L, d_model)\n","        x_3 = self.layer_norm_3(x) # (B, L, d_model)\n","        x = x + self.drop_out_3(self.feed_forward(x_3)) # (B, L, d_model)\n","\n","        return x # (B, L, d_model)\n","\n","\n","class MultiheadAttention(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.inf = 1e9\n","\n","        # W^Q, W^K, W^V in the paper\n","        self.w_q = nn.Linear(D_MODEL, D_MODEL)\n","        self.w_k = nn.Linear(D_MODEL, D_MODEL)\n","        self.w_v = nn.Linear(D_MODEL, D_MODEL)\n","\n","        self.dropout = nn.Dropout(DROP_OUT_RATE)\n","        self.attn_softmax = nn.Softmax(dim=-1)\n","\n","        # Final output linear transformation\n","        self.w_0 = nn.Linear(D_MODEL, D_MODEL)\n","\n","    def forward(self, q, k, v, mask=None):\n","        input_shape = q.shape\n","\n","        # Linear calculation +  split into num_heads\n","        q = self.w_q(q).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n","        k = self.w_k(k).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n","        v = self.w_v(v).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n","\n","        # For convenience, convert all tensors in size (B, num_heads, L, d_k)\n","        q = q.transpose(1, 2)\n","        k = k.transpose(1, 2)\n","        v = v.transpose(1, 2)\n","\n","        # Conduct self-attention\n","        attn_values = self.self_attention(q, k, v, mask=mask) # (B, num_heads, L, d_k)\n","        concat_output = attn_values.transpose(1, 2)\\\n","            .contiguous().view(input_shape[0], -1, D_MODEL) # (B, L, d_model)\n","\n","        return self.w_0(concat_output)\n","\n","    def self_attention(self, q, k, v, mask=None):\n","        # Calculate attention scores with scaled dot-product attention\n","        attn_scores = torch.matmul(q, k.transpose(-2, -1)) # (B, num_heads, L, L)\n","        attn_scores = attn_scores / math.sqrt(D_K)\n","\n","        # If there is a mask, make masked spots -INF\n","        if mask is not None:\n","            mask = mask.unsqueeze(1) # (B, 1, L) => (B, 1, 1, L) or (B, L, L) => (B, 1, L, L)\n","            attn_scores = attn_scores.masked_fill_(mask == 0, -1 * self.inf)\n","\n","        # Softmax and multiplying K to calculate attention value\n","        attn_distribs = self.attn_softmax(attn_scores)\n","\n","        attn_distribs = self.dropout(attn_distribs)\n","        attn_values = torch.matmul(attn_distribs, v) # (B, num_heads, L, d_k)\n","\n","        return attn_values\n","\n","\n","class FeedFowardLayer(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear_1 = nn.Linear(D_MODEL, D_FF, bias=True)\n","        self.relu = nn.ReLU()\n","        self.linear_2 = nn.Linear(D_FF, D_MODEL, bias=True)\n","        self.dropout = nn.Dropout(DROP_OUT_RATE)\n","\n","    def forward(self, x):\n","        x = self.relu(self.linear_1(x)) # (B, L, d_ff)\n","        x = self.dropout(x)\n","        x = self.linear_2(x) # (B, L, d_model)\n","\n","        return x\n","\n","\n","class LayerNormalization(nn.Module):\n","    def __init__(self, eps=1e-6):\n","        super().__init__()\n","        self.eps = eps\n","        self.layer = nn.LayerNorm([D_MODEL], elementwise_affine=True, eps=self.eps)\n","\n","    def forward(self, x):\n","        x = self.layer(x)\n","\n","        return x\n","\n","\n","class PositionalEncoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # Make initial positional encoding matrix with 0\n","        pe_matrix= torch.zeros(SEQUENCE_LENGTH, D_MODEL) # (L, d_model)\n","\n","        # Calculating position encoding values\n","        for pos in range(SEQUENCE_LENGTH):\n","            for i in range(D_MODEL):\n","                if i % 2 == 0:\n","                    pe_matrix[pos, i] = math.sin(pos / (10000 ** (2 * i / D_MODEL)))\n","                elif i % 2 == 1:\n","                    pe_matrix[pos, i] = math.cos(pos / (10000 ** (2 * i / D_MODEL)))\n","\n","        pe_matrix = pe_matrix.unsqueeze(0) # (1, L, d_model)\n","        self.positional_encoding = pe_matrix.to(device=DEVICE).requires_grad_(False)\n","\n","    def forward(self, x):\n","        x = x * math.sqrt(D_MODEL) # (B, L, d_model)\n","        x = x + self.positional_encoding # (B, L, d_model)\n","\n","        return x\n","\n","class Transformer(nn.Module):\n","    def __init__(self, src_vocab_size, trg_vocab_size):\n","        super().__init__()\n","        self.src_vocab_size = src_vocab_size\n","        self.trg_vocab_size = trg_vocab_size\n","\n","        self.src_embedding = en_embedder\n","        self.trg_embedding = zh_embedder\n","        # self.src_embedding = nn.Embedding(self.src_vocab_size, D_MODEL)\n","        # self.trg_embedding = nn.Embedding(self.trg_vocab_size, D_MODEL)\n","        self.positional_encoder = PositionalEncoder()\n","        self.encoder = Encoder()\n","        self.decoder = Decoder()\n","        self.output_linear = nn.Linear(D_MODEL, self.trg_vocab_size)\n","        self.softmax = nn.LogSoftmax(dim=-1)\n","\n","    def forward(self, src_input, trg_input, e_mask=None, d_mask=None):\n","        src_input = self.src_embedding.embed(src_input)\n","        trg_input = self.trg_embedding.embed(trg_input)\n","\n","        src_input = self.positional_encoder(src_input) # (B, L, d_model) => (B, L, d_model)\n","        trg_input = self.positional_encoder(trg_input) # (B, L, d_model) => (B, L, d_model)\n","\n","        e_output = self.encoder(src_input, e_mask) # (B, L, d_model)\n","        d_output = self.decoder(trg_input, e_output, e_mask, d_mask) # (B, L, d_model)\n","\n","        output = self.softmax(self.output_linear(d_output)) # (B, L, d_model) => # (B, L, trg_vocab_size)\n","\n","        return output\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layers = nn.ModuleList([EncoderLayer() for i in range(NUM_LAYERS)])\n","        self.layer_norm = LayerNormalization()\n","\n","    def forward(self, x, e_mask):\n","        for i in range(NUM_LAYERS):\n","            x = self.layers[i](x, e_mask)\n","\n","        return self.layer_norm(x)\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.layers = nn.ModuleList([DecoderLayer() for i in range(NUM_LAYERS)])\n","        self.layer_norm = LayerNormalization()\n","\n","    def forward(self, x, e_output, e_mask, d_mask):\n","        for i in range(NUM_LAYERS):\n","            x = self.layers[i](x, e_output, e_mask, d_mask)\n","\n","        return self.layer_norm(x)\n","    \n","\n","import heapq\n","\n","\n","class BeamNode():\n","    def __init__(self, cur_idx, prob, decoded):\n","        self.cur_idx = cur_idx\n","        self.prob = prob\n","        self.decoded = decoded\n","        self.is_finished = False\n","        \n","    def __gt__(self, other):\n","        return self.prob > other.prob\n","    \n","    def __ge__(self, other):\n","        return self.prob >= other.prob\n","    \n","    def __lt__(self, other):\n","        return self.prob < other.prob\n","    \n","    def __le__(self, other):\n","        return self.prob <= other.prob\n","    \n","    def __eq__(self, other):\n","        return self.prob == other.prob\n","    \n","    def __ne__(self, other):\n","        return self.prob != other.prob\n","    \n","    def print_spec(self):\n","        print(f\"ID: {self} || cur_idx: {self.cur_idx} || prob: {self.prob} || decoded: {self.decoded}\")\n","    \n","\n","class PriorityQueue():\n","    def __init__(self):\n","        self.queue = []\n","        \n","    def put(self, obj):\n","        heapq.heappush(self.queue, (obj.prob, obj))\n","        \n","    def get(self):\n","        return heapq.heappop(self.queue)[1]\n","    \n","    def qsize(self):\n","        return len(self.queue)\n","    \n","    def print_scores(self):\n","        scores = [t[0] for t in self.queue]\n","        print(scores)\n","        \n","    def print_objs(self):\n","        objs = [t[1] for t in self.queue]\n","        print(objs)\n","    "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:36.240566Z","iopub.status.busy":"2024-04-14T01:49:36.240292Z","iopub.status.idle":"2024-04-14T01:49:36.282085Z","shell.execute_reply":"2024-04-14T01:49:36.281270Z","shell.execute_reply.started":"2024-04-14T01:49:36.240543Z"},"trusted":true},"outputs":[],"source":["import copy\n","\n","BEAM_SIZE = 8\n","\n","class Manager():\n","    def __init__(self, is_train=True, ckpt_name=None):\n","\n","        # Load Transformer model & Adam optimizer\n","        print(\"Loading Transformer model & Adam optimizer...\")\n","        self.model = Transformer(src_vocab_size=SRC_VOCAB_SIZE, trg_vocab_size=TGT_VOCAB_SIZE).to(DEVICE)\n","        self.optim = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n","        self.best_loss = sys.float_info.max\n","\n","        if ckpt_name is not None:\n","            assert os.path.exists(f\"{CHECKPOINT_DIR}/{ckpt_name}\"), f\"There is no checkpoint named {ckpt_name}.\"\n","\n","            print(\"Loading checkpoint...\")\n","            checkpoint = torch.load(f\"{CHECKPOINT_DIR}/{ckpt_name}\", map_location=DEVICE)\n","            self.model.load_state_dict(checkpoint['model_state_dict'])\n","            self.optim.load_state_dict(checkpoint['optim_state_dict'])\n","            self.best_loss = checkpoint['loss']\n","        else:\n","            print(\"Initializing the model...\")\n","            for p in self.model.parameters():\n","                if p.dim() > 1:\n","                    nn.init.xavier_uniform_(p)\n","\n","        if is_train:\n","            # Load loss function\n","            print(\"Loading loss function...\")\n","            self.criterion = nn.NLLLoss()\n","\n","            # Load dataloaders\n","            print(\"Loading dataloaders...\")\n","            self.train_loader = get_data_loader(TRAIN_NAME)\n","            self.valid_loader = get_data_loader(VALID_NAME)\n","\n","        print(\"Setting finished.\")\n","\n","    def train(self):\n","        print(\"Training starts.\")\n","        stag_ct = 0\n","        for epoch in range(1, NUM_EPOCHS+1):\n","            self.model.train()\n","\n","            train_losses = []\n","            start_time = datetime.datetime.now()\n","\n","            for i, batch in tqdm(enumerate(self.train_loader)):\n","                src_input, trg_input, trg_output = batch\n","                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n","\n","                e_mask, d_mask = self.make_mask(src_input, trg_input)\n","\n","                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n","\n","                trg_output_shape = trg_output.shape\n","                self.optim.zero_grad()\n","                loss = self.criterion(\n","                    output.view(-1, OUTPUT_VOCAB_SIZE),\n","                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n","                )\n","\n","                loss.backward()\n","                self.optim.step()\n","\n","                train_losses.append(loss.item())\n","\n","                del src_input, trg_input, trg_output, e_mask, d_mask, output\n","                torch.cuda.empty_cache()\n","\n","            end_time = datetime.datetime.now()\n","            training_time = end_time - start_time\n","            seconds = training_time.seconds\n","            hours = seconds // 3600\n","            minutes = (seconds % 3600) // 60\n","            seconds = seconds % 60\n","\n","            mean_train_loss = np.mean(train_losses)\n","            print(f\"#################### Epoch: {epoch} ####################\")\n","            print(f\"Train loss: {mean_train_loss} || One epoch training time: {hours}hrs {minutes}mins {seconds}secs\")\n","\n","            valid_loss, valid_time = self.validation()\n","\n","            if valid_loss < self.best_loss:\n","                if not os.path.exists(CHECKPOINT_DIR):\n","                    os.mkdir(CHECKPOINT_DIR)\n","\n","                self.best_loss = valid_loss\n","                state_dict = {\n","                    'model_state_dict': self.model.state_dict(),\n","                    'optim_state_dict': self.optim.state_dict(),\n","                    'loss': self.best_loss\n","                }\n","                torch.save(state_dict, f\"{CHECKPOINT_DIR}/ckpt-{epoch}.tar\")\n","                print(f\"***** Current best checkpoint is saved. *****\")\n","                stag_ct = 0\n","            else:\n","                stag_ct += 1\n","\n","\n","            print(f\"Best valid loss: {self.best_loss}\")\n","            print(f\"Valid loss: {valid_loss} || One epoch training time: {valid_time}\")\n","\n","            if stag_ct >= 3:\n","                print(\"Has not improved for 3 epochs. Stopping training.\")\n","                break\n","\n","        print(f\"Training finished!\")\n","\n","    def validation(self):\n","        print(\"Validation processing...\")\n","        self.model.eval()\n","\n","        valid_losses = []\n","        start_time = datetime.datetime.now()\n","\n","        with torch.no_grad():\n","            for i, batch in tqdm(enumerate(self.valid_loader)):\n","                src_input, trg_input, trg_output = batch\n","                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n","\n","                e_mask, d_mask = self.make_mask(src_input, trg_input)\n","\n","                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n","\n","                trg_output_shape = trg_output.shape\n","                loss = self.criterion(\n","                    output.view(-1, OUTPUT_VOCAB_SIZE),\n","                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n","                )\n","\n","                valid_losses.append(loss.item())\n","\n","                del src_input, trg_input, trg_output, e_mask, d_mask, output\n","                torch.cuda.empty_cache()\n","\n","        end_time = datetime.datetime.now()\n","        validation_time = end_time - start_time\n","        seconds = validation_time.seconds\n","        hours = seconds // 3600\n","        minutes = (seconds % 3600) // 60\n","        seconds = seconds % 60\n","\n","        mean_valid_loss = np.mean(valid_losses)\n","\n","        return mean_valid_loss, f\"{hours}hrs {minutes}mins {seconds}secs\"\n","\n","    def make_mask(self, src_input, trg_input):\n","        # src_input: (B, L, d_model)\n","        e_mask = (src_input != en_tokeniser.PAD_ID).unsqueeze(1)  # (B, 1, L)\n","        d_mask = (trg_input != zh_tokeniser.PAD_ID).unsqueeze(1)  # (B, 1, L)\n","\n","        nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool)  # (1, L, L)\n","        nopeak_mask = torch.tril(nopeak_mask).to(DEVICE)  # (1, L, L) to triangular shape\n","        d_mask = d_mask & nopeak_mask  # (B, L, L) padding false\n","\n","        return e_mask, d_mask\n","\n","    def beam_search(self, e_output, e_mask):\n","        _, TGT_PAD_IDX, TGT_BOS_IDX, TGT_EOS_IDX = zh_tokeniser.get_special_ids()\n","        cur_queue = PriorityQueue()\n","        \n","        for _ in range(BEAM_SIZE):\n","            cur_queue.put(BeamNode(TGT_BOS_IDX, -0.0, [TGT_BOS_IDX]))\n","        \n","        finished_count = 0\n","\n","        SEQUENCE_LENGTH = 288\n","        \n","        for pos in range(SEQUENCE_LENGTH):\n","            new_queue = PriorityQueue()\n","            for k in range(BEAM_SIZE):\n","                node = cur_queue.get()\n","                if node.is_finished:\n","                    new_queue.put(node)\n","                else:\n","                    trg_input = torch.LongTensor(node.decoded + [TGT_PAD_IDX] * (SEQUENCE_LENGTH - len(node.decoded))).to(DEVICE) # (L)\n","                    d_mask = (trg_input.unsqueeze(0) != TGT_PAD_IDX).unsqueeze(1).to(DEVICE) # (1, 1, L)\n","                    nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool).to(DEVICE)\n","                    nopeak_mask = torch.tril(nopeak_mask) # (1, L, L) to triangular shape\n","                    d_mask = d_mask & nopeak_mask # (1, L, L) padding false\n","                    \n","                    trg_embedded = self.model.trg_embedding.embed(trg_input.unsqueeze(0))\n","                    trg_positional_encoded = self.model.positional_encoder(trg_embedded)\n","                    decoder_output = self.model.decoder(\n","                        trg_positional_encoded,\n","                        e_output,\n","                        e_mask,\n","                        d_mask\n","                    ) # (1, L, d_model)\n","\n","                    output = self.model.softmax(\n","                        self.model.output_linear(decoder_output)\n","                    ) # (1, L, trg_vocab_size)\n","                    \n","                    output = torch.topk(output[0][pos], dim=-1, k=BEAM_SIZE)\n","                    last_word_ids = output.indices.tolist() # (k)\n","                    last_word_prob = output.values.tolist() # (k)\n","                    \n","                    for i, idx in enumerate(last_word_ids):\n","                        new_node = BeamNode(idx, -(-node.prob + last_word_prob[i]), node.decoded + [idx])\n","                        if idx == TGT_EOS_IDX:\n","                            new_node.prob = new_node.prob / float(len(new_node.decoded))\n","                            new_node.is_finished = True\n","                            finished_count += 1\n","                        new_queue.put(new_node)\n","            \n","            cur_queue = copy.deepcopy(new_queue)\n","            \n","            if finished_count == BEAM_SIZE:\n","                break\n","        \n","        decoded_output = cur_queue.get().decoded\n","        \n","        if decoded_output[-1] == TGT_EOS_IDX:\n","            decoded_output = decoded_output[1:-1]\n","        else:\n","            decoded_output = decoded_output[1:]\n","            \n","        return zh_tokeniser.decode(decoded_output)\n","\n","    def translate(self, text: str, verbose=False):\n","        _, SRC_PAD, _, SRC_EOS = en_tokeniser.get_special_ids()\n","        tokenized = en_tokeniser.encode(text)\n","        src_data = torch.LongTensor(\n","            pad_or_truncate(tokenized + [SRC_EOS], SRC_PAD)\n","        ).unsqueeze(0).to(DEVICE) # (1, L)\n","        e_mask = (src_data != SRC_PAD).unsqueeze(1).to(DEVICE) # (1, 1, L)\n","\n","        start_time = datetime.datetime.now()\n","\n","        if verbose:\n","            print(\"Encoding input sentence...\")\n","        src_data = self.model.src_embedding.embed(src_data)\n","        src_data = self.model.positional_encoder(src_data)\n","        e_output = self.model.encoder(src_data, e_mask) # (1, L, d_model)\n","\n","        \n","        result = self.beam_search(e_output, e_mask)\n","\n","        end_time = datetime.datetime.now()\n","\n","        total_inference_time = end_time - start_time\n","        seconds = total_inference_time.seconds\n","        minutes = seconds // 60\n","        seconds = seconds % 60\n","\n","        if verbose:\n","            print(f\"Input: {text}\")\n","            print(f\"Result: {result}\")\n","            print(f\"Inference finished! || Total inference time: {minutes}mins {seconds}secs\")\n","\n","        return result"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T01:49:36.285325Z","iopub.status.busy":"2024-04-14T01:49:36.284960Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading Transformer model & Adam optimizer...\n","Loading checkpoint...\n","Setting finished.\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 11/8549 [01:05<11:39:46,  4.92s/it]"]}],"source":["import json\n","from tqdm import tqdm\n","\n","m = Manager(is_train=False, ckpt_name=CHKPT_NAME)\n","\n","with open('/kaggle/input/iwslt2017-en-zh/iwslt2017-en-zh-test.en', 'r') as in_file, \\\n","        open('/kaggle/working/pred.json', 'w') as out_file:\n","    p = []\n","    for l in tqdm(in_file.readlines()):\n","        p.append(m.translate(l.strip()))\n","    json.dump({ 'predicted': p }, out_file, indent=2)"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4742513,"sourceId":8043496,"sourceType":"datasetVersion"},{"isSourceIdPinned":true,"modelInstanceId":22150,"sourceId":26323,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":25820,"sourceId":30761,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":25822,"sourceId":30764,"sourceType":"modelInstanceVersion"},{"isSourceIdPinned":true,"modelInstanceId":25823,"sourceId":30765,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}

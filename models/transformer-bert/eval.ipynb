{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1mUnRtItsyi_"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import math\n",
        "import sys, os\n",
        "\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import BertForMaskedLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "4CgtW9oNBDF_"
      },
      "outputs": [],
      "source": [
        "en_model_path = '../../tokenisation/sentencepiece_custom/en.model'\n",
        "zh_model_path = '../../tokenisation/sentencepiece_custom/zh.model'\n",
        "\n",
        "en_ds_test_path = '../../tokenisation/data/iwslt2017-en-zh-test.en'\n",
        "zh_ds_test_path = '../../tokenisation/data/iwslt2017-en-zh-test.zh'\n",
        "\n",
        "bert_en_path = '../../models/bert-embed/BPE-bert-en'\n",
        "bert_zh_path = '../../models/bert-embed/BPE-bert-zh'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Co97KqSsvkV4"
      },
      "outputs": [],
      "source": [
        "class SentencePieceBPETokeniser(object):\n",
        "    PAD_ID = 3  # Defined as sentencepiece custom token\n",
        "\n",
        "    def __init__(self, lang: str, model_file=None):\n",
        "        self.model = spm.SentencePieceProcessor(\n",
        "            model_file=model_file or f\"./{lang}.model\"\n",
        "        )\n",
        "        self.special_ids = (\n",
        "            self.model.unk_id(),\n",
        "            SentencePieceBPETokeniser.PAD_ID,  # self.model.pad_id(), # this is -1 and may give errors.\n",
        "            self.model.bos_id(),\n",
        "            self.model.eos_id(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.model)\n",
        "\n",
        "    def encode_batch(self, sents: list[str], max_len=None):\n",
        "        return [self.encode(sent, max_len) for sent in sents]\n",
        "\n",
        "    def encode(self, sent: str | list[str], max_len=None):\n",
        "        if type(sent) == list:\n",
        "            return self.encode_batch(sent, max_len)\n",
        "        ids = self.model.encode(sent)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: list[int]):\n",
        "        return self.model.decode(\n",
        "            list(\n",
        "                filter(\n",
        "                    lambda id: id >= 0\n",
        "                    and id < len(self),\n",
        "                    ids\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def decode_batch(self, ids: list[list[int]]):\n",
        "        return [self.decode(id) for id in ids]\n",
        "\n",
        "    def get_special_ids(self):\n",
        "        UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = self.special_ids\n",
        "        return (UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX)\n",
        "\n",
        "en_tokeniser = SentencePieceBPETokeniser('en', model_file=en_model_path)\n",
        "zh_tokeniser = SentencePieceBPETokeniser('zh', model_file=zh_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "myw2imuPsQx2"
      },
      "outputs": [],
      "source": [
        "class BPEBertTokeniser:\n",
        "    out_keys = [\n",
        "        \"input_ids\",\n",
        "        \"token_type_ids\",\n",
        "        \"attention_mask\",\n",
        "        \"special_tokens_mask\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.pad_token_id = 3\n",
        "\n",
        "    def _process_id(self, input_ids):\n",
        "        \"\"\"\n",
        "        Called inside the model.\n",
        "        \"\"\"\n",
        "        # src_i: EOS, # trg_i: BOS, #trg_o: EOS\n",
        "        # input_ids = torch.tensor(input_ids) # 288, padded\n",
        "        token_type_ids = torch.zeros(input_ids.shape)\n",
        "\n",
        "        attention_mask = torch.where(input_ids == self.pad_token_id, 0, 1)\n",
        "\n",
        "        special_tokens_mask = torch.where(input_ids < 4, 1, 0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"token_type_ids\": token_type_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"special_tokens_mask\": special_tokens_mask,\n",
        "        }\n",
        "\n",
        "    def encode(self, row):\n",
        "        if type(row) != torch.Tensor:\n",
        "            row = torch.tensor(row)\n",
        "        if len(row.shape) == 1:\n",
        "            return self.encode_batch(row.unsqueeze(0))\n",
        "        return self.encode_batch(row)\n",
        "\n",
        "    def encode_batch(self, rows):\n",
        "        ids = list(map(lambda row: self._process_id(row), rows))\n",
        "        return {key: torch.vstack([example[key] for example in ids]) for key in BPEBertTokeniser.out_keys}\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, **_kwargs):\n",
        "        return self.encode(inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.model)\n",
        "\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "class BPEBertEmbedder:\n",
        "    def __init__(self, lang: str, model_file: str):\n",
        "        # Init the model from the pretrained weights\n",
        "        self.model = BertForMaskedLM.from_pretrained(\n",
        "            model_file, output_hidden_states=True\n",
        "        ).to(DEVICE)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Init the BPE tokeniser (padded to length 288, vocab 16384)\n",
        "        self.tokeniser = BPEBertTokeniser(lang)\n",
        "        pass\n",
        "\n",
        "    def embed(self, sent: list[int]):\n",
        "        tokens = self.tokeniser(\n",
        "            sent\n",
        "        )  # input_ids, token_type_mask, attention_mask, special_tokens_mask\n",
        "        with torch.no_grad():\n",
        "            input_ids = tokens[\"input_ids\"].to(DEVICE)\n",
        "            token_type_ids = tokens[\"token_type_ids\"].to(DEVICE)\n",
        "            attention_mask = tokens[\"attention_mask\"].to(DEVICE)\n",
        "            # Perform inference\n",
        "            output = self.model(input_ids, token_type_ids, attention_mask)\n",
        "            hidden_states = output.hidden_states\n",
        "\n",
        "            # Permutate and obtain hidden states\n",
        "        token_embeddings = torch.stack(hidden_states, dim=0)  # 13, batch, 288, 256\n",
        "\n",
        "        token_embeddings = token_embeddings.permute(1, 2, 0, 3)  # (batch, 288, n_layers, 256)\n",
        "\n",
        "        # Take last 4 layers\n",
        "        token_embeddings = token_embeddings[:, :, -4:, :]\n",
        "\n",
        "        # Take sum of last 4 layers\n",
        "        token_embeddings = token_embeddings.sum(axis=2)  # (batch, 288, 256)\n",
        "\n",
        "        token_embeddings.requires_grad_(False)\n",
        "\n",
        "        # Matrix of embeddings of dim 256, one per word\n",
        "        return token_embeddings\n",
        "\n",
        "\n",
        "en_embedder = BPEBertEmbedder('en', bert_en_path)\n",
        "zh_embedder = BPEBertEmbedder('zh', bert_zh_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "biMm2ZdasbMS"
      },
      "outputs": [],
      "source": [
        "# Path or parameters for data\n",
        "RSC_DIR = None\n",
        "\n",
        "TRAIN_NAME = 'iwslt2017-en-zh-train'\n",
        "\n",
        "VALID_NAME = 'iwslt2017-en-zh-validation'\n",
        "\n",
        "# For resuming from checkpoint\n",
        "CHKPT_NAME = 'ckpt-msk3-18.tar' # The key within the checkpoint directory\n",
        "\n",
        "SRC_VOCAB_SIZE = 16384\n",
        "TGT_VOCAB_SIZE = 16384\n",
        "OUTPUT_VOCAB_SIZE = TGT_VOCAB_SIZE\n",
        "\n",
        "PAD_IDX = 16384\n",
        "\n",
        "# Parameters for Transformer & training\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 90\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 6\n",
        "D_MODEL = 256\n",
        "D_FF = 1024\n",
        "D_K = D_MODEL // NUM_HEADS\n",
        "\n",
        "DROP_OUT_RATE = 0.1\n",
        "NUM_EPOCHS = 10\n",
        "BEAM_SIZE = 8\n",
        "\n",
        "# Resuming training from saved checkpoint\n",
        "CHECKPOINT_DIR = f'.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NvcyzimZshRW"
      },
      "outputs": [],
      "source": [
        "SEQUENCE_LENGTH = 288\n",
        "\n",
        "\n",
        "def get_data_loader(file_name):\n",
        "    print(f\"Getting source/target {file_name}...\")\n",
        "    with open(f\"{RSC_DIR}/{file_name}.en\", 'r') as f:\n",
        "        src_text_list = f.readlines()\n",
        "\n",
        "    with open(f\"{RSC_DIR}/{file_name}.zh\", 'r') as f:\n",
        "        trg_text_list = f.readlines()\n",
        "\n",
        "    print(\"Tokenizing & Padding src data...\")\n",
        "    src_list = process_src(src_text_list) # (sample_num, L)\n",
        "    print(f\"The shape of src data: {np.shape(src_list)}\")\n",
        "\n",
        "    print(\"Tokenizing & Padding trg data...\")\n",
        "    input_trg_list, output_trg_list = process_trg(trg_text_list) # (sample_num, L)\n",
        "    print(f\"The shape of input trg data: {np.shape(input_trg_list)}\")\n",
        "    print(f\"The shape of output trg data: {np.shape(output_trg_list)}\")\n",
        "\n",
        "    dataset = CustomDataset(src_list, input_trg_list, output_trg_list)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def pad_or_truncate(tokenized_text, pad_idx, max_len=SEQUENCE_LENGTH):\n",
        "    if len(tokenized_text) < max_len:\n",
        "        left = max_len - len(tokenized_text)\n",
        "        padding = [pad_idx] * left\n",
        "        tokenized_text += padding\n",
        "    else:\n",
        "        tokenized_text = tokenized_text[:max_len]\n",
        "\n",
        "    return tokenized_text\n",
        "\n",
        "MAX_SRC_LEN = 0\n",
        "MAX_TGT_LEN = 0\n",
        "\n",
        "MAX_SIZE=300_000\n",
        "\n",
        "def process_src(text_list):\n",
        "    \"\"\"\n",
        "    Add EOS to end, but only ignore pad\n",
        "    -> Add BOS, pad start idx and pad end + 1\n",
        "    \"\"\"\n",
        "    _, PAD_IDX, _, EOS_IDX = en_tokeniser.get_special_ids()\n",
        "    src_input_ids = []\n",
        "    for text in tqdm(text_list[:MAX_SIZE]):\n",
        "        ids = en_tokeniser.encode(text.strip())\n",
        "        src_input_ids.append(pad_or_truncate([PAD_IDX] + ids + [EOS_IDX], PAD_IDX))\n",
        "    return src_input_ids\n",
        "\n",
        "def process_trg(text_list):\n",
        "    \"\"\"\n",
        "    Add BOS to start, ignore pad\n",
        "    -> Add EOS, pad end - 1\n",
        "    Add EOS to end, ignore pad\n",
        "    -> Add BOS, pad start idx and pad end + 1\n",
        "    \"\"\"\n",
        "    _, PAD_IDX, BOS_IDX, EOS_IDX = zh_tokeniser.get_special_ids()\n",
        "    input_tokenized_list = []\n",
        "    output_tokenized_list = []\n",
        "    for text in tqdm(text_list[:MAX_SIZE]):\n",
        "        tokenized = zh_tokeniser.encode(text.strip())\n",
        "        trg_input = [BOS_IDX] + tokenized\n",
        "        trg_output = [PAD_IDX] + tokenized + [EOS_IDX]\n",
        "        input_tokenized_list.append(pad_or_truncate(trg_input, PAD_IDX))\n",
        "        output_tokenized_list.append(pad_or_truncate(trg_output, PAD_IDX))\n",
        "    return input_tokenized_list, output_tokenized_list\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, src_list, input_trg_list, output_trg_list):\n",
        "        super().__init__()\n",
        "        self.src_data = torch.LongTensor(src_list)\n",
        "        self.input_trg_data = torch.LongTensor(input_trg_list)\n",
        "        self.output_trg_data = torch.LongTensor(output_trg_list)\n",
        "\n",
        "        assert np.shape(src_list) == np.shape(input_trg_list), f\"The shape of src_list and input_trg_list are different: {np.shape(src_list)} {np.shape(input_trg_list)}\"\n",
        "        assert np.shape(input_trg_list) == np.shape(output_trg_list), f\"The shape of input_trg_list and output_trg_list are different: {np.shape(input_trg_list)} {np.shape(output_trg_list)}\"\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.input_trg_data[idx], self.output_trg_data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.src_data)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0CE_XX_vsiTs"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = LayerNormalization()\n",
        "        self.multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_2 = LayerNormalization()\n",
        "        self.feed_forward = FeedFowardLayer()\n",
        "        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x, e_mask):\n",
        "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_1(\n",
        "            self.multihead_attention(x_1, x_1, x_1, mask=e_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_2(self.feed_forward(x_2)) # (B, L, d_model)\n",
        "\n",
        "        return x # (B, L, d_model)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = LayerNormalization()\n",
        "        self.masked_multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_2 = LayerNormalization()\n",
        "        self.multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_3 = LayerNormalization()\n",
        "        self.feed_forward = FeedFowardLayer()\n",
        "        self.drop_out_3 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x, e_output, e_mask,  d_mask):\n",
        "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_1(\n",
        "            self.masked_multihead_attention(x_1, x_1, x_1, mask=d_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_2(\n",
        "            self.multihead_attention(x_2, e_output, e_output, mask=e_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_3 = self.layer_norm_3(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_3(self.feed_forward(x_3)) # (B, L, d_model)\n",
        "\n",
        "        return x # (B, L, d_model)\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inf = 1e9\n",
        "\n",
        "        # W^Q, W^K, W^V in the paper\n",
        "        self.w_q = nn.Linear(D_MODEL, D_MODEL)\n",
        "        self.w_k = nn.Linear(D_MODEL, D_MODEL)\n",
        "        self.w_v = nn.Linear(D_MODEL, D_MODEL)\n",
        "\n",
        "        self.dropout = nn.Dropout(DROP_OUT_RATE)\n",
        "        self.attn_softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Final output linear transformation\n",
        "        self.w_0 = nn.Linear(D_MODEL, D_MODEL)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        input_shape = q.shape\n",
        "\n",
        "        # Linear calculation +  split into num_heads\n",
        "        q = self.w_q(q).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "        k = self.w_k(k).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "        v = self.w_v(v).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "\n",
        "        # For convenience, convert all tensors in size (B, num_heads, L, d_k)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Conduct self-attention\n",
        "        attn_values = self.self_attention(q, k, v, mask=mask) # (B, num_heads, L, d_k)\n",
        "        concat_output = attn_values.transpose(1, 2)\\\n",
        "            .contiguous().view(input_shape[0], -1, D_MODEL) # (B, L, d_model)\n",
        "\n",
        "        return self.w_0(concat_output)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask=None):\n",
        "        # Calculate attention scores with scaled dot-product attention\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) # (B, num_heads, L, L)\n",
        "        attn_scores = attn_scores / math.sqrt(D_K)\n",
        "\n",
        "        # If there is a mask, make masked spots -INF\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1) # (B, 1, L) => (B, 1, 1, L) or (B, L, L) => (B, 1, L, L)\n",
        "            attn_scores = attn_scores.masked_fill_(mask == 0, -1 * self.inf)\n",
        "\n",
        "        # Softmax and multiplying K to calculate attention value\n",
        "        attn_distribs = self.attn_softmax(attn_scores)\n",
        "\n",
        "        attn_distribs = self.dropout(attn_distribs)\n",
        "        attn_values = torch.matmul(attn_distribs, v) # (B, num_heads, L, d_k)\n",
        "\n",
        "        return attn_values\n",
        "\n",
        "\n",
        "class FeedFowardLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(D_MODEL, D_FF, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(D_FF, D_MODEL, bias=True)\n",
        "        self.dropout = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear_1(x)) # (B, L, d_ff)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_2(x) # (B, L, d_model)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.layer = nn.LayerNorm([D_MODEL], elementwise_affine=True, eps=self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Make initial positional encoding matrix with 0\n",
        "        pe_matrix= torch.zeros(SEQUENCE_LENGTH, D_MODEL) # (L, d_model)\n",
        "\n",
        "        # Calculating position encoding values\n",
        "        for pos in range(SEQUENCE_LENGTH):\n",
        "            for i in range(D_MODEL):\n",
        "                if i % 2 == 0:\n",
        "                    pe_matrix[pos, i] = math.sin(pos / (10000 ** (2 * i / D_MODEL)))\n",
        "                elif i % 2 == 1:\n",
        "                    pe_matrix[pos, i] = math.cos(pos / (10000 ** (2 * i / D_MODEL)))\n",
        "\n",
        "        pe_matrix = pe_matrix.unsqueeze(0) # (1, L, d_model)\n",
        "        self.positional_encoding = pe_matrix.to(device=DEVICE).requires_grad_(False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * math.sqrt(D_MODEL) # (B, L, d_model)\n",
        "        x = x + self.positional_encoding # (B, L, d_model)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "66OApnHysm3Q"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
        "        super().__init__()\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.trg_vocab_size = trg_vocab_size\n",
        "\n",
        "        self.src_embedding = en_embedder\n",
        "        self.trg_embedding = zh_embedder\n",
        "        # self.src_embedding = nn.Embedding(self.src_vocab_size, D_MODEL)\n",
        "        # self.trg_embedding = nn.Embedding(self.trg_vocab_size, D_MODEL)\n",
        "        self.positional_encoder = PositionalEncoder()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.output_linear = nn.Linear(D_MODEL, self.trg_vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, src_input, trg_input, e_mask=None, d_mask=None):\n",
        "        src_input = self.src_embedding.embed(src_input)\n",
        "        trg_input = self.trg_embedding.embed(trg_input)\n",
        "\n",
        "        src_input = self.positional_encoder(src_input) # (B, L, d_model) => (B, L, d_model)\n",
        "        trg_input = self.positional_encoder(trg_input) # (B, L, d_model) => (B, L, d_model)\n",
        "\n",
        "        e_output = self.encoder(src_input, e_mask) # (B, L, d_model)\n",
        "        d_output = self.decoder(trg_input, e_output, e_mask, d_mask) # (B, L, d_model)\n",
        "\n",
        "        output = self.softmax(self.output_linear(d_output)) # (B, L, d_model) => # (B, L, trg_vocab_size)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for i in range(NUM_LAYERS)])\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, e_mask):\n",
        "        for i in range(NUM_LAYERS):\n",
        "            x = self.layers[i](x, e_mask)\n",
        "\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for i in range(NUM_LAYERS)])\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, e_output, e_mask, d_mask):\n",
        "        for i in range(NUM_LAYERS):\n",
        "            x = self.layers[i](x, e_output, e_mask, d_mask)\n",
        "\n",
        "        return self.layer_norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "\n",
        "class BeamNode():\n",
        "    def __init__(self, cur_idx, prob, decoded):\n",
        "        self.cur_idx = cur_idx\n",
        "        self.prob = prob\n",
        "        self.decoded = decoded\n",
        "        self.is_finished = False\n",
        "        \n",
        "    def __gt__(self, other):\n",
        "        return self.prob > other.prob\n",
        "    \n",
        "    def __ge__(self, other):\n",
        "        return self.prob >= other.prob\n",
        "    \n",
        "    def __lt__(self, other):\n",
        "        return self.prob < other.prob\n",
        "    \n",
        "    def __le__(self, other):\n",
        "        return self.prob <= other.prob\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return self.prob == other.prob\n",
        "    \n",
        "    def __ne__(self, other):\n",
        "        return self.prob != other.prob\n",
        "    \n",
        "    def print_spec(self):\n",
        "        print(f\"ID: {self} || cur_idx: {self.cur_idx} || prob: {self.prob} || decoded: {self.decoded}\")\n",
        "    \n",
        "\n",
        "class PriorityQueue():\n",
        "    def __init__(self):\n",
        "        self.queue = []\n",
        "        \n",
        "    def put(self, obj):\n",
        "        heapq.heappush(self.queue, (obj.prob, obj))\n",
        "        \n",
        "    def get(self):\n",
        "        return heapq.heappop(self.queue)[1]\n",
        "    \n",
        "    def qsize(self):\n",
        "        return len(self.queue)\n",
        "    \n",
        "    def print_scores(self):\n",
        "        scores = [t[0] for t in self.queue]\n",
        "        print(scores)\n",
        "        \n",
        "    def print_objs(self):\n",
        "        objs = [t[1] for t in self.queue]\n",
        "        print(objs)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "dQyMnNdEspvi"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "BEAM_SIZE = 8\n",
        "\n",
        "class Manager():\n",
        "    def __init__(self, is_train=True, ckpt_name=None):\n",
        "\n",
        "        # Load Transformer model & Adam optimizer\n",
        "        print(\"Loading Transformer model & Adam optimizer...\")\n",
        "        self.model = Transformer(src_vocab_size=SRC_VOCAB_SIZE, trg_vocab_size=TGT_VOCAB_SIZE).to(DEVICE)\n",
        "        self.optim = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "        self.best_loss = sys.float_info.max\n",
        "\n",
        "        if ckpt_name is not None:\n",
        "            assert os.path.exists(f\"{CHECKPOINT_DIR}/{ckpt_name}\"), f\"There is no checkpoint named {ckpt_name}.\"\n",
        "\n",
        "            print(\"Loading checkpoint...\")\n",
        "            checkpoint = torch.load(f\"{CHECKPOINT_DIR}/{ckpt_name}\", map_location=DEVICE)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optim.load_state_dict(checkpoint['optim_state_dict'])\n",
        "            self.best_loss = checkpoint['loss']\n",
        "        else:\n",
        "            print(\"Initializing the model...\")\n",
        "            for p in self.model.parameters():\n",
        "                if p.dim() > 1:\n",
        "                    nn.init.xavier_uniform_(p)\n",
        "\n",
        "        if is_train:\n",
        "            # Load loss function\n",
        "            print(\"Loading loss function...\")\n",
        "            self.criterion = nn.NLLLoss()\n",
        "\n",
        "            # Load dataloaders\n",
        "            print(\"Loading dataloaders...\")\n",
        "            self.train_loader = get_data_loader(TRAIN_NAME)\n",
        "            self.valid_loader = get_data_loader(VALID_NAME)\n",
        "\n",
        "        print(\"Setting finished.\")\n",
        "\n",
        "    def train(self):\n",
        "        print(\"Training starts.\")\n",
        "        stag_ct = 0\n",
        "        for epoch in range(1, NUM_EPOCHS+1):\n",
        "            self.model.train()\n",
        "\n",
        "            train_losses = []\n",
        "            start_time = datetime.datetime.now()\n",
        "\n",
        "            for i, batch in tqdm(enumerate(self.train_loader)):\n",
        "                src_input, trg_input, trg_output = batch\n",
        "                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n",
        "\n",
        "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
        "\n",
        "                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n",
        "\n",
        "                trg_output_shape = trg_output.shape\n",
        "                self.optim.zero_grad()\n",
        "                loss = self.criterion(\n",
        "                    output.view(-1, OUTPUT_VOCAB_SIZE),\n",
        "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n",
        "                )\n",
        "\n",
        "                loss.backward()\n",
        "                self.optim.step()\n",
        "\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            end_time = datetime.datetime.now()\n",
        "            training_time = end_time - start_time\n",
        "            seconds = training_time.seconds\n",
        "            hours = seconds // 3600\n",
        "            minutes = (seconds % 3600) // 60\n",
        "            seconds = seconds % 60\n",
        "\n",
        "            mean_train_loss = np.mean(train_losses)\n",
        "            print(f\"#################### Epoch: {epoch} ####################\")\n",
        "            print(f\"Train loss: {mean_train_loss} || One epoch training time: {hours}hrs {minutes}mins {seconds}secs\")\n",
        "\n",
        "            valid_loss, valid_time = self.validation()\n",
        "\n",
        "            if valid_loss < self.best_loss:\n",
        "                if not os.path.exists(CHECKPOINT_DIR):\n",
        "                    os.mkdir(CHECKPOINT_DIR)\n",
        "\n",
        "                self.best_loss = valid_loss\n",
        "                state_dict = {\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optim_state_dict': self.optim.state_dict(),\n",
        "                    'loss': self.best_loss\n",
        "                }\n",
        "                torch.save(state_dict, f\"{CHECKPOINT_DIR}/ckpt-{epoch}.tar\")\n",
        "                print(f\"***** Current best checkpoint is saved. *****\")\n",
        "                stag_ct = 0\n",
        "            else:\n",
        "                stag_ct += 1\n",
        "\n",
        "\n",
        "            print(f\"Best valid loss: {self.best_loss}\")\n",
        "            print(f\"Valid loss: {valid_loss} || One epoch training time: {valid_time}\")\n",
        "\n",
        "            if stag_ct >= 3:\n",
        "                print(\"Has not improved for 3 epochs. Stopping training.\")\n",
        "                break\n",
        "\n",
        "        print(f\"Training finished!\")\n",
        "\n",
        "    def validation(self):\n",
        "        print(\"Validation processing...\")\n",
        "        self.model.eval()\n",
        "\n",
        "        valid_losses = []\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in tqdm(enumerate(self.valid_loader)):\n",
        "                src_input, trg_input, trg_output = batch\n",
        "                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n",
        "\n",
        "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
        "\n",
        "                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n",
        "\n",
        "                trg_output_shape = trg_output.shape\n",
        "                loss = self.criterion(\n",
        "                    output.view(-1, OUTPUT_VOCAB_SIZE),\n",
        "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n",
        "                )\n",
        "\n",
        "                valid_losses.append(loss.item())\n",
        "\n",
        "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = datetime.datetime.now()\n",
        "        validation_time = end_time - start_time\n",
        "        seconds = validation_time.seconds\n",
        "        hours = seconds // 3600\n",
        "        minutes = (seconds % 3600) // 60\n",
        "        seconds = seconds % 60\n",
        "\n",
        "        mean_valid_loss = np.mean(valid_losses)\n",
        "\n",
        "        return mean_valid_loss, f\"{hours}hrs {minutes}mins {seconds}secs\"\n",
        "\n",
        "    def make_mask(self, src_input, trg_input):\n",
        "        # src_input: (B, L, d_model)\n",
        "        e_mask = (src_input != en_tokeniser.PAD_ID).unsqueeze(1)  # (B, 1, L)\n",
        "        d_mask = (trg_input != zh_tokeniser.PAD_ID).unsqueeze(1)  # (B, 1, L)\n",
        "\n",
        "        nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool)  # (1, L, L)\n",
        "        nopeak_mask = torch.tril(nopeak_mask).to(DEVICE)  # (1, L, L) to triangular shape\n",
        "        d_mask = d_mask & nopeak_mask  # (B, L, L) padding false\n",
        "\n",
        "        return e_mask, d_mask\n",
        "\n",
        "    def beam_search(self, e_output, e_mask):\n",
        "        _, TGT_PAD_IDX, TGT_BOS_IDX, TGT_EOS_IDX = zh_tokeniser.get_special_ids()\n",
        "        cur_queue = PriorityQueue()\n",
        "        \n",
        "        for _ in range(BEAM_SIZE):\n",
        "            cur_queue.put(BeamNode(TGT_BOS_IDX, -0.0, [TGT_BOS_IDX]))\n",
        "        \n",
        "        finished_count = 0\n",
        "\n",
        "        SEQUENCE_LENGTH = 288\n",
        "        \n",
        "        for pos in range(SEQUENCE_LENGTH):\n",
        "            new_queue = PriorityQueue()\n",
        "            for k in range(BEAM_SIZE):\n",
        "                node = cur_queue.get()\n",
        "                if node.is_finished:\n",
        "                    new_queue.put(node)\n",
        "                else:\n",
        "                    trg_input = torch.LongTensor(node.decoded + [TGT_PAD_IDX] * (SEQUENCE_LENGTH - len(node.decoded))).to(DEVICE) # (L)\n",
        "                    d_mask = (trg_input.unsqueeze(0) != TGT_PAD_IDX).unsqueeze(1).to(DEVICE) # (1, 1, L)\n",
        "                    nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool).to(DEVICE)\n",
        "                    nopeak_mask = torch.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
        "                    d_mask = d_mask & nopeak_mask # (1, L, L) padding false\n",
        "                    \n",
        "                    trg_embedded = self.model.trg_embedding.embed(trg_input.unsqueeze(0))\n",
        "                    trg_positional_encoded = self.model.positional_encoder(trg_embedded)\n",
        "                    decoder_output = self.model.decoder(\n",
        "                        trg_positional_encoded,\n",
        "                        e_output,\n",
        "                        e_mask,\n",
        "                        d_mask\n",
        "                    ) # (1, L, d_model)\n",
        "\n",
        "                    output = self.model.softmax(\n",
        "                        self.model.output_linear(decoder_output)\n",
        "                    ) # (1, L, trg_vocab_size)\n",
        "                    \n",
        "                    output = torch.topk(output[0][pos], dim=-1, k=BEAM_SIZE)\n",
        "                    last_word_ids = output.indices.tolist() # (k)\n",
        "                    last_word_prob = output.values.tolist() # (k)\n",
        "                    \n",
        "                    for i, idx in enumerate(last_word_ids):\n",
        "                        new_node = BeamNode(idx, -(-node.prob + last_word_prob[i]), node.decoded + [idx])\n",
        "                        if idx == TGT_EOS_IDX:\n",
        "                            new_node.prob = new_node.prob / float(len(new_node.decoded))\n",
        "                            new_node.is_finished = True\n",
        "                            finished_count += 1\n",
        "                        new_queue.put(new_node)\n",
        "            \n",
        "            cur_queue = copy.deepcopy(new_queue)\n",
        "            \n",
        "            if finished_count == BEAM_SIZE:\n",
        "                break\n",
        "        \n",
        "        decoded_output = cur_queue.get().decoded\n",
        "        \n",
        "        if decoded_output[-1] == TGT_EOS_IDX:\n",
        "            decoded_output = decoded_output[1:-1]\n",
        "        else:\n",
        "            decoded_output = decoded_output[1:]\n",
        "            \n",
        "        return zh_tokeniser.decode(decoded_output)\n",
        "\n",
        "    def translate(self, text: str, verbose=False):\n",
        "        _, SRC_PAD, _, SRC_EOS = en_tokeniser.get_special_ids()\n",
        "        tokenized = en_tokeniser.encode(text)\n",
        "        src_data = torch.LongTensor(\n",
        "            pad_or_truncate([SRC_PAD] + tokenized + [SRC_EOS], SRC_PAD)\n",
        "        ).unsqueeze(0).to(DEVICE) # (1, L)\n",
        "        e_mask = (src_data != SRC_PAD).unsqueeze(1).to(DEVICE) # (1, 1, L)\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Encoding input sentence...\")\n",
        "        src_data = self.model.src_embedding.embed(src_data)\n",
        "        src_data = self.model.positional_encoder(src_data)\n",
        "        e_output = self.model.encoder(src_data, e_mask) # (1, L, d_model)\n",
        "\n",
        "        \n",
        "        result = self.beam_search(e_output, e_mask)\n",
        "\n",
        "        end_time = datetime.datetime.now()\n",
        "\n",
        "        total_inference_time = end_time - start_time\n",
        "        seconds = total_inference_time.seconds\n",
        "        minutes = seconds // 60\n",
        "        seconds = seconds % 60\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Input: {text}\")\n",
        "            print(f\"Result: {result}\")\n",
        "            print(f\"Inference finished! || Total inference time: {minutes}mins {seconds}secs\")\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TtZYbLpeDVkH"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "torch._logging.set_logs(dynamo = logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Transformer model & Adam optimizer...\n",
            "Loading checkpoint...\n",
            "Setting finished.\n"
          ]
        }
      ],
      "source": [
        "m = Manager(is_train=False, ckpt_name=CHKPT_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow challenge.\\n'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(en_ds_test_path, 'r') as f:\n",
        "    sent = f.readline()\n",
        "    # sent = f.readline()\n",
        "    # sent = f.readline()\n",
        "\n",
        "sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'在TED演讲之前, 一个叫做《纽约客》的TED演讲。'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m.translate(sent.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = [\n",
        "    (1,0.7736834673102264),(2,0.452577741844181),(3,0.43574248257778037),(4,0.4193727361436948),(5,0.4040934379462602),(6,0.3919896937645827),(7,0.38231937961819573),(8,0.37428049209748726),(9,0.3673555763663021),(10,0.36116769517905983),(11,0.3555560631974662),(12,0.3504237517888444),(13,0.3457374642157369),(14,0.3414346778207252),(15,0.33737991907949116),(16,0.3335086542112818),(17,0.3298915220837649),(18,0.3264561823369928),\n",
        "]\n",
        "val_losses = [\n",
        "    (1,0.6115157932043076),(2,0.5932737350463867),(3,0.579197782278061),(4,0.5622445940971375),(5,0.5466036826372147),(6,0.5386703252792359),(7,0.533087819814682),(8,0.5283167630434036),(9,0.5239318042993546),(10,0.5177332490682602),(11,0.5167269051074982),(12,0.512279036641121),(13,0.5082521975040436),(14,0.5074395030736923),(15,0.5053986370563507),(16,0.5037084251642228),(17,0.5014634490013122),(18,0.499367943406105),\n",
        "]\n",
        "\n",
        "x = list(map(lambda p: p[0], train_losses))\n",
        "y_1 = list(map(lambda p: p[1], train_losses))\n",
        "y_2 = list(map(lambda p: p[1], val_losses))\n",
        "\n",
        "plt.plot(x, y_1, label = \"Train\") \n",
        "plt.plot(x, y_2, label = \"Validation\") \n",
        "\n",
        "plt.legend() \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

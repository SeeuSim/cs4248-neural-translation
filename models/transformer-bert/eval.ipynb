{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1mUnRtItsyi_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/seeusim/University/Y3/Y3S2/CS4248/Project/neural-translation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import math\n",
        "import sys, os\n",
        "\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import BertForMaskedLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4CgtW9oNBDF_"
      },
      "outputs": [],
      "source": [
        "en_model_path = '../../tokenisation/sentencepiece_custom/en.model'\n",
        "zh_model_path = '../../tokenisation/sentencepiece_custom/zh.model'\n",
        "\n",
        "en_ds_test_path = '../../tokenisation/data/iwslt2017-en-zh-test.en'\n",
        "zh_ds_test_path = '../../tokenisation/data/iwslt2017-en-zh-test.zh'\n",
        "\n",
        "bert_en_path = '../../models/bert-embed/BPE-bert-en'\n",
        "bert_zh_path = '../../models/bert-embed/BPE-bert-zh'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Co97KqSsvkV4"
      },
      "outputs": [],
      "source": [
        "class SentencePieceBPETokeniser(object):\n",
        "    PAD_ID = 3  # Defined as sentencepiece custom token\n",
        "\n",
        "    def __init__(self, lang: str, model_file=None):\n",
        "        self.model = spm.SentencePieceProcessor(\n",
        "            model_file=model_file or f\"./{lang}.model\"\n",
        "        )\n",
        "        self.special_ids = (\n",
        "            self.model.unk_id(),\n",
        "            SentencePieceBPETokeniser.PAD_ID,  # self.model.pad_id(), # this is -1 and may give errors.\n",
        "            self.model.bos_id(),\n",
        "            self.model.eos_id(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.model)\n",
        "\n",
        "    def encode_batch(self, sents: list[str], max_len=None):\n",
        "        return [self.encode(sent, max_len) for sent in sents]\n",
        "\n",
        "    def encode(self, sent: str | list[str], max_len=None):\n",
        "        if type(sent) == list:\n",
        "            return self.encode_batch(sent, max_len)\n",
        "        ids = self.model.encode(sent)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: list[int]):\n",
        "        return self.model.decode(\n",
        "            list(\n",
        "                filter(\n",
        "                    lambda id: id >= 0\n",
        "                    and id < len(self),\n",
        "                    ids\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def decode_batch(self, ids: list[list[int]]):\n",
        "        return [self.decode(id) for id in ids]\n",
        "\n",
        "    def get_special_ids(self):\n",
        "        UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = self.special_ids\n",
        "        return (UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX)\n",
        "\n",
        "en_tokeniser = SentencePieceBPETokeniser('en', model_file=en_model_path)\n",
        "zh_tokeniser = SentencePieceBPETokeniser('zh', model_file=zh_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "myw2imuPsQx2"
      },
      "outputs": [],
      "source": [
        "class BPEBertTokeniser:\n",
        "    out_keys = [\n",
        "        \"input_ids\",\n",
        "        \"token_type_ids\",\n",
        "        \"attention_mask\",\n",
        "        \"special_tokens_mask\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.pad_token_id = 3\n",
        "\n",
        "    def _process_id(self, input_ids):\n",
        "        \"\"\"\n",
        "        Called inside the model.\n",
        "        \"\"\"\n",
        "        # src_i: EOS, # trg_i: BOS, #trg_o: EOS\n",
        "        # input_ids = torch.tensor(input_ids) # 288, padded\n",
        "        token_type_ids = torch.zeros(input_ids.shape)\n",
        "\n",
        "        attention_mask = torch.where(input_ids == self.pad_token_id, 0, 1)\n",
        "\n",
        "        special_tokens_mask = torch.where(input_ids < 4, 1, 0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"token_type_ids\": token_type_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"special_tokens_mask\": special_tokens_mask,\n",
        "        }\n",
        "\n",
        "    def encode(self, row):\n",
        "        if type(row) != torch.Tensor:\n",
        "            row = torch.tensor(row)\n",
        "        if len(row.shape) == 1:\n",
        "            return self.encode_batch(row.unsqueeze(0))\n",
        "        return self.encode_batch(row)\n",
        "\n",
        "    def encode_batch(self, rows):\n",
        "        ids = list(map(lambda row: self._process_id(row), rows))\n",
        "        return {key: torch.vstack([example[key] for example in ids]) for key in BPEBertTokeniser.out_keys}\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, **_kwargs):\n",
        "        return self.encode(inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.model)\n",
        "\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "class BPEBertEmbedder:\n",
        "    def __init__(self, lang: str, model_file: str):\n",
        "        # Init the model from the pretrained weights\n",
        "        self.model = BertForMaskedLM.from_pretrained(\n",
        "            model_file, output_hidden_states=True\n",
        "        ).to(DEVICE)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Init the BPE tokeniser (padded to length 288, vocab 16384)\n",
        "        self.tokeniser = BPEBertTokeniser(lang)\n",
        "        pass\n",
        "\n",
        "    def embed(self, sent: list[int]):\n",
        "        tokens = self.tokeniser(\n",
        "            sent\n",
        "        )  # input_ids, token_type_mask, attention_mask, special_tokens_mask\n",
        "        with torch.no_grad():\n",
        "            input_ids = tokens[\"input_ids\"].to(DEVICE)\n",
        "            token_type_ids = tokens[\"token_type_ids\"].to(DEVICE)\n",
        "            attention_mask = tokens[\"attention_mask\"].to(DEVICE)\n",
        "            # Perform inference\n",
        "            output = self.model(input_ids, token_type_ids, attention_mask)\n",
        "            hidden_states = output.hidden_states\n",
        "\n",
        "            # Permutate and obtain hidden states\n",
        "        token_embeddings = torch.stack(hidden_states, dim=0)  # 13, batch, 288, 256\n",
        "\n",
        "        token_embeddings = token_embeddings.permute(1, 2, 0, 3)  # (batch, 288, n_layers, 256)\n",
        "\n",
        "        # Take last 4 layers\n",
        "        token_embeddings = token_embeddings[:, :, -4:, :]\n",
        "\n",
        "        # Take sum of last 4 layers\n",
        "        token_embeddings = token_embeddings.sum(axis=2)  # (batch, 288, 256)\n",
        "\n",
        "        token_embeddings.requires_grad_(False)\n",
        "\n",
        "        # Matrix of embeddings of dim 256, one per word\n",
        "        return token_embeddings\n",
        "\n",
        "\n",
        "en_embedder = BPEBertEmbedder('en', bert_en_path)\n",
        "zh_embedder = BPEBertEmbedder('zh', bert_zh_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "biMm2ZdasbMS"
      },
      "outputs": [],
      "source": [
        "# Path or parameters for data\n",
        "RSC_DIR = None\n",
        "\n",
        "TRAIN_NAME = 'iwslt2017-en-zh-train'\n",
        "\n",
        "VALID_NAME = 'iwslt2017-en-zh-validation'\n",
        "\n",
        "# For resuming from checkpoint\n",
        "CHKPT_NAME = 'ckpt-msk3-20.tar' # The key within the checkpoint directory\n",
        "\n",
        "SRC_VOCAB_SIZE = 16384\n",
        "TGT_VOCAB_SIZE = 16384\n",
        "OUTPUT_VOCAB_SIZE = TGT_VOCAB_SIZE\n",
        "\n",
        "PAD_IDX = 16384\n",
        "\n",
        "# Parameters for Transformer & training\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 90\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 6\n",
        "D_MODEL = 256\n",
        "D_FF = 1024\n",
        "D_K = D_MODEL // NUM_HEADS\n",
        "\n",
        "DROP_OUT_RATE = 0.1\n",
        "NUM_EPOCHS = 10\n",
        "BEAM_SIZE = 8\n",
        "\n",
        "# Resuming training from saved checkpoint\n",
        "CHECKPOINT_DIR = f'.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NvcyzimZshRW"
      },
      "outputs": [],
      "source": [
        "SEQUENCE_LENGTH = 288\n",
        "\n",
        "\n",
        "def get_data_loader(file_name):\n",
        "    print(f\"Getting source/target {file_name}...\")\n",
        "    with open(f\"{RSC_DIR}/{file_name}.en\", 'r') as f:\n",
        "        src_text_list = f.readlines()\n",
        "\n",
        "    with open(f\"{RSC_DIR}/{file_name}.zh\", 'r') as f:\n",
        "        trg_text_list = f.readlines()\n",
        "\n",
        "    print(\"Tokenizing & Padding src data...\")\n",
        "    src_list = process_src(src_text_list) # (sample_num, L)\n",
        "    print(f\"The shape of src data: {np.shape(src_list)}\")\n",
        "\n",
        "    print(\"Tokenizing & Padding trg data...\")\n",
        "    input_trg_list, output_trg_list = process_trg(trg_text_list) # (sample_num, L)\n",
        "    print(f\"The shape of input trg data: {np.shape(input_trg_list)}\")\n",
        "    print(f\"The shape of output trg data: {np.shape(output_trg_list)}\")\n",
        "\n",
        "    dataset = CustomDataset(src_list, input_trg_list, output_trg_list)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def pad_or_truncate(tokenized_text, pad_idx, max_len=SEQUENCE_LENGTH):\n",
        "    if len(tokenized_text) < max_len:\n",
        "        left = max_len - len(tokenized_text)\n",
        "        padding = [pad_idx] * left\n",
        "        tokenized_text += padding\n",
        "    else:\n",
        "        tokenized_text = tokenized_text[:max_len]\n",
        "\n",
        "    return tokenized_text\n",
        "\n",
        "MAX_SRC_LEN = 0\n",
        "MAX_TGT_LEN = 0\n",
        "\n",
        "MAX_SIZE=300_000\n",
        "\n",
        "def process_src(text_list):\n",
        "    \"\"\"\n",
        "    Add EOS to end, but only ignore pad\n",
        "    -> Add BOS, pad start idx and pad end + 1\n",
        "    \"\"\"\n",
        "    _, PAD_IDX, _, EOS_IDX = en_tokeniser.get_special_ids()\n",
        "    src_input_ids = []\n",
        "    for text in tqdm(text_list[:MAX_SIZE]):\n",
        "        ids = en_tokeniser.encode(text.strip())\n",
        "        src_input_ids.append(pad_or_truncate([PAD_IDX] + ids + [EOS_IDX], PAD_IDX))\n",
        "    return src_input_ids\n",
        "\n",
        "def process_trg(text_list):\n",
        "    \"\"\"\n",
        "    Add BOS to start, ignore pad\n",
        "    -> Add EOS, pad end - 1\n",
        "    Add EOS to end, ignore pad\n",
        "    -> Add BOS, pad start idx and pad end + 1\n",
        "    \"\"\"\n",
        "    _, PAD_IDX, BOS_IDX, EOS_IDX = zh_tokeniser.get_special_ids()\n",
        "    input_tokenized_list = []\n",
        "    output_tokenized_list = []\n",
        "    for text in tqdm(text_list[:MAX_SIZE]):\n",
        "        tokenized = zh_tokeniser.encode(text.strip())\n",
        "        trg_input = [BOS_IDX] + tokenized\n",
        "        trg_output = [PAD_IDX] + tokenized + [EOS_IDX]\n",
        "        input_tokenized_list.append(pad_or_truncate(trg_input, PAD_IDX))\n",
        "        output_tokenized_list.append(pad_or_truncate(trg_output, PAD_IDX))\n",
        "    return input_tokenized_list, output_tokenized_list\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, src_list, input_trg_list, output_trg_list):\n",
        "        super().__init__()\n",
        "        self.src_data = torch.LongTensor(src_list)\n",
        "        self.input_trg_data = torch.LongTensor(input_trg_list)\n",
        "        self.output_trg_data = torch.LongTensor(output_trg_list)\n",
        "\n",
        "        assert np.shape(src_list) == np.shape(input_trg_list), f\"The shape of src_list and input_trg_list are different: {np.shape(src_list)} {np.shape(input_trg_list)}\"\n",
        "        assert np.shape(input_trg_list) == np.shape(output_trg_list), f\"The shape of input_trg_list and output_trg_list are different: {np.shape(input_trg_list)} {np.shape(output_trg_list)}\"\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.input_trg_data[idx], self.output_trg_data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.src_data)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0CE_XX_vsiTs"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = LayerNormalization()\n",
        "        self.multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_2 = LayerNormalization()\n",
        "        self.feed_forward = FeedFowardLayer()\n",
        "        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x, e_mask):\n",
        "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_1(\n",
        "            self.multihead_attention(x_1, x_1, x_1, mask=e_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_2(self.feed_forward(x_2)) # (B, L, d_model)\n",
        "\n",
        "        return x # (B, L, d_model)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = LayerNormalization()\n",
        "        self.masked_multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_2 = LayerNormalization()\n",
        "        self.multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_3 = LayerNormalization()\n",
        "        self.feed_forward = FeedFowardLayer()\n",
        "        self.drop_out_3 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x, e_output, e_mask,  d_mask):\n",
        "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_1(\n",
        "            self.masked_multihead_attention(x_1, x_1, x_1, mask=d_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_2(\n",
        "            self.multihead_attention(x_2, e_output, e_output, mask=e_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_3 = self.layer_norm_3(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_3(self.feed_forward(x_3)) # (B, L, d_model)\n",
        "\n",
        "        return x # (B, L, d_model)\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inf = 1e9\n",
        "\n",
        "        # W^Q, W^K, W^V in the paper\n",
        "        self.w_q = nn.Linear(D_MODEL, D_MODEL)\n",
        "        self.w_k = nn.Linear(D_MODEL, D_MODEL)\n",
        "        self.w_v = nn.Linear(D_MODEL, D_MODEL)\n",
        "\n",
        "        self.dropout = nn.Dropout(DROP_OUT_RATE)\n",
        "        self.attn_softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Final output linear transformation\n",
        "        self.w_0 = nn.Linear(D_MODEL, D_MODEL)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        input_shape = q.shape\n",
        "\n",
        "        # Linear calculation +  split into num_heads\n",
        "        q = self.w_q(q).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "        k = self.w_k(k).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "        v = self.w_v(v).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "\n",
        "        # For convenience, convert all tensors in size (B, num_heads, L, d_k)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Conduct self-attention\n",
        "        attn_values = self.self_attention(q, k, v, mask=mask) # (B, num_heads, L, d_k)\n",
        "        concat_output = attn_values.transpose(1, 2)\\\n",
        "            .contiguous().view(input_shape[0], -1, D_MODEL) # (B, L, d_model)\n",
        "\n",
        "        return self.w_0(concat_output)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask=None):\n",
        "        # Calculate attention scores with scaled dot-product attention\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) # (B, num_heads, L, L)\n",
        "        attn_scores = attn_scores / math.sqrt(D_K)\n",
        "\n",
        "        # If there is a mask, make masked spots -INF\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1) # (B, 1, L) => (B, 1, 1, L) or (B, L, L) => (B, 1, L, L)\n",
        "            attn_scores = attn_scores.masked_fill_(mask == 0, -1 * self.inf)\n",
        "\n",
        "        # Softmax and multiplying K to calculate attention value\n",
        "        attn_distribs = self.attn_softmax(attn_scores)\n",
        "\n",
        "        attn_distribs = self.dropout(attn_distribs)\n",
        "        attn_values = torch.matmul(attn_distribs, v) # (B, num_heads, L, d_k)\n",
        "\n",
        "        return attn_values\n",
        "\n",
        "\n",
        "class FeedFowardLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(D_MODEL, D_FF, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(D_FF, D_MODEL, bias=True)\n",
        "        self.dropout = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear_1(x)) # (B, L, d_ff)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_2(x) # (B, L, d_model)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.layer = nn.LayerNorm([D_MODEL], elementwise_affine=True, eps=self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Make initial positional encoding matrix with 0\n",
        "        pe_matrix= torch.zeros(SEQUENCE_LENGTH, D_MODEL) # (L, d_model)\n",
        "\n",
        "        # Calculating position encoding values\n",
        "        for pos in range(SEQUENCE_LENGTH):\n",
        "            for i in range(D_MODEL):\n",
        "                if i % 2 == 0:\n",
        "                    pe_matrix[pos, i] = math.sin(pos / (10000 ** (2 * i / D_MODEL)))\n",
        "                elif i % 2 == 1:\n",
        "                    pe_matrix[pos, i] = math.cos(pos / (10000 ** (2 * i / D_MODEL)))\n",
        "\n",
        "        pe_matrix = pe_matrix.unsqueeze(0) # (1, L, d_model)\n",
        "        self.positional_encoding = pe_matrix.to(device=DEVICE).requires_grad_(False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * math.sqrt(D_MODEL) # (B, L, d_model)\n",
        "        x = x + self.positional_encoding # (B, L, d_model)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "66OApnHysm3Q"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
        "        super().__init__()\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.trg_vocab_size = trg_vocab_size\n",
        "\n",
        "        self.src_embedding = en_embedder\n",
        "        self.trg_embedding = zh_embedder\n",
        "        # self.src_embedding = nn.Embedding(self.src_vocab_size, D_MODEL)\n",
        "        # self.trg_embedding = nn.Embedding(self.trg_vocab_size, D_MODEL)\n",
        "        self.positional_encoder = PositionalEncoder()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.output_linear = nn.Linear(D_MODEL, self.trg_vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, src_input, trg_input, e_mask=None, d_mask=None):\n",
        "        src_input = self.src_embedding.embed(src_input)\n",
        "        trg_input = self.trg_embedding.embed(trg_input)\n",
        "\n",
        "        src_input = self.positional_encoder(src_input) # (B, L, d_model) => (B, L, d_model)\n",
        "        trg_input = self.positional_encoder(trg_input) # (B, L, d_model) => (B, L, d_model)\n",
        "\n",
        "        e_output = self.encoder(src_input, e_mask) # (B, L, d_model)\n",
        "        d_output = self.decoder(trg_input, e_output, e_mask, d_mask) # (B, L, d_model)\n",
        "\n",
        "        output = self.softmax(self.output_linear(d_output)) # (B, L, d_model) => # (B, L, trg_vocab_size)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for i in range(NUM_LAYERS)])\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, e_mask):\n",
        "        for i in range(NUM_LAYERS):\n",
        "            x = self.layers[i](x, e_mask)\n",
        "\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for i in range(NUM_LAYERS)])\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, e_output, e_mask, d_mask):\n",
        "        for i in range(NUM_LAYERS):\n",
        "            x = self.layers[i](x, e_output, e_mask, d_mask)\n",
        "\n",
        "        return self.layer_norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "\n",
        "class BeamNode():\n",
        "    def __init__(self, cur_idx, prob, decoded):\n",
        "        self.cur_idx = cur_idx\n",
        "        self.prob = prob\n",
        "        self.decoded = decoded\n",
        "        self.is_finished = False\n",
        "        \n",
        "    def __gt__(self, other):\n",
        "        return self.prob > other.prob\n",
        "    \n",
        "    def __ge__(self, other):\n",
        "        return self.prob >= other.prob\n",
        "    \n",
        "    def __lt__(self, other):\n",
        "        return self.prob < other.prob\n",
        "    \n",
        "    def __le__(self, other):\n",
        "        return self.prob <= other.prob\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return self.prob == other.prob\n",
        "    \n",
        "    def __ne__(self, other):\n",
        "        return self.prob != other.prob\n",
        "    \n",
        "    def print_spec(self):\n",
        "        print(f\"ID: {self} || cur_idx: {self.cur_idx} || prob: {self.prob} || decoded: {self.decoded}\")\n",
        "    \n",
        "\n",
        "class PriorityQueue():\n",
        "    def __init__(self):\n",
        "        self.queue = []\n",
        "        \n",
        "    def put(self, obj):\n",
        "        heapq.heappush(self.queue, (obj.prob, obj))\n",
        "        \n",
        "    def get(self):\n",
        "        return heapq.heappop(self.queue)[1]\n",
        "    \n",
        "    def qsize(self):\n",
        "        return len(self.queue)\n",
        "    \n",
        "    def print_scores(self):\n",
        "        scores = [t[0] for t in self.queue]\n",
        "        print(scores)\n",
        "        \n",
        "    def print_objs(self):\n",
        "        objs = [t[1] for t in self.queue]\n",
        "        print(objs)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dQyMnNdEspvi"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "BEAM_SIZE = 8\n",
        "\n",
        "class Manager():\n",
        "    def __init__(self, is_train=True, ckpt_name=None):\n",
        "\n",
        "        # Load Transformer model & Adam optimizer\n",
        "        print(\"Loading Transformer model & Adam optimizer...\")\n",
        "        self.model = Transformer(src_vocab_size=SRC_VOCAB_SIZE, trg_vocab_size=TGT_VOCAB_SIZE).to(DEVICE)\n",
        "        self.optim = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "        self.best_loss = sys.float_info.max\n",
        "\n",
        "        if ckpt_name is not None:\n",
        "            assert os.path.exists(f\"{CHECKPOINT_DIR}/{ckpt_name}\"), f\"There is no checkpoint named {ckpt_name}.\"\n",
        "\n",
        "            print(\"Loading checkpoint...\")\n",
        "            checkpoint = torch.load(f\"{CHECKPOINT_DIR}/{ckpt_name}\", map_location=DEVICE)\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optim.load_state_dict(checkpoint['optim_state_dict'])\n",
        "            self.best_loss = checkpoint['loss']\n",
        "        else:\n",
        "            print(\"Initializing the model...\")\n",
        "            for p in self.model.parameters():\n",
        "                if p.dim() > 1:\n",
        "                    nn.init.xavier_uniform_(p)\n",
        "\n",
        "        if is_train:\n",
        "            # Load loss function\n",
        "            print(\"Loading loss function...\")\n",
        "            self.criterion = nn.NLLLoss()\n",
        "\n",
        "            # Load dataloaders\n",
        "            print(\"Loading dataloaders...\")\n",
        "            self.train_loader = get_data_loader(TRAIN_NAME)\n",
        "            self.valid_loader = get_data_loader(VALID_NAME)\n",
        "\n",
        "        print(\"Setting finished.\")\n",
        "\n",
        "    def train(self):\n",
        "        print(\"Training starts.\")\n",
        "        stag_ct = 0\n",
        "        for epoch in range(1, NUM_EPOCHS+1):\n",
        "            self.model.train()\n",
        "\n",
        "            train_losses = []\n",
        "            start_time = datetime.datetime.now()\n",
        "\n",
        "            for i, batch in tqdm(enumerate(self.train_loader)):\n",
        "                src_input, trg_input, trg_output = batch\n",
        "                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n",
        "\n",
        "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
        "\n",
        "                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n",
        "\n",
        "                trg_output_shape = trg_output.shape\n",
        "                self.optim.zero_grad()\n",
        "                loss = self.criterion(\n",
        "                    output.view(-1, OUTPUT_VOCAB_SIZE),\n",
        "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n",
        "                )\n",
        "\n",
        "                loss.backward()\n",
        "                self.optim.step()\n",
        "\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            end_time = datetime.datetime.now()\n",
        "            training_time = end_time - start_time\n",
        "            seconds = training_time.seconds\n",
        "            hours = seconds // 3600\n",
        "            minutes = (seconds % 3600) // 60\n",
        "            seconds = seconds % 60\n",
        "\n",
        "            mean_train_loss = np.mean(train_losses)\n",
        "            print(f\"#################### Epoch: {epoch} ####################\")\n",
        "            print(f\"Train loss: {mean_train_loss} || One epoch training time: {hours}hrs {minutes}mins {seconds}secs\")\n",
        "\n",
        "            valid_loss, valid_time = self.validation()\n",
        "\n",
        "            if valid_loss < self.best_loss:\n",
        "                if not os.path.exists(CHECKPOINT_DIR):\n",
        "                    os.mkdir(CHECKPOINT_DIR)\n",
        "\n",
        "                self.best_loss = valid_loss\n",
        "                state_dict = {\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optim_state_dict': self.optim.state_dict(),\n",
        "                    'loss': self.best_loss\n",
        "                }\n",
        "                torch.save(state_dict, f\"{CHECKPOINT_DIR}/ckpt-{epoch}.tar\")\n",
        "                print(f\"***** Current best checkpoint is saved. *****\")\n",
        "                stag_ct = 0\n",
        "            else:\n",
        "                stag_ct += 1\n",
        "\n",
        "\n",
        "            print(f\"Best valid loss: {self.best_loss}\")\n",
        "            print(f\"Valid loss: {valid_loss} || One epoch training time: {valid_time}\")\n",
        "\n",
        "            if stag_ct >= 3:\n",
        "                print(\"Has not improved for 3 epochs. Stopping training.\")\n",
        "                break\n",
        "\n",
        "        print(f\"Training finished!\")\n",
        "\n",
        "    def validation(self):\n",
        "        print(\"Validation processing...\")\n",
        "        self.model.eval()\n",
        "\n",
        "        valid_losses = []\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in tqdm(enumerate(self.valid_loader)):\n",
        "                src_input, trg_input, trg_output = batch\n",
        "                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n",
        "\n",
        "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
        "\n",
        "                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n",
        "\n",
        "                trg_output_shape = trg_output.shape\n",
        "                loss = self.criterion(\n",
        "                    output.view(-1, OUTPUT_VOCAB_SIZE),\n",
        "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n",
        "                )\n",
        "\n",
        "                valid_losses.append(loss.item())\n",
        "\n",
        "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = datetime.datetime.now()\n",
        "        validation_time = end_time - start_time\n",
        "        seconds = validation_time.seconds\n",
        "        hours = seconds // 3600\n",
        "        minutes = (seconds % 3600) // 60\n",
        "        seconds = seconds % 60\n",
        "\n",
        "        mean_valid_loss = np.mean(valid_losses)\n",
        "\n",
        "        return mean_valid_loss, f\"{hours}hrs {minutes}mins {seconds}secs\"\n",
        "\n",
        "    def make_mask(self, src_input, trg_input):\n",
        "        # src_input: (B, L, d_model)\n",
        "        e_mask = (src_input != en_tokeniser.PAD_ID).unsqueeze(1)  # (B, 1, L)\n",
        "        d_mask = (trg_input != zh_tokeniser.PAD_ID).unsqueeze(1)  # (B, 1, L)\n",
        "\n",
        "        nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool)  # (1, L, L)\n",
        "        nopeak_mask = torch.tril(nopeak_mask).to(DEVICE)  # (1, L, L) to triangular shape\n",
        "        d_mask = d_mask & nopeak_mask  # (B, L, L) padding false\n",
        "\n",
        "        return e_mask, d_mask\n",
        "\n",
        "    def beam_search(self, e_output, e_mask):\n",
        "        _, TGT_PAD_IDX, TGT_BOS_IDX, TGT_EOS_IDX = zh_tokeniser.get_special_ids()\n",
        "        cur_queue = PriorityQueue()\n",
        "        \n",
        "        for _ in range(BEAM_SIZE):\n",
        "            cur_queue.put(BeamNode(TGT_BOS_IDX, -0.0, [TGT_BOS_IDX]))\n",
        "        \n",
        "        finished_count = 0\n",
        "\n",
        "        SEQUENCE_LENGTH = 288\n",
        "        \n",
        "        for pos in range(SEQUENCE_LENGTH):\n",
        "            new_queue = PriorityQueue()\n",
        "            for k in range(BEAM_SIZE):\n",
        "                node = cur_queue.get()\n",
        "                if node.is_finished:\n",
        "                    new_queue.put(node)\n",
        "                else:\n",
        "                    trg_input = torch.LongTensor(node.decoded + [TGT_PAD_IDX] * (SEQUENCE_LENGTH - len(node.decoded))).to(DEVICE) # (L)\n",
        "                    d_mask = (trg_input.unsqueeze(0) != TGT_PAD_IDX).unsqueeze(1).to(DEVICE) # (1, 1, L)\n",
        "                    nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool).to(DEVICE)\n",
        "                    nopeak_mask = torch.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
        "                    d_mask = d_mask & nopeak_mask # (1, L, L) padding false\n",
        "                    \n",
        "                    trg_embedded = self.model.trg_embedding.embed(trg_input.unsqueeze(0))\n",
        "                    trg_positional_encoded = self.model.positional_encoder(trg_embedded)\n",
        "                    decoder_output = self.model.decoder(\n",
        "                        trg_positional_encoded,\n",
        "                        e_output,\n",
        "                        e_mask,\n",
        "                        d_mask\n",
        "                    ) # (1, L, d_model)\n",
        "\n",
        "                    output = self.model.softmax(\n",
        "                        self.model.output_linear(decoder_output)\n",
        "                    ) # (1, L, trg_vocab_size)\n",
        "                    \n",
        "                    output = torch.topk(output[0][pos], dim=-1, k=BEAM_SIZE)\n",
        "                    last_word_ids = output.indices.tolist() # (k)\n",
        "                    last_word_prob = output.values.tolist() # (k)\n",
        "                    \n",
        "                    for i, idx in enumerate(last_word_ids):\n",
        "                        new_node = BeamNode(idx, -(-node.prob + last_word_prob[i]), node.decoded + [idx])\n",
        "                        if idx == TGT_EOS_IDX:\n",
        "                            new_node.prob = new_node.prob / float(len(new_node.decoded))\n",
        "                            new_node.is_finished = True\n",
        "                            finished_count += 1\n",
        "                        new_queue.put(new_node)\n",
        "            \n",
        "            cur_queue = copy.deepcopy(new_queue)\n",
        "            \n",
        "            if finished_count == BEAM_SIZE:\n",
        "                break\n",
        "        \n",
        "        decoded_output = cur_queue.get().decoded\n",
        "        \n",
        "        if decoded_output[-1] == TGT_EOS_IDX:\n",
        "            decoded_output = decoded_output[1:-1]\n",
        "        else:\n",
        "            decoded_output = decoded_output[1:]\n",
        "            \n",
        "        return zh_tokeniser.decode(decoded_output)\n",
        "\n",
        "    def translate(self, text: str, verbose=False):\n",
        "        _, SRC_PAD, _, SRC_EOS = en_tokeniser.get_special_ids()\n",
        "        tokenized = en_tokeniser.encode(text)\n",
        "        src_data = torch.LongTensor(\n",
        "            pad_or_truncate([SRC_PAD] + tokenized + [SRC_EOS], SRC_PAD)\n",
        "        ).unsqueeze(0).to(DEVICE) # (1, L)\n",
        "        e_mask = (src_data != SRC_PAD).unsqueeze(1).to(DEVICE) # (1, 1, L)\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Encoding input sentence...\")\n",
        "        src_data = self.model.src_embedding.embed(src_data)\n",
        "        src_data = self.model.positional_encoder(src_data)\n",
        "        e_output = self.model.encoder(src_data, e_mask) # (1, L, d_model)\n",
        "\n",
        "        \n",
        "        result = self.beam_search(e_output, e_mask)\n",
        "\n",
        "        end_time = datetime.datetime.now()\n",
        "\n",
        "        total_inference_time = end_time - start_time\n",
        "        seconds = total_inference_time.seconds\n",
        "        minutes = seconds // 60\n",
        "        seconds = seconds % 60\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Input: {text}\")\n",
        "            print(f\"Result: {result}\")\n",
        "            print(f\"Inference finished! || Total inference time: {minutes}mins {seconds}secs\")\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "TtZYbLpeDVkH"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "torch._logging.set_logs(dynamo = logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Transformer model & Adam optimizer...\n",
            "Loading checkpoint...\n",
            "Setting finished.\n"
          ]
        }
      ],
      "source": [
        "m = Manager(is_train=False, ckpt_name=CHKPT_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow challenge.\\n'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(en_ds_test_path, 'r') as f:\n",
        "    sent = f.readline()\n",
        "    # sent = f.readline()\n",
        "    # sent = f.readline()\n",
        "\n",
        "sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'这是TED演讲的TED演讲, 叫做“ ⁇ ”的项目。'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "m.translate(sent.strip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

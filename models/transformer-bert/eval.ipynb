{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUZXc7GCxDbB",
        "outputId": "6bde3ad9-2a60-473f-b485-9cb49cd0277d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Using cached accelerate-0.29.2-py3-none-any.whl (297 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.2 nvidia-cudnn-cu12-8.9.2.26 nvidia-cusolver-cu12-11.4.5.107\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy sentencepiece transformers[torch] tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1mUnRtItsyi_"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import math\n",
        "import sys, os\n",
        "\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import BatchEncoding, BertForMaskedLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJqVf6g_9Ncx",
        "outputId": "91ca2219-dc9a-499b-9aab-6049434728bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zSfpdL4mAr3W"
      },
      "outputs": [],
      "source": [
        "wdir_path = '/content/drive/MyDrive/TransformerMT'\n",
        "\n",
        "def get_path(path: str):\n",
        "    return f'{wdir_path}/{path}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4CgtW9oNBDF_"
      },
      "outputs": [],
      "source": [
        "en_model_path = get_path('en.model')\n",
        "zh_model_path = get_path('zh.model')\n",
        "\n",
        "en_ds_test_path = get_path('iwslt2017-en-zh-test.en')\n",
        "zh_ds_test_path = get_path('iwslt2017-en-zh-test.zh')\n",
        "\n",
        "bert_en_path = get_path('bert-en/model')\n",
        "bert_zh_path = get_path('bert-zh/model')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Co97KqSsvkV4"
      },
      "outputs": [],
      "source": [
        "class SentencePieceBPETokeniser(object):\n",
        "    PAD_ID = 3  # Defined as sentencepiece custom token\n",
        "\n",
        "    def __init__(self, lang: str, model_file=None):\n",
        "        self.model = spm.SentencePieceProcessor(\n",
        "            model_file=model_file or f\"./{lang}.model\"\n",
        "        )\n",
        "        self.special_ids = (\n",
        "            self.model.unk_id(),\n",
        "            SentencePieceBPETokeniser.PAD_ID,  # self.model.pad_id(), # this is -1 and may give errors.\n",
        "            self.model.bos_id(),\n",
        "            self.model.eos_id(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.model)\n",
        "\n",
        "    def encode_batch(self, sents: list[str], max_len=None):\n",
        "        return [self.encode(sent, max_len) for sent in sents]\n",
        "\n",
        "    def encode(self, sent: str | list[str], max_len=None):\n",
        "        if type(sent) == list:\n",
        "            return self.encode_batch(sent, max_len)\n",
        "        ids = self.model.encode(sent)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: list[int]):\n",
        "        return self.model.decode(\n",
        "            list(\n",
        "                filter(\n",
        "                    lambda id: id >= 0\n",
        "                    and id < len(self),\n",
        "                    ids\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def decode_batch(self, ids: list[list[int]]):\n",
        "        return [self.decode(id) for id in ids]\n",
        "\n",
        "    def get_special_ids(self):\n",
        "        UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = self.special_ids\n",
        "        return (UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX)\n",
        "\n",
        "en_tokeniser = SentencePieceBPETokeniser('en', model_file=en_model_path)\n",
        "zh_tokeniser = SentencePieceBPETokeniser('zh', model_file=zh_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "myw2imuPsQx2"
      },
      "outputs": [],
      "source": [
        "class BPEBertTokeniser:\n",
        "    out_keys = [\n",
        "        \"input_ids\",\n",
        "        \"token_type_ids\",\n",
        "        \"attention_mask\",\n",
        "        \"special_tokens_mask\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, lang):\n",
        "        self.lang = lang\n",
        "        self.pad_token_id = 3\n",
        "\n",
        "    def _process_id(self, input_ids):\n",
        "        \"\"\"\n",
        "        Called inside the model.\n",
        "        \"\"\"\n",
        "\n",
        "        # src_i: EOS, # trg_i: BOS, #trg_o: EOS\n",
        "        # input_ids = torch.tensor(input_ids) # 288, padded\n",
        "\n",
        "        token_type_ids = torch.zeros(input_ids.shape)\n",
        "\n",
        "        attention_mask = torch.where(input_ids == self.pad_token_id, 0, 1)\n",
        "\n",
        "        special_tokens_mask = torch.where(input_ids < 4, 1, 0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"token_type_ids\": token_type_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"special_tokens_mask\": special_tokens_mask,\n",
        "        }\n",
        "\n",
        "    def encode(self, row):\n",
        "        if len(row.shape) == 1:\n",
        "            return self.encode_batch(row.unsqueeze(0))\n",
        "\n",
        "        return self.encode_batch(row)\n",
        "\n",
        "    def encode_batch(self, rows):\n",
        "        ids = list(map(lambda row: self._process_id(row), rows))\n",
        "        return {key: torch.vstack([example[key] for example in ids]) for key in BPEBertTokeniser.out_keys}\n",
        "\n",
        "\n",
        "    def __call__(self, inputs, **_kwargs):\n",
        "        return self.encode(inputs)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.model)\n",
        "\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "class BPEBertEmbedder:\n",
        "    def __init__(self, lang: str, model_file: str):\n",
        "        # Init the model from the pretrained weights\n",
        "        self.model = BertForMaskedLM.from_pretrained(\n",
        "            model_file, output_hidden_states=True\n",
        "        ).to(DEVICE)\n",
        "        self.model.eval()\n",
        "\n",
        "        # Init the BPE tokeniser (padded to length 288, vocab 16384)\n",
        "        self.tokeniser = BPEBertTokeniser(lang)\n",
        "        pass\n",
        "\n",
        "    def embed(self, sent: list[int]):\n",
        "        tokens = self.tokeniser(\n",
        "            sent\n",
        "        )  # input_ids, token_type_mask, attention_mask, special_tokens_mask\n",
        "        with torch.no_grad():\n",
        "            input_ids = tokens[\"input_ids\"].to(DEVICE)\n",
        "            token_type_ids = tokens[\"token_type_ids\"].to(DEVICE)\n",
        "            attention_mask = tokens[\"attention_mask\"].to(DEVICE)\n",
        "            # Perform inference\n",
        "            output = self.model(input_ids, token_type_ids, attention_mask)\n",
        "            hidden_states = output.hidden_states\n",
        "\n",
        "            # Permutate and obtain hidden states\n",
        "        token_embeddings = torch.stack(hidden_states, dim=0)  # 13, batch, 288, 256\n",
        "\n",
        "        token_embeddings = token_embeddings.permute(1, 2, 0, 3)  # (batch, 288, n_layers, 256)\n",
        "\n",
        "        # Take last 4 layers\n",
        "        token_embeddings = token_embeddings[:, :, -4:, :]\n",
        "\n",
        "        # Take sum of last 4 layers\n",
        "        token_embeddings = token_embeddings.sum(axis=2)  # (batch, 288, 256)\n",
        "\n",
        "        token_embeddings.requires_grad_(False)\n",
        "\n",
        "        # Matrix of embeddings of dim 256, one per word\n",
        "        return token_embeddings\n",
        "\n",
        "\n",
        "en_embedder = BPEBertEmbedder('en', bert_en_path)\n",
        "zh_embedder = BPEBertEmbedder('zh', bert_zh_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "biMm2ZdasbMS"
      },
      "outputs": [],
      "source": [
        "# Path or parameters for data\n",
        "RSC_DIR = wdir_path\n",
        "\n",
        "TRAIN_NAME = 'iwslt2017-en-zh-train'\n",
        "\n",
        "VALID_NAME = 'iwslt2017-en-zh-validation'\n",
        "\n",
        "# For resuming from checkpoint\n",
        "CHKPT_NAME = None # The key within the checkpoint directory\n",
        "\n",
        "SRC_VOCAB_SIZE = 16384\n",
        "TGT_VOCAB_SIZE = 16384\n",
        "OUTPUT_VOCAB_SIZE = TGT_VOCAB_SIZE\n",
        "\n",
        "PAD_IDX = 16384\n",
        "\n",
        "# Parameters for Transformer & training\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 90\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 6\n",
        "D_MODEL = 256\n",
        "D_FF = 1024\n",
        "D_K = D_MODEL // NUM_HEADS\n",
        "\n",
        "DROP_OUT_RATE = 0.1\n",
        "NUM_EPOCHS = 10\n",
        "BEAM_SIZE = 8\n",
        "\n",
        "# Resuming training from saved checkpoint\n",
        "CHECKPOINT_DIR = f'{RSC_DIR}/saved_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NvcyzimZshRW"
      },
      "outputs": [],
      "source": [
        "SEQUENCE_LENGTH = 288\n",
        "\n",
        "\n",
        "def get_data_loader(file_name):\n",
        "    print(f\"Getting source/target {file_name}...\")\n",
        "    with open(f\"{RSC_DIR}/{file_name}.en\", 'r') as f:\n",
        "        src_text_list = f.readlines()\n",
        "\n",
        "    with open(f\"{RSC_DIR}/{file_name}.zh\", 'r') as f:\n",
        "        trg_text_list = f.readlines()\n",
        "\n",
        "    print(\"Tokenizing & Padding src data...\")\n",
        "    src_list = process_src(src_text_list) # (sample_num, L)\n",
        "    print(f\"The shape of src data: {np.shape(src_list)}\")\n",
        "\n",
        "    print(\"Tokenizing & Padding trg data...\")\n",
        "    input_trg_list, output_trg_list = process_trg(trg_text_list) # (sample_num, L)\n",
        "    print(f\"The shape of input trg data: {np.shape(input_trg_list)}\")\n",
        "    print(f\"The shape of output trg data: {np.shape(output_trg_list)}\")\n",
        "\n",
        "    dataset = CustomDataset(src_list, input_trg_list, output_trg_list)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def pad_or_truncate(tokenized_text, pad_idx, max_len=SEQUENCE_LENGTH):\n",
        "    if len(tokenized_text) < max_len:\n",
        "        left = max_len - len(tokenized_text)\n",
        "        padding = [pad_idx] * left\n",
        "        tokenized_text += padding\n",
        "    else:\n",
        "        tokenized_text = tokenized_text[:max_len]\n",
        "\n",
        "    return tokenized_text\n",
        "\n",
        "MAX_SRC_LEN = 0\n",
        "MAX_TGT_LEN = 0\n",
        "\n",
        "MAX_SIZE=300_000\n",
        "\n",
        "def process_src(text_list):\n",
        "    \"\"\"\n",
        "    Add EOS to end, but only ignore pad\n",
        "    -> Add BOS, pad start idx and pad end + 1\n",
        "    \"\"\"\n",
        "    _, PAD_IDX, _, EOS_IDX = en_tokeniser.get_special_ids()\n",
        "    src_input_ids = []\n",
        "    for text in tqdm(text_list[:MAX_SIZE]):\n",
        "        ids = en_tokeniser.encode(text.strip())\n",
        "        src_input_ids.append(pad_or_truncate([PAD_IDX] + ids + [EOS_IDX], PAD_IDX))\n",
        "    return src_input_ids\n",
        "\n",
        "def process_trg(text_list):\n",
        "    \"\"\"\n",
        "    Add BOS to start, ignore pad\n",
        "    -> Add EOS, pad end - 1\n",
        "    Add EOS to end, ignore pad\n",
        "    -> Add BOS, pad start idx and pad end + 1\n",
        "    \"\"\"\n",
        "    _, PAD_IDX, BOS_IDX, EOS_IDX = zh_tokeniser.get_special_ids()\n",
        "    input_tokenized_list = []\n",
        "    output_tokenized_list = []\n",
        "    for text in tqdm(text_list[:MAX_SIZE]):\n",
        "        tokenized = zh_tokeniser.encode(text.strip())\n",
        "        trg_input = [BOS_IDX] + tokenized\n",
        "        trg_output = [PAD_IDX] + tokenized + [EOS_IDX]\n",
        "        input_tokenized_list.append(pad_or_truncate(trg_input, PAD_IDX))\n",
        "        output_tokenized_list.append(pad_or_truncate(trg_output, PAD_IDX))\n",
        "    return input_tokenized_list, output_tokenized_list\n",
        "\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, src_list, input_trg_list, output_trg_list):\n",
        "        super().__init__()\n",
        "        self.src_data = torch.LongTensor(src_list)\n",
        "        self.input_trg_data = torch.LongTensor(input_trg_list)\n",
        "        self.output_trg_data = torch.LongTensor(output_trg_list)\n",
        "\n",
        "        assert np.shape(src_list) == np.shape(input_trg_list), f\"The shape of src_list and input_trg_list are different: {np.shape(src_list)} {np.shape(input_trg_list)}\"\n",
        "        assert np.shape(input_trg_list) == np.shape(output_trg_list), f\"The shape of input_trg_list and output_trg_list are different: {np.shape(input_trg_list)} {np.shape(output_trg_list)}\"\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.input_trg_data[idx], self.output_trg_data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.src_data)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0CE_XX_vsiTs"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = LayerNormalization()\n",
        "        self.multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_2 = LayerNormalization()\n",
        "        self.feed_forward = FeedFowardLayer()\n",
        "        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x, e_mask):\n",
        "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_1(\n",
        "            self.multihead_attention(x_1, x_1, x_1, mask=e_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_2(self.feed_forward(x_2)) # (B, L, d_model)\n",
        "\n",
        "        return x # (B, L, d_model)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = LayerNormalization()\n",
        "        self.masked_multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_2 = LayerNormalization()\n",
        "        self.multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_3 = LayerNormalization()\n",
        "        self.feed_forward = FeedFowardLayer()\n",
        "        self.drop_out_3 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x, e_output, e_mask,  d_mask):\n",
        "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_1(\n",
        "            self.masked_multihead_attention(x_1, x_1, x_1, mask=d_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_2(\n",
        "            self.multihead_attention(x_2, e_output, e_output, mask=e_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_3 = self.layer_norm_3(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_3(self.feed_forward(x_3)) # (B, L, d_model)\n",
        "\n",
        "        return x # (B, L, d_model)\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inf = 1e9\n",
        "\n",
        "        # W^Q, W^K, W^V in the paper\n",
        "        self.w_q = nn.Linear(D_MODEL, D_MODEL)\n",
        "        self.w_k = nn.Linear(D_MODEL, D_MODEL)\n",
        "        self.w_v = nn.Linear(D_MODEL, D_MODEL)\n",
        "\n",
        "        self.dropout = nn.Dropout(DROP_OUT_RATE)\n",
        "        self.attn_softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Final output linear transformation\n",
        "        self.w_0 = nn.Linear(D_MODEL, D_MODEL)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        input_shape = q.shape\n",
        "\n",
        "        # Linear calculation +  split into num_heads\n",
        "        q = self.w_q(q).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "        k = self.w_k(k).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "        v = self.w_v(v).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "\n",
        "        # For convenience, convert all tensors in size (B, num_heads, L, d_k)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Conduct self-attention\n",
        "        attn_values = self.self_attention(q, k, v, mask=mask) # (B, num_heads, L, d_k)\n",
        "        concat_output = attn_values.transpose(1, 2)\\\n",
        "            .contiguous().view(input_shape[0], -1, D_MODEL) # (B, L, d_model)\n",
        "\n",
        "        return self.w_0(concat_output)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask=None):\n",
        "        # Calculate attention scores with scaled dot-product attention\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) # (B, num_heads, L, L)\n",
        "        attn_scores = attn_scores / math.sqrt(D_K)\n",
        "\n",
        "        # If there is a mask, make masked spots -INF\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1) # (B, 1, L) => (B, 1, 1, L) or (B, L, L) => (B, 1, L, L)\n",
        "            attn_scores = attn_scores.masked_fill_(mask == 0, -1 * self.inf)\n",
        "\n",
        "        # Softmax and multiplying K to calculate attention value\n",
        "        attn_distribs = self.attn_softmax(attn_scores)\n",
        "\n",
        "        attn_distribs = self.dropout(attn_distribs)\n",
        "        attn_values = torch.matmul(attn_distribs, v) # (B, num_heads, L, d_k)\n",
        "\n",
        "        return attn_values\n",
        "\n",
        "\n",
        "class FeedFowardLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(D_MODEL, D_FF, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(D_FF, D_MODEL, bias=True)\n",
        "        self.dropout = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear_1(x)) # (B, L, d_ff)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_2(x) # (B, L, d_model)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.layer = nn.LayerNorm([D_MODEL], elementwise_affine=True, eps=self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Make initial positional encoding matrix with 0\n",
        "        pe_matrix= torch.zeros(SEQUENCE_LENGTH, D_MODEL) # (L, d_model)\n",
        "\n",
        "        # Calculating position encoding values\n",
        "        for pos in range(SEQUENCE_LENGTH):\n",
        "            for i in range(D_MODEL):\n",
        "                if i % 2 == 0:\n",
        "                    pe_matrix[pos, i] = math.sin(pos / (10000 ** (2 * i / D_MODEL)))\n",
        "                elif i % 2 == 1:\n",
        "                    pe_matrix[pos, i] = math.cos(pos / (10000 ** (2 * i / D_MODEL)))\n",
        "\n",
        "        pe_matrix = pe_matrix.unsqueeze(0) # (1, L, d_model)\n",
        "        self.positional_encoding = pe_matrix.to(device=DEVICE).requires_grad_(False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * math.sqrt(D_MODEL) # (B, L, d_model)\n",
        "        x = x + self.positional_encoding # (B, L, d_model)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "66OApnHysm3Q"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
        "        super().__init__()\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.trg_vocab_size = trg_vocab_size\n",
        "\n",
        "        self.src_embedding = en_embedder\n",
        "        self.trg_embedding = zh_embedder\n",
        "        # self.src_embedding = nn.Embedding(self.src_vocab_size, D_MODEL)\n",
        "        # self.trg_embedding = nn.Embedding(self.trg_vocab_size, D_MODEL)\n",
        "        self.positional_encoder = PositionalEncoder()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.output_linear = nn.Linear(D_MODEL, self.trg_vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, src_input, trg_input, e_mask=None, d_mask=None):\n",
        "        src_input = self.src_embedding.embed(src_input)\n",
        "        trg_input = self.trg_embedding.embed(trg_input)\n",
        "\n",
        "        src_input = self.positional_encoder(src_input) # (B, L, d_model) => (B, L, d_model)\n",
        "        trg_input = self.positional_encoder(trg_input) # (B, L, d_model) => (B, L, d_model)\n",
        "\n",
        "        e_output = self.encoder(src_input, e_mask) # (B, L, d_model)\n",
        "        d_output = self.decoder(trg_input, e_output, e_mask, d_mask) # (B, L, d_model)\n",
        "\n",
        "        output = self.softmax(self.output_linear(d_output)) # (B, L, d_model) => # (B, L, trg_vocab_size)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for i in range(NUM_LAYERS)])\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, e_mask):\n",
        "        for i in range(NUM_LAYERS):\n",
        "            x = self.layers[i](x, e_mask)\n",
        "\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for i in range(NUM_LAYERS)])\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, e_output, e_mask, d_mask):\n",
        "        for i in range(NUM_LAYERS):\n",
        "            x = self.layers[i](x, e_output, e_mask, d_mask)\n",
        "\n",
        "        return self.layer_norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import heapq\n",
        "\n",
        "\n",
        "class BeamNode():\n",
        "    def __init__(self, cur_idx, prob, decoded):\n",
        "        self.cur_idx = cur_idx\n",
        "        self.prob = prob\n",
        "        self.decoded = decoded\n",
        "        self.is_finished = False\n",
        "        \n",
        "    def __gt__(self, other):\n",
        "        return self.prob > other.prob\n",
        "    \n",
        "    def __ge__(self, other):\n",
        "        return self.prob >= other.prob\n",
        "    \n",
        "    def __lt__(self, other):\n",
        "        return self.prob < other.prob\n",
        "    \n",
        "    def __le__(self, other):\n",
        "        return self.prob <= other.prob\n",
        "    \n",
        "    def __eq__(self, other):\n",
        "        return self.prob == other.prob\n",
        "    \n",
        "    def __ne__(self, other):\n",
        "        return self.prob != other.prob\n",
        "    \n",
        "    def print_spec(self):\n",
        "        print(f\"ID: {self} || cur_idx: {self.cur_idx} || prob: {self.prob} || decoded: {self.decoded}\")\n",
        "    \n",
        "\n",
        "class PriorityQueue():\n",
        "    def __init__(self):\n",
        "        self.queue = []\n",
        "        \n",
        "    def put(self, obj):\n",
        "        heapq.heappush(self.queue, (obj.prob, obj))\n",
        "        \n",
        "    def get(self):\n",
        "        return heapq.heappop(self.queue)[1]\n",
        "    \n",
        "    def qsize(self):\n",
        "        return len(self.queue)\n",
        "    \n",
        "    def print_scores(self):\n",
        "        scores = [t[0] for t in self.queue]\n",
        "        print(scores)\n",
        "        \n",
        "    def print_objs(self):\n",
        "        objs = [t[1] for t in self.queue]\n",
        "        print(objs)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dQyMnNdEspvi"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "BEAM_SIZE = 8\n",
        "\n",
        "class Manager():\n",
        "    def __init__(self, is_train=True, ckpt_name=None):\n",
        "\n",
        "        # Load Transformer model & Adam optimizer\n",
        "        print(\"Loading Transformer model & Adam optimizer...\")\n",
        "        self.model = Transformer(src_vocab_size=SRC_VOCAB_SIZE, trg_vocab_size=TGT_VOCAB_SIZE).to(DEVICE)\n",
        "        self.optim = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "        self.best_loss = sys.float_info.max\n",
        "\n",
        "        if ckpt_name is not None:\n",
        "            assert os.path.exists(f\"{CHECKPOINT_DIR}/{ckpt_name}\"), f\"There is no checkpoint named {ckpt_name}.\"\n",
        "\n",
        "            print(\"Loading checkpoint...\")\n",
        "            checkpoint = torch.load(f\"{CHECKPOINT_DIR}/{ckpt_name}\")\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optim.load_state_dict(checkpoint['optim_state_dict'])\n",
        "            self.best_loss = checkpoint['loss']\n",
        "        else:\n",
        "            print(\"Initializing the model...\")\n",
        "            for p in self.model.parameters():\n",
        "                if p.dim() > 1:\n",
        "                    nn.init.xavier_uniform_(p)\n",
        "\n",
        "        if is_train:\n",
        "            # Load loss function\n",
        "            print(\"Loading loss function...\")\n",
        "            self.criterion = nn.NLLLoss()\n",
        "\n",
        "            # Load dataloaders\n",
        "            print(\"Loading dataloaders...\")\n",
        "            self.train_loader = get_data_loader(TRAIN_NAME)\n",
        "            self.valid_loader = get_data_loader(VALID_NAME)\n",
        "\n",
        "        print(\"Setting finished.\")\n",
        "\n",
        "    def train(self):\n",
        "        print(\"Training starts.\")\n",
        "        stag_ct = 0\n",
        "        for epoch in range(1, NUM_EPOCHS+1):\n",
        "            self.model.train()\n",
        "\n",
        "            train_losses = []\n",
        "            start_time = datetime.datetime.now()\n",
        "\n",
        "            for i, batch in tqdm(enumerate(self.train_loader)):\n",
        "                src_input, trg_input, trg_output = batch\n",
        "                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n",
        "\n",
        "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
        "\n",
        "                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n",
        "\n",
        "                trg_output_shape = trg_output.shape\n",
        "                self.optim.zero_grad()\n",
        "                loss = self.criterion(\n",
        "                    output.view(-1, OUTPUT_VOCAB_SIZE),\n",
        "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n",
        "                )\n",
        "\n",
        "                loss.backward()\n",
        "                self.optim.step()\n",
        "\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            end_time = datetime.datetime.now()\n",
        "            training_time = end_time - start_time\n",
        "            seconds = training_time.seconds\n",
        "            hours = seconds // 3600\n",
        "            minutes = (seconds % 3600) // 60\n",
        "            seconds = seconds % 60\n",
        "\n",
        "            mean_train_loss = np.mean(train_losses)\n",
        "            print(f\"#################### Epoch: {epoch} ####################\")\n",
        "            print(f\"Train loss: {mean_train_loss} || One epoch training time: {hours}hrs {minutes}mins {seconds}secs\")\n",
        "\n",
        "            valid_loss, valid_time = self.validation()\n",
        "\n",
        "            if valid_loss < self.best_loss:\n",
        "                if not os.path.exists(CHECKPOINT_DIR):\n",
        "                    os.mkdir(CHECKPOINT_DIR)\n",
        "\n",
        "                self.best_loss = valid_loss\n",
        "                state_dict = {\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optim_state_dict': self.optim.state_dict(),\n",
        "                    'loss': self.best_loss\n",
        "                }\n",
        "                torch.save(state_dict, f\"{CHECKPOINT_DIR}/ckpt-{epoch}.tar\")\n",
        "                print(f\"***** Current best checkpoint is saved. *****\")\n",
        "                stag_ct = 0\n",
        "            else:\n",
        "                stag_ct += 1\n",
        "\n",
        "\n",
        "            print(f\"Best valid loss: {self.best_loss}\")\n",
        "            print(f\"Valid loss: {valid_loss} || One epoch training time: {valid_time}\")\n",
        "\n",
        "            if stag_ct >= 3:\n",
        "                print(\"Has not improved for 3 epochs. Stopping training.\")\n",
        "                break\n",
        "\n",
        "        print(f\"Training finished!\")\n",
        "\n",
        "    def validation(self):\n",
        "        print(\"Validation processing...\")\n",
        "        self.model.eval()\n",
        "\n",
        "        valid_losses = []\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in tqdm(enumerate(self.valid_loader)):\n",
        "                src_input, trg_input, trg_output = batch\n",
        "                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n",
        "\n",
        "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
        "\n",
        "                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n",
        "\n",
        "                trg_output_shape = trg_output.shape\n",
        "                loss = self.criterion(\n",
        "                    output.view(-1, OUTPUT_VOCAB_SIZE),\n",
        "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n",
        "                )\n",
        "\n",
        "                valid_losses.append(loss.item())\n",
        "\n",
        "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = datetime.datetime.now()\n",
        "        validation_time = end_time - start_time\n",
        "        seconds = validation_time.seconds\n",
        "        hours = seconds // 3600\n",
        "        minutes = (seconds % 3600) // 60\n",
        "        seconds = seconds % 60\n",
        "\n",
        "        mean_valid_loss = np.mean(valid_losses)\n",
        "\n",
        "        return mean_valid_loss, f\"{hours}hrs {minutes}mins {seconds}secs\"\n",
        "\n",
        "    def make_mask(self, src_input, trg_input):\n",
        "        # src_input: (B, L, d_model)\n",
        "        e_mask = (src_input != en_tokeniser.PAD_ID).unsqueeze(1)  # (B, 1, L)\n",
        "        d_mask = (trg_input != zh_tokeniser.PAD_ID).unsqueeze(1)  # (B, 1, L)\n",
        "\n",
        "        nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool)  # (1, L, L)\n",
        "        nopeak_mask = torch.tril(nopeak_mask).to(DEVICE)  # (1, L, L) to triangular shape\n",
        "        d_mask = d_mask & nopeak_mask  # (B, L, L) padding false\n",
        "\n",
        "        return e_mask, d_mask\n",
        "\n",
        "    def beam_search(self, e_output, e_mask):\n",
        "        _, TGT_PAD_IDX, TGT_BOS_IDX, TGT_EOS_IDX = zh_tokeniser.get_special_ids()\n",
        "        cur_queue = PriorityQueue()\n",
        "        \n",
        "        for _ in range(BEAM_SIZE):\n",
        "            cur_queue.put(BeamNode(TGT_BOS_IDX, -0.0, [TGT_BOS_IDX]))\n",
        "        \n",
        "        finished_count = 0\n",
        "\n",
        "        SEQUENCE_LENGTH = 288\n",
        "        \n",
        "        for pos in range(SEQUENCE_LENGTH):\n",
        "            new_queue = PriorityQueue()\n",
        "            for k in range(BEAM_SIZE):\n",
        "                node = cur_queue.get()\n",
        "                if node.is_finished:\n",
        "                    new_queue.put(node)\n",
        "                else:\n",
        "                    trg_input = torch.LongTensor(node.decoded + [TGT_PAD_IDX] * (SEQUENCE_LENGTH - len(node.decoded))).to(DEVICE) # (L)\n",
        "                    d_mask = (trg_input.unsqueeze(0) != TGT_PAD_IDX).unsqueeze(1).to(DEVICE) # (1, 1, L)\n",
        "                    nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool).to(DEVICE)\n",
        "                    nopeak_mask = torch.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
        "                    d_mask = d_mask & nopeak_mask # (1, L, L) padding false\n",
        "                    \n",
        "                    trg_embedded = self.model.trg_embedding.encode(trg_input.unsqueeze(0))\n",
        "                    trg_positional_encoded = self.model.positional_encoder(trg_embedded)\n",
        "                    decoder_output = self.model.decoder(\n",
        "                        trg_positional_encoded,\n",
        "                        e_output,\n",
        "                        e_mask,\n",
        "                        d_mask\n",
        "                    ) # (1, L, d_model)\n",
        "\n",
        "                    output = self.model.softmax(\n",
        "                        self.model.output_linear(decoder_output)\n",
        "                    ) # (1, L, trg_vocab_size)\n",
        "                    \n",
        "                    output = torch.topk(output[0][pos], dim=-1, k=BEAM_SIZE)\n",
        "                    last_word_ids = output.indices.tolist() # (k)\n",
        "                    last_word_prob = output.values.tolist() # (k)\n",
        "                    \n",
        "                    for i, idx in enumerate(last_word_ids):\n",
        "                        new_node = BeamNode(idx, -(-node.prob + last_word_prob[i]), node.decoded + [idx])\n",
        "                        if idx == TGT_EOS_IDX:\n",
        "                            new_node.prob = new_node.prob / float(len(new_node.decoded))\n",
        "                            new_node.is_finished = True\n",
        "                            finished_count += 1\n",
        "                        new_queue.put(new_node)\n",
        "            \n",
        "            cur_queue = copy.deepcopy(new_queue)\n",
        "            \n",
        "            if finished_count == BEAM_SIZE:\n",
        "                break\n",
        "        \n",
        "        decoded_output = cur_queue.get().decoded\n",
        "        \n",
        "        if decoded_output[-1] == TGT_EOS_IDX:\n",
        "            decoded_output = decoded_output[1:-1]\n",
        "        else:\n",
        "            decoded_output = decoded_output[1:]\n",
        "            \n",
        "        return zh_tokeniser.decode(decoded_output)\n",
        "\n",
        "    def translate(self, text: str, verbose=False):\n",
        "        _, SRC_PAD, _, _ = en_tokeniser.get_special_ids()\n",
        "        tokenized = en_tokeniser.encode(text)\n",
        "        src_data = torch.LongTensor(\n",
        "            pad_or_truncate(tokenized, SRC_PAD)\n",
        "        ).unsqueeze(0).to(DEVICE) # (1, L)\n",
        "        e_mask = (src_data != SRC_PAD).unsqueeze(1).to(DEVICE) # (1, 1, L)\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Encoding input sentence...\")\n",
        "        src_data = self.model.src_embedding.embed(src_data)\n",
        "        src_data = self.model.positional_encoder(src_data)\n",
        "        e_output = self.model.encoder(src_data, e_mask) # (1, L, d_model)\n",
        "\n",
        "        \n",
        "        result = self.beam_search(e_output, e_mask)\n",
        "\n",
        "        end_time = datetime.datetime.now()\n",
        "\n",
        "        total_inference_time = end_time - start_time\n",
        "        seconds = total_inference_time.seconds\n",
        "        minutes = seconds // 60\n",
        "        seconds = seconds % 60\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Input: {text}\")\n",
        "            print(f\"Result: {result}\")\n",
        "            print(f\"Inference finished! || Total inference time: {minutes}mins {seconds}secs\")\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "TtZYbLpeDVkH"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "torch._logging.set_logs(dynamo = logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Skp03PnOssFT",
        "outputId": "55a24910-3047-4fd7-855d-d6431efdb998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Transformer model & Adam optimizer...\n",
            "Initializing the model...\n",
            "Loading loss function...\n",
            "Loading dataloaders...\n",
            "Getting source/target iwslt2017-en-zh-train...\n",
            "Tokenizing & Padding src data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 231266/231266 [00:14<00:00, 15785.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of src data: (231266, 288)\n",
            "Tokenizing & Padding trg data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 231266/231266 [00:11<00:00, 20706.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of input trg data: (231266, 288)\n",
            "The shape of output trg data: (231266, 288)\n",
            "Getting source/target iwslt2017-en-zh-validation...\n",
            "Tokenizing & Padding src data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 879/879 [00:00<00:00, 14501.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of src data: (879, 288)\n",
            "Tokenizing & Padding trg data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 879/879 [00:00<00:00, 25532.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of input trg data: (879, 288)\n",
            "The shape of output trg data: (879, 288)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting finished.\n",
            "Training starts.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2570it [1:19:37,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################### Epoch: 1 ####################\n",
            "Train loss: 0.5346486325437341 || One epoch training time: 1hrs 19mins 37secs\n",
            "Validation processing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10it [00:10,  1.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Current best checkpoint is saved. *****\n",
            "Best valid loss: 0.08907114937901497\n",
            "Valid loss: 0.08907114937901497 || One epoch training time: 0hrs 0mins 10secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2570it [1:19:23,  1.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################### Epoch: 2 ####################\n",
            "Train loss: 0.029130945210471923 || One epoch training time: 1hrs 19mins 23secs\n",
            "Validation processing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10it [00:10,  1.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Current best checkpoint is saved. *****\n",
            "Best valid loss: 0.01187729723751545\n",
            "Valid loss: 0.01187729723751545 || One epoch training time: 0hrs 0mins 10secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2570it [1:19:19,  1.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################### Epoch: 3 ####################\n",
            "Train loss: 0.002819384383907404 || One epoch training time: 1hrs 19mins 19secs\n",
            "Validation processing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10it [00:10,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Current best checkpoint is saved. *****\n",
            "Best valid loss: 0.0024034160189330576\n",
            "Valid loss: 0.0024034160189330576 || One epoch training time: 0hrs 0mins 10secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2570it [1:19:24,  1.85s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################### Epoch: 4 ####################\n",
            "Train loss: 0.0003883385322109256 || One epoch training time: 1hrs 19mins 24secs\n",
            "Validation processing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10it [00:10,  1.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Current best checkpoint is saved. *****\n",
            "Best valid loss: 0.0016785235362476668\n",
            "Valid loss: 0.0016785235362476668 || One epoch training time: 0hrs 0mins 10secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2570it [1:19:29,  1.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#################### Epoch: 5 ####################\n",
            "Train loss: 0.00010272818267526506 || One epoch training time: 1hrs 19mins 29secs\n",
            "Validation processing...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10it [00:10,  1.00s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** Current best checkpoint is saved. *****\n",
            "Best valid loss: 0.0014898291188728762\n",
            "Valid loss: 0.0014898291188728762 || One epoch training time: 0hrs 0mins 10secs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "497it [15:21,  1.86s/it]"
          ]
        }
      ],
      "source": [
        "manager = Manager(is_train=False, ckpt_name=CHKPT_NAME)\n",
        "\n",
        "with open(en_ds_test_path, 'r') as test_f, \\\n",
        "        open(get_path('230k-trf-bert-pred-msk.out'), 'w') as out_f:\n",
        "    inputs = test_f.readlines()\n",
        "    for line in tqdm(inputs):\n",
        "        out_f.write(manager.translate(line) + '\\n')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtFy6hbfLHcI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

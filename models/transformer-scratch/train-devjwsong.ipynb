{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1mUnRtItsyi_"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import math\n",
        "import sys, os\n",
        "\n",
        "import numpy as np\n",
        "import sentencepiece as spm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJqVf6g_9Ncx",
        "outputId": "6bea54e7-8e90-4466-b32c-dabd3177c10c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zSfpdL4mAr3W"
      },
      "outputs": [],
      "source": [
        "wdir_path = '/content/drive/MyDrive/TransformerMT'\n",
        "\n",
        "def get_path(path: str):\n",
        "    return f'{wdir_path}/{path}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4CgtW9oNBDF_"
      },
      "outputs": [],
      "source": [
        "en_model_path = get_path('en.model')\n",
        "zh_model_path = get_path('zh.model')\n",
        "\n",
        "en_ds_test_path = get_path('iwslt2017-en-zh-test.en')\n",
        "zh_ds_test_path = get_path('iwslt2017-en-zh-test.zh')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OPkU8pGABwkQ"
      },
      "outputs": [],
      "source": [
        "class SentencePieceBPETokeniser(object):\n",
        "    PAD_ID = 3  # Defined as sentencepiece custom token\n",
        "\n",
        "    def __init__(self, lang: str, model_file=None):\n",
        "        self.model = spm.SentencePieceProcessor(\n",
        "            model_file=model_file or f\"./{lang}.model\"\n",
        "        )\n",
        "        self.special_ids = (\n",
        "            self.model.unk_id(),\n",
        "            SentencePieceBPETokeniser.PAD_ID,  # self.model.pad_id(), # this is -1 and may give errors.\n",
        "            self.model.bos_id(),\n",
        "            self.model.eos_id(),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.model)\n",
        "\n",
        "    def encode_batch(self, sents: list[str], max_len=None):\n",
        "        return [self.encode(sent, max_len) for sent in sents]\n",
        "\n",
        "    def encode(self, sent: str | list[str], max_len=None):\n",
        "        if type(sent) == list:\n",
        "            return self.encode_batch(sent, max_len)\n",
        "        ids = self.model.encode(sent)\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: list[int]):\n",
        "        return self.model.decode(\n",
        "            list(\n",
        "                filter(\n",
        "                    lambda id: id >= 0\n",
        "                    and id < len(self),\n",
        "                    ids\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def decode_batch(self, ids: list[list[int]]):\n",
        "        return [self.decode(id) for id in ids]\n",
        "\n",
        "    def get_special_ids(self):\n",
        "        UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = self.special_ids\n",
        "        return (UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "myw2imuPsQx2"
      },
      "outputs": [],
      "source": [
        "en_tokeniser = SentencePieceBPETokeniser('en', model_file=en_model_path)\n",
        "zh_tokeniser = SentencePieceBPETokeniser('zh', model_file=zh_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "biMm2ZdasbMS"
      },
      "outputs": [],
      "source": [
        "# Path or parameters for data\n",
        "RSC_DIR = wdir_path\n",
        "\n",
        "TRAIN_NAME = 'iwslt2017-en-zh-train'\n",
        "\n",
        "VALID_NAME = 'iwslt2017-en-zh-validation'\n",
        "\n",
        "# For resuming from checkpoint\n",
        "CHKPT_NAME = None # The key within the checkpoint directory\n",
        "\n",
        "SRC_VOCAB_SIZE = len(en_tokeniser)\n",
        "TGT_VOCAB_SIZE = len(zh_tokeniser)\n",
        "OUTPUT_VOCAB_SIZE = TGT_VOCAB_SIZE\n",
        "\n",
        "# Parameters for sentencepiece tokenizer\n",
        "_, SRC_PAD_IDX, _, _ = en_tokeniser.get_special_ids()\n",
        "_, TGT_PAD_IDX, _, _ = en_tokeniser.get_special_ids()\n",
        "\n",
        "# Parameters for Transformer & training\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 80\n",
        "SEQUENCE_LENGTH = 296\n",
        "NUM_HEADS = 4\n",
        "NUM_LAYERS = 3\n",
        "D_MODEL = 256\n",
        "D_FF = 1024\n",
        "D_K = D_MODEL // NUM_HEADS\n",
        "\n",
        "DROP_OUT_RATE = 0.1\n",
        "NUM_EPOCHS = 10\n",
        "BEAM_SIZE = 8\n",
        "\n",
        "# Resuming training from saved checkpoint\n",
        "CHECKPOINT_DIR = f'{RSC_DIR}/saved_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NvcyzimZshRW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def get_data_loader(file_name):\n",
        "    print(f\"Getting source/target {file_name}...\")\n",
        "    with open(f\"{RSC_DIR}/{file_name}.en\", 'r') as f:\n",
        "        src_text_list = f.readlines()\n",
        "\n",
        "    with open(f\"{RSC_DIR}/{file_name}.zh\", 'r') as f:\n",
        "        trg_text_list = f.readlines()\n",
        "\n",
        "    print(\"Tokenizing & Padding src data...\")\n",
        "    src_list = process_src(src_text_list) # (sample_num, L)\n",
        "    print(f\"The shape of src data: {np.shape(src_list)}\")\n",
        "\n",
        "    print(\"Tokenizing & Padding trg data...\")\n",
        "    input_trg_list, output_trg_list = process_trg(trg_text_list) # (sample_num, L)\n",
        "    print(f\"The shape of input trg data: {np.shape(input_trg_list)}\")\n",
        "    print(f\"The shape of output trg data: {np.shape(output_trg_list)}\")\n",
        "\n",
        "    dataset = CustomDataset(src_list, input_trg_list, output_trg_list)\n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "def pad_or_truncate(tokenized_text, pad_idx, max_len=SEQUENCE_LENGTH):\n",
        "    if len(tokenized_text) < max_len:\n",
        "        left = max_len - len(tokenized_text)\n",
        "        padding = [pad_idx] * left\n",
        "        tokenized_text += padding\n",
        "    else:\n",
        "        tokenized_text = tokenized_text[:max_len]\n",
        "\n",
        "    return tokenized_text\n",
        "\n",
        "MAX_SRC_LEN = 0\n",
        "MAX_TGT_LEN = 0\n",
        "\n",
        "MAX_SIZE=10_000\n",
        "\n",
        "def process_src(text_list):\n",
        "    global MAX_SRC_LEN\n",
        "    _, PAD_IDX, _, EOS_IDX = en_tokeniser.get_special_ids()\n",
        "    src_input_ids = []\n",
        "    for text in tqdm(text_list[:MAX_SIZE]):\n",
        "        ids = en_tokeniser.encode(text.strip())\n",
        "        src_input_ids.append(pad_or_truncate(ids + [EOS_IDX], PAD_IDX))\n",
        "    return src_input_ids\n",
        "\n",
        "def process_trg(text_list):\n",
        "    global MAX_TGT_LEN\n",
        "    _, PAD_IDX, BOS_IDX, EOS_IDX = zh_tokeniser.get_special_ids()\n",
        "    input_tokenized_list = []\n",
        "    output_tokenized_list = []\n",
        "    for text in tqdm(text_list[:MAX_SIZE]):\n",
        "        tokenized = zh_tokeniser.encode(text.strip())\n",
        "        trg_input = [BOS_IDX] + tokenized\n",
        "        trg_output = tokenized + [EOS_IDX]\n",
        "        input_tokenized_list.append(pad_or_truncate(trg_input, PAD_IDX))\n",
        "        output_tokenized_list.append(pad_or_truncate(trg_output, PAD_IDX))\n",
        "    return input_tokenized_list, output_tokenized_list\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, src_list, input_trg_list, output_trg_list):\n",
        "        super().__init__()\n",
        "        self.src_data = torch.LongTensor(src_list)\n",
        "        self.input_trg_data = torch.LongTensor(input_trg_list)\n",
        "        self.output_trg_data = torch.LongTensor(output_trg_list)\n",
        "\n",
        "        assert np.shape(src_list) == np.shape(input_trg_list), f\"The shape of src_list and input_trg_list are different: {np.shape(src_list)} {np.shape(input_trg_list)}\"\n",
        "        assert np.shape(input_trg_list) == np.shape(output_trg_list), f\"The shape of input_trg_list and output_trg_list are different: {np.shape(input_trg_list)} {np.shape(output_trg_list)}\"\n",
        "\n",
        "    def make_mask(self):\n",
        "        e_mask = (self.src_data != SRC_PAD_IDX).unsqueeze(1) # (num_samples, 1, L)\n",
        "        d_mask = (self.input_trg_data != TGT_PAD_IDX).unsqueeze(1) # (num_samples, 1, L)\n",
        "\n",
        "        nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool) # (1, L, L)\n",
        "        nopeak_mask = torch.tril(nopeak_mask) # (1, L, L) to triangular shape\n",
        "        d_mask = d_mask & nopeak_mask # (num_samples, L, L) padding false\n",
        "\n",
        "        return e_mask, d_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_data[idx], self.input_trg_data[idx], self.output_trg_data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.src_data)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0CE_XX_vsiTs"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = LayerNormalization()\n",
        "        self.multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_2 = LayerNormalization()\n",
        "        self.feed_forward = FeedFowardLayer()\n",
        "        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x, e_mask):\n",
        "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_1(\n",
        "            self.multihead_attention(x_1, x_1, x_1, mask=e_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_2(self.feed_forward(x_2)) # (B, L, d_model)\n",
        "\n",
        "        return x # (B, L, d_model)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer_norm_1 = LayerNormalization()\n",
        "        self.masked_multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_1 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_2 = LayerNormalization()\n",
        "        self.multihead_attention = MultiheadAttention()\n",
        "        self.drop_out_2 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "        self.layer_norm_3 = LayerNormalization()\n",
        "        self.feed_forward = FeedFowardLayer()\n",
        "        self.drop_out_3 = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x, e_output, e_mask,  d_mask):\n",
        "        x_1 = self.layer_norm_1(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_1(\n",
        "            self.masked_multihead_attention(x_1, x_1, x_1, mask=d_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_2 = self.layer_norm_2(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_2(\n",
        "            self.multihead_attention(x_2, e_output, e_output, mask=e_mask)\n",
        "        ) # (B, L, d_model)\n",
        "        x_3 = self.layer_norm_3(x) # (B, L, d_model)\n",
        "        x = x + self.drop_out_3(self.feed_forward(x_3)) # (B, L, d_model)\n",
        "\n",
        "        return x # (B, L, d_model)\n",
        "\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.inf = 1e9\n",
        "\n",
        "        # W^Q, W^K, W^V in the paper\n",
        "        self.w_q = nn.Linear(D_MODEL, D_MODEL)\n",
        "        self.w_k = nn.Linear(D_MODEL, D_MODEL)\n",
        "        self.w_v = nn.Linear(D_MODEL, D_MODEL)\n",
        "\n",
        "        self.dropout = nn.Dropout(DROP_OUT_RATE)\n",
        "        self.attn_softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Final output linear transformation\n",
        "        self.w_0 = nn.Linear(D_MODEL, D_MODEL)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        input_shape = q.shape\n",
        "\n",
        "        # Linear calculation +  split into num_heads\n",
        "        q = self.w_q(q).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "        k = self.w_k(k).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "        v = self.w_v(v).view(input_shape[0], -1, NUM_HEADS, D_K) # (B, L, num_heads, d_k)\n",
        "\n",
        "        # For convenience, convert all tensors in size (B, num_heads, L, d_k)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Conduct self-attention\n",
        "        attn_values = self.self_attention(q, k, v, mask=mask) # (B, num_heads, L, d_k)\n",
        "        concat_output = attn_values.transpose(1, 2)\\\n",
        "            .contiguous().view(input_shape[0], -1, D_MODEL) # (B, L, d_model)\n",
        "\n",
        "        return self.w_0(concat_output)\n",
        "\n",
        "    def self_attention(self, q, k, v, mask=None):\n",
        "        # Calculate attention scores with scaled dot-product attention\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) # (B, num_heads, L, L)\n",
        "        attn_scores = attn_scores / math.sqrt(D_K)\n",
        "\n",
        "        # If there is a mask, make masked spots -INF\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1) # (B, 1, L) => (B, 1, 1, L) or (B, L, L) => (B, 1, L, L)\n",
        "            attn_scores = attn_scores.masked_fill_(mask == 0, -1 * self.inf)\n",
        "\n",
        "        # Softmax and multiplying K to calculate attention value\n",
        "        attn_distribs = self.attn_softmax(attn_scores)\n",
        "\n",
        "        attn_distribs = self.dropout(attn_distribs)\n",
        "        attn_values = torch.matmul(attn_distribs, v) # (B, num_heads, L, d_k)\n",
        "\n",
        "        return attn_values\n",
        "\n",
        "\n",
        "class FeedFowardLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(D_MODEL, D_FF, bias=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear_2 = nn.Linear(D_FF, D_MODEL, bias=True)\n",
        "        self.dropout = nn.Dropout(DROP_OUT_RATE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear_1(x)) # (B, L, d_ff)\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear_2(x) # (B, L, d_model)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.layer = nn.LayerNorm([D_MODEL], elementwise_affine=True, eps=self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Make initial positional encoding matrix with 0\n",
        "        pe_matrix= torch.zeros(SEQUENCE_LENGTH, D_MODEL) # (L, d_model)\n",
        "\n",
        "        # Calculating position encoding values\n",
        "        for pos in range(SEQUENCE_LENGTH):\n",
        "            for i in range(D_MODEL):\n",
        "                if i % 2 == 0:\n",
        "                    pe_matrix[pos, i] = math.sin(pos / (10000 ** (2 * i / D_MODEL)))\n",
        "                elif i % 2 == 1:\n",
        "                    pe_matrix[pos, i] = math.cos(pos / (10000 ** (2 * i / D_MODEL)))\n",
        "\n",
        "        pe_matrix = pe_matrix.unsqueeze(0) # (1, L, d_model)\n",
        "        self.positional_encoding = pe_matrix.to(device=DEVICE).requires_grad_(False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x * math.sqrt(D_MODEL) # (B, L, d_model)\n",
        "        x = x + self.positional_encoding # (B, L, d_model)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "66OApnHysm3Q"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size):\n",
        "        super().__init__()\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.trg_vocab_size = trg_vocab_size\n",
        "\n",
        "        self.src_embedding = nn.Embedding(self.src_vocab_size, D_MODEL)\n",
        "        self.trg_embedding = nn.Embedding(self.trg_vocab_size, D_MODEL)\n",
        "        self.positional_encoder = PositionalEncoder()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "        self.output_linear = nn.Linear(D_MODEL, self.trg_vocab_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, src_input, trg_input, e_mask=None, d_mask=None):\n",
        "        src_input = self.src_embedding(src_input) # (B, L) => (B, L, d_model)\n",
        "        trg_input = self.trg_embedding(trg_input) # (B, L) => (B, L, d_model)\n",
        "        src_input = self.positional_encoder(src_input) # (B, L, d_model) => (B, L, d_model)\n",
        "        trg_input = self.positional_encoder(trg_input) # (B, L, d_model) => (B, L, d_model)\n",
        "\n",
        "        e_output = self.encoder(src_input, e_mask) # (B, L, d_model)\n",
        "        d_output = self.decoder(trg_input, e_output, e_mask, d_mask) # (B, L, d_model)\n",
        "\n",
        "        output = self.softmax(self.output_linear(d_output)) # (B, L, d_model) => # (B, L, trg_vocab_size)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for i in range(NUM_LAYERS)])\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, e_mask):\n",
        "        for i in range(NUM_LAYERS):\n",
        "            x = self.layers[i](x, e_mask)\n",
        "\n",
        "        return self.layer_norm(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([DecoderLayer() for i in range(NUM_LAYERS)])\n",
        "        self.layer_norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, e_output, e_mask, d_mask):\n",
        "        for i in range(NUM_LAYERS):\n",
        "            x = self.layers[i](x, e_output, e_mask, d_mask)\n",
        "\n",
        "        return self.layer_norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dQyMnNdEspvi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class Manager():\n",
        "    def __init__(self, is_train=True, ckpt_name=None):\n",
        "\n",
        "        # Load Transformer model & Adam optimizer\n",
        "        print(\"Loading Transformer model & Adam optimizer...\")\n",
        "        self.model = Transformer(src_vocab_size=SRC_VOCAB_SIZE, trg_vocab_size=TGT_VOCAB_SIZE).to(DEVICE)\n",
        "        self.optim = torch.optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
        "        self.best_loss = sys.float_info.max\n",
        "\n",
        "        if ckpt_name is not None:\n",
        "            assert os.path.exists(f\"{CHECKPOINT_DIR}/{ckpt_name}\"), f\"There is no checkpoint named {ckpt_name}.\"\n",
        "\n",
        "            print(\"Loading checkpoint...\")\n",
        "            checkpoint = torch.load(f\"{CHECKPOINT_DIR}/{ckpt_name}\")\n",
        "            self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.optim.load_state_dict(checkpoint['optim_state_dict'])\n",
        "            self.best_loss = checkpoint['loss']\n",
        "        else:\n",
        "            print(\"Initializing the model...\")\n",
        "            for p in self.model.parameters():\n",
        "                if p.dim() > 1:\n",
        "                    nn.init.xavier_uniform_(p)\n",
        "\n",
        "        if is_train:\n",
        "            # Load loss function\n",
        "            print(\"Loading loss function...\")\n",
        "            self.criterion = nn.NLLLoss()\n",
        "\n",
        "            # Load dataloaders\n",
        "            print(\"Loading dataloaders...\")\n",
        "            self.train_loader = get_data_loader(TRAIN_NAME)\n",
        "            self.valid_loader = get_data_loader(VALID_NAME)\n",
        "\n",
        "        print(\"Setting finished.\")\n",
        "\n",
        "    def train(self):\n",
        "        print(\"Training starts.\")\n",
        "\n",
        "        for epoch in range(1, NUM_EPOCHS+1):\n",
        "            self.model.train()\n",
        "\n",
        "            train_losses = []\n",
        "            start_time = datetime.datetime.now()\n",
        "\n",
        "            for i, batch in tqdm(enumerate(self.train_loader)):\n",
        "                src_input, trg_input, trg_output = batch\n",
        "                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n",
        "\n",
        "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
        "\n",
        "                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n",
        "\n",
        "                trg_output_shape = trg_output.shape\n",
        "                self.optim.zero_grad()\n",
        "                loss = self.criterion(\n",
        "                    output.view(-1, OUTPUT_VOCAB_SIZE),\n",
        "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n",
        "                )\n",
        "\n",
        "                loss.backward()\n",
        "                self.optim.step()\n",
        "\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            end_time = datetime.datetime.now()\n",
        "            training_time = end_time - start_time\n",
        "            seconds = training_time.seconds\n",
        "            hours = seconds // 3600\n",
        "            minutes = (seconds % 3600) // 60\n",
        "            seconds = seconds % 60\n",
        "\n",
        "            mean_train_loss = np.mean(train_losses)\n",
        "            print(f\"#################### Epoch: {epoch} ####################\")\n",
        "            print(f\"Train loss: {mean_train_loss} || One epoch training time: {hours}hrs {minutes}mins {seconds}secs\")\n",
        "\n",
        "            valid_loss, valid_time = self.validation()\n",
        "\n",
        "            if valid_loss < self.best_loss:\n",
        "                if not os.path.exists(CHECKPOINT_DIR):\n",
        "                    os.mkdir(CHECKPOINT_DIR)\n",
        "\n",
        "                self.best_loss = valid_loss\n",
        "                state_dict = {\n",
        "                    'model_state_dict': self.model.state_dict(),\n",
        "                    'optim_state_dict': self.optim.state_dict(),\n",
        "                    'loss': self.best_loss\n",
        "                }\n",
        "                torch.save(state_dict, f\"{CHECKPOINT_DIR}/best_ckpt.tar\")\n",
        "                print(f\"***** Current best checkpoint is saved. *****\")\n",
        "\n",
        "            print(f\"Best valid loss: {self.best_loss}\")\n",
        "            print(f\"Valid loss: {valid_loss} || One epoch training time: {valid_time}\")\n",
        "\n",
        "        print(f\"Training finished!\")\n",
        "\n",
        "    def validation(self):\n",
        "        print(\"Validation processing...\")\n",
        "        self.model.eval()\n",
        "\n",
        "        valid_losses = []\n",
        "        start_time = datetime.datetime.now()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in tqdm(enumerate(self.valid_loader)):\n",
        "                src_input, trg_input, trg_output = batch\n",
        "                src_input, trg_input, trg_output = src_input.to(DEVICE), trg_input.to(DEVICE), trg_output.to(DEVICE)\n",
        "\n",
        "                e_mask, d_mask = self.make_mask(src_input, trg_input)\n",
        "\n",
        "                output = self.model(src_input, trg_input, e_mask, d_mask) # (B, L, vocab_size)\n",
        "\n",
        "                trg_output_shape = trg_output.shape\n",
        "                loss = self.criterion(\n",
        "                    output.view(-1, OUTPUT_VOCAB_SIZE),\n",
        "                    trg_output.view(trg_output_shape[0] * trg_output_shape[1])\n",
        "                )\n",
        "\n",
        "                valid_losses.append(loss.item())\n",
        "\n",
        "                del src_input, trg_input, trg_output, e_mask, d_mask, output\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        end_time = datetime.datetime.now()\n",
        "        validation_time = end_time - start_time\n",
        "        seconds = validation_time.seconds\n",
        "        hours = seconds // 3600\n",
        "        minutes = (seconds % 3600) // 60\n",
        "        seconds = seconds % 60\n",
        "\n",
        "        mean_valid_loss = np.mean(valid_losses)\n",
        "\n",
        "        return mean_valid_loss, f\"{hours}hrs {minutes}mins {seconds}secs\"\n",
        "\n",
        "    def make_mask(self, src_input, trg_input):\n",
        "        e_mask = (src_input != SRC_PAD_IDX).unsqueeze(1)  # (B, 1, L)\n",
        "        d_mask = (trg_input != TGT_PAD_IDX).unsqueeze(1)  # (B, 1, L)\n",
        "\n",
        "        nopeak_mask = torch.ones([1, SEQUENCE_LENGTH, SEQUENCE_LENGTH], dtype=torch.bool)  # (1, L, L)\n",
        "        nopeak_mask = torch.tril(nopeak_mask).to(DEVICE)  # (1, L, L) to triangular shape\n",
        "        d_mask = d_mask & nopeak_mask  # (B, L, L) padding false\n",
        "\n",
        "        return e_mask, d_mask\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "id": "Skp03PnOssFT",
        "outputId": "35d83bf6-a4f7-4e0b-c57c-5b8188bfaf32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Transformer model & Adam optimizer...\n",
            "Initializing the model...\n",
            "Loading loss function...\n",
            "Loading dataloaders...\n",
            "Getting source/target iwslt2017-en-zh-train...\n",
            "Tokenizing & Padding src data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 15261.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of src data: (10000, 296)\n",
            "Tokenizing & Padding trg data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 26818.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of input trg data: (10000, 296)\n",
            "The shape of output trg data: (10000, 296)\n",
            "Getting source/target iwslt2017-en-zh-validation...\n",
            "Tokenizing & Padding src data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 879/879 [00:00<00:00, 13576.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of src data: (879, 296)\n",
            "Tokenizing & Padding trg data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 879/879 [00:00<00:00, 24601.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape of input trg data: (879, 296)\n",
            "The shape of output trg data: (879, 296)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting finished.\n",
            "Training starts.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "67it [00:23,  2.91it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1fcc2376f8e8>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmanager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-4177dae59cbb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0msrc_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "if CHKPT_NAME is not None:\n",
        "    manager = Manager(is_train=True, ckpt_name=CHKPT_NAME)\n",
        "else:\n",
        "    manager = Manager(is_train=True)\n",
        "\n",
        "manager.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epMVdu-ns6S_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

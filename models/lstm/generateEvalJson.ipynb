{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model, Model\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras import optimizers\n",
    "from transformers import MarianTokenizer\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"iwslt2017\", \"iwslt2017-en-zh\")\n",
    "train_ds, valid_ds, test_ds = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")\n",
    "\n",
    "# first 10k rows\n",
    "train_ds = train_ds.select(range(10000))\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_zh_test = [[data['translation']['en'], data['translation']['zh']] for data in test_ds]\n",
    "en_test = [pair[0] for pair in en_zh_test]\n",
    "zh_test = [pair[1] for pair in en_zh_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from typing import List, Union\n",
    "\n",
    "class LangTokeniser(object):\n",
    "    PAD_ID = 3  # Defined as sentencepiece custom token\n",
    "\n",
    "    def __init__(self, lang: str, model_file=None):\n",
    "        self.model = spm.SentencePieceProcessor(model_file=model_file or f\"./{lang}.model\")\n",
    "        self.special_ids = (\n",
    "            self.model.unk_id(),\n",
    "            LangTokeniser.PAD_ID,  # self.model.pad_id(), # this is -1 and may give errors.\n",
    "            self.model.bos_id(),\n",
    "            self.model.eos_id(),\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.model)\n",
    "    \n",
    "    def encode_no_padding(self, sent: Union[str, List[str]], max_len=None):\n",
    "        ids = self.model.encode(sent)\n",
    "        if max_len is not None and len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "        return ids\n",
    "\n",
    "    def encode_batch(self, sents: List[str], max_len=None):\n",
    "        return [self.encode(sent, max_len) for sent in sents]\n",
    "\n",
    "    def encode(self, sent: Union[str, List[str]], max_len=None):\n",
    "        if isinstance(sent, list):\n",
    "            return self.encode_batch(sent, max_len)\n",
    "        ids = self.model.encode(sent)\n",
    "        if max_len is not None:\n",
    "            if len(ids) < max_len:\n",
    "                ids.extend([LangTokeniser.PAD_ID] * (max_len - len(ids)))\n",
    "            elif len(ids) > max_len:\n",
    "                ids = ids[:max_len]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]):\n",
    "        return self.model.decode([id for id in ids if 0 <= id < len(self) and id != LangTokeniser.PAD_ID])\n",
    "\n",
    "    def decode_batch(self, ids: List[List[int]]):\n",
    "        return [self.decode(id) for id in ids]\n",
    "\n",
    "    def get_special_ids(self):\n",
    "        UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = self.special_ids\n",
    "        return UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX\n",
    "\n",
    "\n",
    "class BaseBPETokeniser(object):\n",
    "    \"\"\"\n",
    "    The class to tokenise input English sentences, and decode output Chinese Vocab IDs.\n",
    "\n",
    "    Examples:\n",
    "    ```py\n",
    "    from tokenisation.sentencepiece_custom import BaseBPETokeniser\n",
    "\n",
    "    tokeniser = BaseBPETokeniser()\n",
    "    # or initialise with the model files in a separate path:\n",
    "    tokeniser = BaseBPETokeniser(en_model_file=\"/path/to/en.model\", zh_model_file=\"/path/to/zh.model\")\n",
    "\n",
    "    row = dataset[0]['translation']\n",
    "\n",
    "    # Tokenise and truncate to max length of 512 for both.\n",
    "    inputs = tokeniser(row['en'], text_target=row['zh'], max_len=512)\n",
    "    # {\n",
    "    #     'input_ids': [...],       # The English IDs\n",
    "    #     'attention_mask': [...],\n",
    "    #     'labels': [...]           # The Chinese IDs\n",
    "    # }\n",
    "\n",
    "    # should generate the Chinese tokens output.\n",
    "    translated = tokeniser.decode(ids)\n",
    "\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, en_model_file=None, zh_model_file=None):\n",
    "        self.en_model = LangTokeniser(\"en\", model_file=en_model_file)\n",
    "        self.zh_model = LangTokeniser(\"zh\", model_file=zh_model_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Both the english and chinese tokenisers have the same length.\n",
    "        \"\"\"\n",
    "        return len(self.en_model)\n",
    "\n",
    "    def __call__(self, sent: str, text_target=None, max_len=128, max_zh_len=None):\n",
    "        out = {\n",
    "            \"input_ids\": self.en_model.encode(sent, max_len=max_len),\n",
    "            \"attention_mask\": [1] * max_len,\n",
    "        }\n",
    "        if text_target:\n",
    "            out[\"labels\"] = self.zh_model.encode(\n",
    "                text_target, max_len=max_zh_len or max_len\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    def encode_zh(self, sent: str, max_len=128):\n",
    "        return self.zh_model.encode(sent, max_len=max_len)\n",
    "\n",
    "    def encode_en(self, sent: str, max_len=128):\n",
    "        return self.en_model.encode(sent, max_len=max_len)\n",
    "    \n",
    "    def decode_zh(self, labels: list[int]):\n",
    "        return self.zh_model.decode(labels)\n",
    "\n",
    "    def decode_zh_batch(self, labels: List[List[int]]):\n",
    "        return self.zh_model.decode_batch(labels)\n",
    "    \n",
    "    def decode_en(self, labels: list[int]):\n",
    "        return self.en_model.decode(labels)\n",
    "\n",
    "    def decode_en_batch(self, labels: list[int]):\n",
    "        return self.en_model.decode_batch(labels)\n",
    "    \n",
    "    def get_special_ids(self, lang: str):\n",
    "        if lang == \"en\":\n",
    "            return self.en_model.get_special_ids()\n",
    "        elif lang == \"zh\":\n",
    "            return self.zh_model.get_special_ids()\n",
    "\n",
    "    def encode_en_no_padding(self, sent: str, max_len=None):\n",
    "        return self.en_model.encode_no_padding(sent, max_len=max_len)\n",
    "\n",
    "    def encode_zh_no_padding(self, sent: str, max_len=None):\n",
    "        return self.zh_model.encode_no_padding(sent, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model_absolute_path = os.path.abspath('../../tokenisation/sentencepiece_custom/en.model')\n",
    "zh_model_absolute_path = os.path.abspath('../../tokenisation/sentencepiece_custom/zh.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BaseBPETokeniser(en_model_file=en_model_absolute_path, zh_model_file=zh_model_absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the en vocab size\n",
    "def read_vocab_file(vocab_file_path):\n",
    "    vocab = {}\n",
    "    with open(vocab_file_path, 'r', encoding='utf-8') as f:\n",
    "        index = 0\n",
    "        for line in f:\n",
    "            token, ignore = line.strip().split()  # Assuming tokens and indices are separated by space\n",
    "            vocab[token] = index \n",
    "            index += 1\n",
    "    return vocab\n",
    "# retrieve en vocab\n",
    "en_vocab_file = \"../../tokenisation/sentencepiece_custom/en.vocab\"\n",
    "en_vocab = read_vocab_file(en_vocab_file)\n",
    "# retrieve zh vocab\n",
    "zh_vocab_file = \"../../tokenisation/sentencepiece_custom/zh.vocab\"\n",
    "zh_vocab = read_vocab_file(zh_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 133)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode english\n",
    "max_len_en = len(max(tokenizer.encode_en_no_padding(en_test), key=len))\n",
    "# encode chinese\n",
    "max_len_zh = len(max(tokenizer.encode_zh_no_padding(zh_test), key=len))\n",
    "max_len_en, max_len_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode english\n",
    "en_outputs = tokenizer.encode_en(en_test, max_len=211)\n",
    "# encode chinese\n",
    "zh_outputs = tokenizer.encode_zh(zh_test, max_len=285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel = load_model(r\"C:\\Users\\glenl\\OneDrive - National University of Singapore\\Documents\\NUS\\Current semester\\CS4248\\4248project\\models\\lstm\\savedModels\\1712672287.5377033_model.l5.07.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The english sentence is: Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow challenge.\n",
      "The chinese sentence is: 几年前，在TED大会上， Peter Skillman 介绍了一个设计挑战 叫做“棉花糖挑战”\n",
      "The predicted sentence is :\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "▁And o ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "def logits_to_sentence(logits, vocab):\n",
    "\n",
    "    index_to_words = {idx: word for word, idx in vocab.items()}\n",
    "#     index_to_words[0] = '<empty>' \n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in argmax(logits, 1)])\n",
    "\n",
    "index = 0\n",
    "print(\"The english sentence is: {}\".format(en_test[index]))\n",
    "print(\"The chinese sentence is: {}\".format(zh_test[index]))\n",
    "print('The predicted sentence is :')\n",
    "print(logits_to_sentence(trainedModel.predict(array(en_outputs[index:index+1]))[0], zh_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn out predictions from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n"
     ]
    }
   ],
   "source": [
    "results = logits_to_sentence(trainedModel.predict(array(en_outputs[index:index+1])) [0], zh_vocab)\n",
    "# results = logits_to_sentence(trainedModel.predict(array(en_outputs[index:index+1]))[0], zh_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS2109S",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

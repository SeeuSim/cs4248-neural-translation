{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from numpy import array, argmax, random, take\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Input, TimeDistributed, Dense, Activation, RepeatVector, Embedding\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model, Model\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras import optimizers\n",
    "from transformers import MarianTokenizer\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"iwslt2017\", \"iwslt2017-en-zh\")\n",
    "train_ds, valid_ds, test_ds = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")\n",
    "\n",
    "# first 10k rows\n",
    "train_ds = train_ds.select(range(10000))\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_zh_test = [[data['translation']['en'], data['translation']['zh']] for data in test_ds]\n",
    "en_test = [pair[0] for pair in en_zh_test]\n",
    "zh_test = [pair[1] for pair in en_zh_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from typing import List, Union\n",
    "\n",
    "class LangTokeniser(object):\n",
    "    PAD_ID = 3  # Defined as sentencepiece custom token\n",
    "\n",
    "    def __init__(self, lang: str, model_file=None):\n",
    "        self.model = spm.SentencePieceProcessor(model_file=model_file or f\"./{lang}.model\")\n",
    "        self.special_ids = (\n",
    "            self.model.unk_id(),\n",
    "            LangTokeniser.PAD_ID,  # self.model.pad_id(), # this is -1 and may give errors.\n",
    "            self.model.bos_id(),\n",
    "            self.model.eos_id(),\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.model)\n",
    "    \n",
    "    def encode_no_padding(self, sent: Union[str, List[str]], max_len=None):\n",
    "        ids = self.model.encode(sent)\n",
    "        if max_len is not None and len(ids) > max_len:\n",
    "            ids = ids[:max_len]\n",
    "        return ids\n",
    "\n",
    "    def encode_batch(self, sents: List[str], max_len=None):\n",
    "        return [self.encode(sent, max_len) for sent in sents]\n",
    "\n",
    "    def encode(self, sent: Union[str, List[str]], max_len=None):\n",
    "        if isinstance(sent, list):\n",
    "            return self.encode_batch(sent, max_len)\n",
    "        ids = self.model.encode(sent)\n",
    "        if max_len is not None:\n",
    "            if len(ids) < max_len:\n",
    "                ids.extend([LangTokeniser.PAD_ID] * (max_len - len(ids)))\n",
    "            elif len(ids) > max_len:\n",
    "                ids = ids[:max_len]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids: List[int]):\n",
    "        return self.model.decode([id for id in ids if 0 <= id < len(self) and id != LangTokeniser.PAD_ID])\n",
    "\n",
    "    def decode_batch(self, ids: List[List[int]]):\n",
    "        return [self.decode(id) for id in ids]\n",
    "\n",
    "    def get_special_ids(self):\n",
    "        UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = self.special_ids\n",
    "        return UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX\n",
    "\n",
    "\n",
    "class BaseBPETokeniser(object):\n",
    "    \"\"\"\n",
    "    The class to tokenise input English sentences, and decode output Chinese Vocab IDs.\n",
    "\n",
    "    Examples:\n",
    "    ```py\n",
    "    from tokenisation.sentencepiece_custom import BaseBPETokeniser\n",
    "\n",
    "    tokeniser = BaseBPETokeniser()\n",
    "    # or initialise with the model files in a separate path:\n",
    "    tokeniser = BaseBPETokeniser(en_model_file=\"/path/to/en.model\", zh_model_file=\"/path/to/zh.model\")\n",
    "\n",
    "    row = dataset[0]['translation']\n",
    "\n",
    "    # Tokenise and truncate to max length of 512 for both.\n",
    "    inputs = tokeniser(row['en'], text_target=row['zh'], max_len=512)\n",
    "    # {\n",
    "    #     'input_ids': [...],       # The English IDs\n",
    "    #     'attention_mask': [...],\n",
    "    #     'labels': [...]           # The Chinese IDs\n",
    "    # }\n",
    "\n",
    "    # should generate the Chinese tokens output.\n",
    "    translated = tokeniser.decode(ids)\n",
    "\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, en_model_file=None, zh_model_file=None):\n",
    "        self.en_model = LangTokeniser(\"en\", model_file=en_model_file)\n",
    "        self.zh_model = LangTokeniser(\"zh\", model_file=zh_model_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Both the english and chinese tokenisers have the same length.\n",
    "        \"\"\"\n",
    "        return len(self.en_model)\n",
    "\n",
    "    def __call__(self, sent: str, text_target=None, max_len=128, max_zh_len=None):\n",
    "        out = {\n",
    "            \"input_ids\": self.en_model.encode(sent, max_len=max_len),\n",
    "            \"attention_mask\": [1] * max_len,\n",
    "        }\n",
    "        if text_target:\n",
    "            out[\"labels\"] = self.zh_model.encode(\n",
    "                text_target, max_len=max_zh_len or max_len\n",
    "            )\n",
    "        return out\n",
    "\n",
    "    def encode_zh(self, sent: str, max_len=128):\n",
    "        return self.zh_model.encode(sent, max_len=max_len)\n",
    "\n",
    "    def encode_en(self, sent: str, max_len=128):\n",
    "        return self.en_model.encode(sent, max_len=max_len)\n",
    "    \n",
    "    def decode_zh(self, labels: list[int]):\n",
    "        return self.zh_model.decode(labels)\n",
    "\n",
    "    def decode_zh_batch(self, labels: List[List[int]]):\n",
    "        return self.zh_model.decode_batch(labels)\n",
    "    \n",
    "    def decode_en(self, labels: list[int]):\n",
    "        return self.en_model.decode(labels)\n",
    "\n",
    "    def decode_en_batch(self, labels: list[int]):\n",
    "        return self.en_model.decode_batch(labels)\n",
    "    \n",
    "    def get_special_ids(self, lang: str):\n",
    "        if lang == \"en\":\n",
    "            return self.en_model.get_special_ids()\n",
    "        elif lang == \"zh\":\n",
    "            return self.zh_model.get_special_ids()\n",
    "\n",
    "    def encode_en_no_padding(self, sent: str, max_len=None):\n",
    "        return self.en_model.encode_no_padding(sent, max_len=max_len)\n",
    "\n",
    "    def encode_zh_no_padding(self, sent: str, max_len=None):\n",
    "        return self.zh_model.encode_no_padding(sent, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model_absolute_path = os.path.abspath('../../tokenisation/sentencepiece_custom/en.model')\n",
    "zh_model_absolute_path = os.path.abspath('../../tokenisation/sentencepiece_custom/zh.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BaseBPETokeniser(en_model_file=en_model_absolute_path, zh_model_file=zh_model_absolute_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the en vocab size\n",
    "def read_vocab_file(vocab_file_path):\n",
    "    vocab = {}\n",
    "    with open(vocab_file_path, 'r', encoding='utf-8') as f:\n",
    "        index = 0\n",
    "        for line in f:\n",
    "            token, ignore = line.strip().split()  # Assuming tokens and indices are separated by space\n",
    "            vocab[token] = index \n",
    "            index += 1\n",
    "    return vocab\n",
    "# retrieve en vocab\n",
    "en_vocab_file = \"../../tokenisation/sentencepiece_custom/en.vocab\"\n",
    "en_vocab = read_vocab_file(en_vocab_file)\n",
    "# retrieve zh vocab\n",
    "zh_vocab_file = \"../../tokenisation/sentencepiece_custom/zh.vocab\"\n",
    "zh_vocab = read_vocab_file(zh_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 133)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode english\n",
    "max_len_en = len(max(tokenizer.encode_en_no_padding(en_test), key=len))\n",
    "# encode chinese\n",
    "max_len_zh = len(max(tokenizer.encode_zh_no_padding(zh_test), key=len))\n",
    "max_len_en, max_len_zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode english\n",
    "en_outputs = tokenizer.encode_en(en_test, max_len=211)\n",
    "# encode chinese\n",
    "zh_outputs = tokenizer.encode_zh(zh_test, max_len=285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedModel = load_model(r\"C:\\Users\\glenl\\OneDrive - National University of Singapore\\Documents\\NUS\\Current semester\\CS4248\\4248project\\models\\lstm\\savedModels\\1712672287.5377033_model.l5.07.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zh_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(en_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n"
     ]
    }
   ],
   "source": [
    "a = trainedModel.predict(array(en_outputs[index:index+5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(285, 16384)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainedModel.predict(array(en_outputs[index:index+2]))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 285)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = argmax(a, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import vectorize\n",
    "\n",
    "def logits_to_sentence(logits, vocab):\n",
    "    index_to_words = {idx: word for word, idx in vocab.items()}\n",
    "#     index_to_words[0] = '<empty>' \n",
    "    bestIndices = argmax(logits, 2)\n",
    "    chars_arr = vectorize(index_to_words.get)(bestIndices)\n",
    "    sents_arr = array([' '.join(sublist) for sublist in chars_arr])\n",
    "    return sents_arr.tolist()\n",
    "\n",
    "# index = 0\n",
    "# print(\"The english sentence is: {}\".format(en_test[index]))\n",
    "# print(\"The chinese sentence is: {}\".format(zh_test[index]))\n",
    "# print('The predicted sentence is :')\n",
    "# print(logits_to_sentence(trainedModel.predict(array(en_outputs[index:index+1])), zh_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8549"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_outputs[0:len(en_outputs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Churn out predictions from test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 49/268\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12:24\u001b[0m 3s/step"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node functional_1_1/time_distributed_1/transpose_1 defined at (most recent call last):\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n\n  File \"C:\\Users\\glenl\\AppData\\Local\\Temp\\ipykernel_11232\\584290947.py\", line 1, in <cell line: 1>\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 515, in predict\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 213, in one_step_on_data_distributed\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 202, in one_step_on_data\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 94, in predict_step\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\layer.py\", line 814, in __call__\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\ops\\operation.py\", line 48, in __call__\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\models\\functional.py\", line 194, in call\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\ops\\function.py\", line 151, in _run_through_graph\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\models\\functional.py\", line 578, in call\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\layer.py\", line 814, in __call__\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\ops\\operation.py\", line 48, in __call__\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\rnn\\time_distributed.py\", line 110, in call\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\rnn\\time_distributed.py\", line 90, in time_distributed_transpose\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\ops\\numpy.py\", line 5809, in transpose\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 1946, in transpose\n\nOOM when allocating tensor with shape[32,285,16384] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node functional_1_1/time_distributed_1/transpose_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_data_distributed_5965]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m logits_to_sentence(\u001b[43mtrainedModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43men_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43men_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, zh_vocab)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node functional_1_1/time_distributed_1/transpose_1 defined at (most recent call last):\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\runpy.py\", line 197, in _run_module_as_main\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\runpy.py\", line 87, in _run_code\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\traitlets\\config\\application.py\", line 976, in launch_instance\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\asyncio\\base_events.py\", line 596, in run_forever\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\asyncio\\base_events.py\", line 1890, in _run_once\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_cell\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2936, in _run_cell\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3135, in run_cell_async\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n\n  File \"c:\\Users\\glenl\\anaconda3\\envs\\CS2109S\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3398, in run_code\n\n  File \"C:\\Users\\glenl\\AppData\\Local\\Temp\\ipykernel_11232\\584290947.py\", line 1, in <cell line: 1>\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 515, in predict\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 213, in one_step_on_data_distributed\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 202, in one_step_on_data\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 94, in predict_step\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\layer.py\", line 814, in __call__\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\ops\\operation.py\", line 48, in __call__\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\models\\functional.py\", line 194, in call\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\ops\\function.py\", line 151, in _run_through_graph\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\models\\functional.py\", line 578, in call\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\layer.py\", line 814, in __call__\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\ops\\operation.py\", line 48, in __call__\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\rnn\\time_distributed.py\", line 110, in call\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\rnn\\time_distributed.py\", line 90, in time_distributed_transpose\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\ops\\numpy.py\", line 5809, in transpose\n\n  File \"C:\\Users\\glenl\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\", line 1946, in transpose\n\nOOM when allocating tensor with shape[32,285,16384] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator mklcpu\n\t [[{{node functional_1_1/time_distributed_1/transpose_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_one_step_on_data_distributed_5965]"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "results = logits_to_sentence(trainedModel.predict(array(en_outputs[index:len(en_outputs)])), zh_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS2109S",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

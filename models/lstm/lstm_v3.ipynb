{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install -q tfds-nightly tensorflow matplotlib datasets\n",
    "\n",
    "%pip install nltk\n",
    "%pip install contractions\n",
    "%pip install spacy\n",
    "%pip install SentencePiece \n",
    "%pip install transformers==4.26.1"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2eedeb77cd88e2fc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\kelvi\\lib\\site-packages (2.15.0)\n",
      "Collecting keras\n",
      "  Downloading keras-3.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\kelvi\\lib\\site-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kelvi\\lib\\site-packages (from keras) (1.26.4)\n",
      "Collecting rich (from keras)\n",
      "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras)\n",
      "  Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Requirement already satisfied: h5py in c:\\users\\kelvi\\lib\\site-packages (from keras) (3.10.0)\n",
      "Collecting optree (from keras)\n",
      "  Downloading optree-0.11.0-cp39-cp39-win_amd64.whl.metadata (46 kB)\n",
      "     ---------------------------------------- 0.0/46.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 46.2/46.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\kelvi\\lib\\site-packages (from keras) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\kelvi\\lib\\site-packages (from optree->keras) (4.10.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kelvi\\lib\\site-packages (from rich->keras) (2.14.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading keras-3.1.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 33.8 MB/s eta 0:00:00\n",
      "Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.11.0-cp39-cp39-win_amd64.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 240.0/240.0 kB 14.4 MB/s eta 0:00:00\n",
      "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 240.7/240.7 kB 14.4 MB/s eta 0:00:00\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.5/87.5 kB 5.2 MB/s eta 0:00:00\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, optree, mdurl, markdown-it-py, rich, keras\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "Successfully installed keras-3.1.1 markdown-it-py-3.0.0 mdurl-0.1.2 namex-0.0.7 optree-0.11.0 rich-13.7.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "import contractions\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Embedding, Input"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(\"Libraries imported\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b623f2e95d73a8b",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelvi\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builders\\huggingface_dataset_builder.py:160: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  hf_names = hf_datasets.list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Structure:\n",
      "FeaturesDict({\n",
      "    'translation': Translation({\n",
      "        'en': Text(shape=(), dtype=string),\n",
      "        'zh': Text(shape=(), dtype=string),\n",
      "    }),\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "(train_ds, test_ds, val_ds), metadata = tfds.load('huggingface:iwslt2017/iwslt2017-en-zh', split=['train', 'test', 'validation'], with_info=True)\n",
    "\n",
    "full_ds = train_ds.concatenate(test_ds).concatenate(val_ds)\n",
    "\n",
    "print(\"Data Structure:\")\n",
    "print(metadata.features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T11:25:39.493304Z",
     "start_time": "2024-04-03T11:25:39.317394Z"
    }
   },
   "id": "eaeefa9282ebd07",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 231266\n",
      "Test: 8549\n",
      "Validation: 879\n",
      "Total: 240694\n"
     ]
    }
   ],
   "source": [
    "print(\"%s%s\" % (\"Train: \", train_ds.cardinality().numpy()))\n",
    "print(\"%s%s\" % (\"Test: \", test_ds.cardinality().numpy()))\n",
    "print(\"%s%s\" % (\"Validation: \", val_ds.cardinality().numpy()))\n",
    "print(\"%s%s\" % (\"Total: \", full_ds.cardinality().numpy()))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37b5653327e7aff8",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lists created\n",
      "Size: 231266\n"
     ]
    }
   ],
   "source": [
    "# Split en and zh\n",
    "texts_en = list()\n",
    "texts_zh = list()\n",
    "for elm in train_ds:\n",
    "    texts_en.append(elm['translation']['en'].numpy().decode(\"utf-8\"))\n",
    "    texts_zh.append(elm['translation']['zh'].numpy().decode(\"utf-8\"))\n",
    "\n",
    "print(\"Lists created\")\n",
    "print(\"%s%s\" % (\"Size: \", train_ds.cardinality().numpy()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenizer stuff"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "839c0f5363208cba"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>cmn<< We experience the world through a sequence of patterns, and we store them, and we recall them. And when we recall them, we match them up against reality, and we're making predictions all the time.\n",
      "我们通过一系列的模式来感受世界环境，然后贮存， 再回想，当我们回想时，我们会比较和对应 实际情况，就这样我们不断的推测\n"
     ]
    }
   ],
   "source": [
    "sample_txt_en = texts_en[0]\n",
    "sample_txt_zh = texts_zh[0]\n",
    "print(sample_txt_en)\n",
    "print(sample_txt_zh)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kelvi\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianModel\n",
    "\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "print(tokenizer.supported_language_codes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFMarianMTModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mTFMarianMTModel\u001B[49m\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name)\n\u001B[0;32m      2\u001B[0m translated \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mgenerate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtokenizer(sample_txt_en, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[0;32m      3\u001B[0m [tokenizer\u001B[38;5;241m.\u001B[39mdecode(t, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m translated]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'TFMarianMTModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = MarianModel.from_pretrained(model_name)\n",
    "translated = model.generate(**tokenizer(sample_txt_en, return_tensors=\"pt\", padding=True))\n",
    "[tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>cmn<< We experience the world through a sequence of patterns, and we store them, and we recall them. And when we recall them, we match them up against reality, and we're making predictions all the time.\n",
      "['>>cmn<<', '▁We', '▁experience', '▁the', '▁world', '▁through', '▁a', '▁sequence', '▁of', '▁patterns', ',', '▁and', '▁we', '▁store', '▁them', ',', '▁and', '▁we', '▁recall', '▁them', '.', '▁And', '▁when', '▁we', '▁recall', '▁them', ',', '▁we', '▁match', '▁them', '▁up', '▁against', '▁reality', ',', '▁and', '▁we', \"'\", 're', '▁making', '▁prediction', 's', '▁all', '▁the', '▁time', '.']\n",
      "我们通过一系列的模式来感受世界环境，然后贮存， 再回想，当我们回想时，我们会比较和对应 实际情况，就这样我们不断的推测\n",
      "['▁', '我们通过一系列的模式来感受世界环境', ',', '然后贮存', ',', '▁', '再回想', ',', '当我们回想时', ',', '我们会比较和对应', '▁', '实际情况', ',', '就这样我们不断的推测']\n"
     ]
    }
   ],
   "source": [
    "print(sample_txt_en)\n",
    "print(str(tokenizer.tokenize(sample_txt_en)))\n",
    "print(sample_txt_zh)\n",
    "print(str(tokenizer.tokenize(sample_txt_zh)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ca7b19dfa5ddfa8",
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    return seq"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def lstm(input_shape):\n",
    "\n",
    "    X_indices = Input(input_shape)\n",
    "\n",
    "    embeddings = Embedding(input_dim = vocab_size, output_dim = embed_vector_len,\n",
    "                           input_length = maxLen, weights = [embedding_matrix], trainable = False)(X_indices)\n",
    "\n",
    "    X = LSTM(128)(embeddings)\n",
    "\n",
    "    # X = Dropout(0.6)(X)\n",
    "\n",
    "    # X = LSTM(128, return_sequences = True)(X)\n",
    "\n",
    "    # X = Dropout(0.6)(X)\n",
    "\n",
    "    # X = LSTM(128)(X)\n",
    "\n",
    "    X = Dense(4, activation = 'softmax')(X)\n",
    "\n",
    "    model = Model(inputs = X_indices, outputs = X)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Experimental"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "zh_tokenizer = tokenizer\n",
    "en_tokenizer = tokenizer\n",
    "\n",
    "zh_length = len(sample_txt_zh)\n",
    "en_length = len(sample_txt_en)\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(zh_tokenizer, zh_length, train_ds[:, 1])\n",
    "trainY = encode_sequences(en_tokenizer, en_length, train_ds[:, 0])\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(zh_tokenizer, zh_length, test_ds[:, 1])\n",
    "testY = encode_sequences(en_tokenizer, en_length, test_ds[:, 0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# build NMT model\n",
    "def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps,units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model compilation\n",
    "model = define_model(zh_vocab_size, en_vocab_size, zh_length, en_length, 512)\n",
    "\n",
    "rms = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filename = 'iwslt-en-zh-lstm-v3'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# train model\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
    "                    epochs=30, batch_size=512, validation_split = 0.2,callbacks=[checkpoint],\n",
    "                    verbose=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "plot result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ignore"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "\n",
    "model = lstm((maxLen,   ))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Train the model on the training data\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "callback = EarlyStopping(monitor='loss', patience=3)\n",
    "history = model.fit(X, Y, epochs = epochs, batch_size = batch_size, validation_split = 0.1, callbacks = [callback])"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

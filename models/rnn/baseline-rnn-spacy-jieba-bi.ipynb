{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import torchtext\n",
    "import tqdm\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(\"Using\", device, \"device\")\n",
    "from transformers import MarianTokenizer \n",
    "spacy.prefer_gpu() # require_gpu raises an error if no GPU is avail \n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "import re\n",
    "import unicodedata\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from \"https://github.com/bentrevett/pytorch-seq2seq/blob/main/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 231266\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"iwslt2017\", \"iwslt2017-en-zh\")\n",
    "train_data, valid_data, test_data = (\n",
    "    dataset[\"train\"],\n",
    "    dataset[\"validation\"],\n",
    "    dataset[\"test\"],\n",
    ")\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_backup = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data = train_data.select(range(10000))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizers Spacy & Jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_pair(pair, max_length, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in nlp.tokenizer(pair['translation']['en'])][:max_length]\n",
    "    zh_tokens = list(jieba.cut(pair['translation']['zh'], cut_all=False))[:max_length] # if cut_all is True, it will use the full mode. Default is accurate mode.\n",
    "    en_tokens = [sos_token] + en_tokens +  [eos_token]\n",
    "    zh_tokens = [sos_token] + zh_tokens +  [eos_token]\n",
    "    return {\"en_tokens\": en_tokens, \"zh_tokens\": zh_tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8549/8549 [00:02<00:00, 3741.80 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['translation', 'en_tokens', 'zh_tokens'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max lengths from eda \n",
    "max_length_g = 249 # max of en and zh \n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"max_length\":max_length_g,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_pair, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_pair, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_pair, fn_kwargs=fn_kwargs)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 2\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = [\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "]\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "zh_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"zh_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', '的', ' ', '，', '。', '我', '是']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_vocab.get_itos()[:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert en_vocab[unk_token] == zh_vocab[unk_token]\n",
    "assert en_vocab[pad_token] == zh_vocab[pad_token]\n",
    "\n",
    "unk_index = en_vocab[unk_token]\n",
    "pad_index = en_vocab[pad_token]\n",
    "\n",
    "en_vocab.set_default_index(unk_index)\n",
    "zh_vocab.set_default_index(unk_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_vocab[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_example(example, en_vocab, zh_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    zh_ids = zh_vocab.lookup_indices(example[\"zh_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"zh_ids\": zh_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8549/8549 [00:00<00:00, 9393.73 examples/s]\n"
     ]
    }
   ],
   "source": [
    "fn_kwargs = {\"en_vocab\": en_vocab, \"zh_vocab\": zh_vocab}\n",
    "\n",
    "train_data = train_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_example, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 2, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['en_ids'][0] ,  train_data[0]['en_ids'][-1] , train_data[0]['zh_ids'][0] ,  train_data[0]['zh_ids'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\", \"zh_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type,\n",
    "    columns=format_columns,\n",
    "    output_all_columns=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " tensor([   2,   72,  389,    6,  973,    7,  722,   72, 3468,    5,   53,   17,\n",
       "         5259,  561,   11,   21, 3182,    4,  322,    6,    8,  892,    0,    7,\n",
       "            3]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0]['zh_ids']) , train_data[0]['zh_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_zh_ids = [example[\"zh_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_zh_ids = nn.utils.rnn.pad_sequence(batch_zh_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"zh_ids\": batch_zh_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional=False)\n",
    "        self.fc = nn.Linear(encoder_hidden_dim, decoder_hidden_dim) # encoder_hidden  * 2 if bidirectional!\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src length, batch size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded = [src length, batch size, embedding dim]\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs = [src length, batch size, hidden dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, hidden dim]\n",
    "        # hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        # outputs are always from the last layer\n",
    "        # hidden [-2, :, : ] is the last of the forwards RNN\n",
    "        # hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        # initial decoder hidden is final hidden state of the forwards and backwards\n",
    "        # encoder RNNs fed through a linear layer\n",
    "        # hidden = torch.tanh(\n",
    "        #     self.fc(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
    "        # )\n",
    "        hidden = torch.tanh(self.fc(hidden[-1, :, :]))\n",
    "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn_fc = nn.Linear(\n",
    "            (encoder_hidden_dim) + decoder_hidden_dim, decoder_hidden_dim # IF BIDIRECTIONAL MUST encoder_hidden_dim * 2 \n",
    "        )\n",
    "        self.v_fc = nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_length = encoder_outputs.shape[0]\n",
    "        # repeat decoder hidden state src_length times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_length, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # hidden = [batch size, src length, decoder hidden dim]\n",
    "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
    "        energy = torch.tanh(self.attn_fc(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy = [batch size, src length, decoder hidden dim]\n",
    "        attention = self.v_fc(energy).squeeze(2)\n",
    "        # attention = [batch size, src length]\n",
    "        return torch.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        output_dim,\n",
    "        embedding_dim,\n",
    "        encoder_hidden_dim,\n",
    "        decoder_hidden_dim,\n",
    "        dropout,\n",
    "        attention,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU((encoder_hidden_dim) + embedding_dim, decoder_hidden_dim) # IF BIDIRECTIONAL MUST encoder_hidden_dim * 2 \n",
    "        self.fc_out = nn.Linear(\n",
    "            (encoder_hidden_dim) + decoder_hidden_dim + embedding_dim, output_dim # IF BIDIRECTIONAL MUST encoder_hidden_dim * 2 \n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input = [batch size]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # encoder_outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded = [1, batch size, embedding dim]\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        # a = [batch size, src length]\n",
    "        a = a.unsqueeze(1)\n",
    "        # a = [batch size, 1, src length]\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        # encoder_outputs = [batch size, src length, encoder hidden dim * 2]\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        # weighted = [batch size, 1, encoder hidden dim * 2]\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        # weighted = [1, batch size, encoder hidden dim * 2]\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        # rnn_input = [1, batch size, (encoder hidden dim * 2) + embedding dim]\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        # output = [seq length, batch size, decoder hid dim * n directions]\n",
    "        # hidden = [n layers * n directions, batch size, decoder hid dim]\n",
    "        # seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        # output = [1, batch size, decoder hidden dim]\n",
    "        # hidden = [1, batch size, decoder hidden dim]\n",
    "        # this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        # prediction = [batch size, output dim]\n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        # teacher_forcing_ratio is probability to use teacher forcing\n",
    "        # e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        batch_size = src.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        # hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        # outputs = [src length, batch size, encoder hidden dim * 2]\n",
    "        # hidden = [batch size, decoder hidden dim]\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            # insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            # receive output tensor (predictions) and new hidden state\n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "            # output = [batch size, output dim]\n",
    "            # hidden = [n layers, batch size, decoder hidden dim]\n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher forcing, use actual next token as next input\n",
    "            # if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "            # input = [batch size]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(en_vocab)\n",
    "output_dim = len(zh_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "encoder_hidden_dim = 512\n",
    "decoder_hidden_dim = 512\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "attention = Attention(encoder_hidden_dim, decoder_hidden_dim)\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    encoder_hidden_dim,\n",
    "    decoder_hidden_dim,\n",
    "    encoder_dropout,\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    encoder_hidden_dim,\n",
    "    decoder_hidden_dim,\n",
    "    decoder_dropout,\n",
    "    attention,\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7066, 256)\n",
       "    (rnn): GRU(256, 512)\n",
       "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn_fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (v_fc): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(8229, 256)\n",
       "    (rnn): GRU(768, 512)\n",
       "    (fc_out): Linear(in_features=1280, out_features=8229, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 18,396,709 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "    model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device\n",
    "):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"en_ids\"].to(device)\n",
    "        trg = batch[\"zh_ids\"].to(device)\n",
    "        # src = [src length, batch size]\n",
    "        # trg = [trg length, batch size]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [trg length, batch size, trg vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(trg length - 1) * batch size]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"en_ids\"].to(device)\n",
    "            trg = batch[\"zh_ids\"].to(device)\n",
    "            # src = [src length, batch size]\n",
    "            # trg = [trg length, batch size]\n",
    "            output = model(src, trg, 0)  # turn off teacher forcing\n",
    "            # output = [trg length, batch size, trg vocab size]\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            # output = [(trg length - 1) * batch size, trg vocab size]\n",
    "            trg = trg[1:].view(-1)\n",
    "            # trg = [(trg length - 1) * batch size]\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [15:55<2:23:21, 955.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 1\n",
      "\tTrain Loss:   6.544 | Train PPL: 695.123\n",
      "\tValid Loss:   5.881 | Valid PPL: 358.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [35:48<2:26:02, 1095.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   6.016 | Train PPL: 410.093\n",
      "\tValid Loss:   5.900 | Valid PPL: 364.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [1:01:39<2:32:03, 1303.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 3\n",
      "\tTrain Loss:   5.870 | Train PPL: 354.359\n",
      "\tValid Loss:   5.876 | Valid PPL: 356.253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [1:23:43<2:11:08, 1311.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.749 | Train PPL: 313.782\n",
      "\tValid Loss:   5.884 | Valid PPL: 359.236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [1:45:48<1:49:42, 1316.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 5\n",
      "\tTrain Loss:   5.650 | Train PPL: 284.271\n",
      "\tValid Loss:   5.813 | Valid PPL: 334.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [2:08:44<1:29:06, 1336.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.558 | Train PPL: 259.368\n",
      "\tValid Loss:   5.822 | Valid PPL: 337.768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [2:31:08<1:06:56, 1338.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 7\n",
      "\tTrain Loss:   5.487 | Train PPL: 241.494\n",
      "\tValid Loss:   5.801 | Valid PPL: 330.530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [2:54:08<45:04, 1352.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at epoch 8\n",
      "\tTrain Loss:   5.376 | Train PPL: 216.165\n",
      "\tValid Loss:   5.787 | Valid PPL: 325.890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [3:16:18<22:25, 1345.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.254 | Train PPL: 191.348\n",
      "\tValid Loss:   5.815 | Valid PPL: 335.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [3:38:48<00:00, 1312.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.128 | Train PPL: 168.684\n",
      "\tValid Loss:   5.802 | Valid PPL: 330.829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "n_epochs = 10\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "best_valid_loss = float(\"inf\")\n",
    "CHECKPOINT_DIR = \"./model_checkpoints\"\n",
    "checkpoint_path = f\"{CHECKPOINT_DIR}/model_checkpoint.pt\"\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device,\n",
    "    )\n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device,\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save({'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': best_valid_loss}, f\"{CHECKPOINT_DIR}/model_checkpoint.pt\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train from best state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time % 60)\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint...\n",
      "Current best val loss : 5.78656142098563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [26:05<3:54:52, 1565.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 26m 5s\n",
      "\tTrain Loss: 5.268 | Train PPL: 194.093\n",
      "\t Valid Loss: 5.795 | Valid PPL: 328.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [52:22<7:51:20, 3142.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02 | Time: 26m 16s\n",
      "\tTrain Loss: 5.150 | Train PPL: 172.503\n",
      "\t Valid Loss: 5.814 | Valid PPL: 335.026\n",
      "No improvement in val loss for 2 consecutive epochs. Stopping training.\n",
      "Training completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time % 60)\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "patience = 2  \n",
    "no_improvement_epochs = 0\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(\"Loading checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    best_valid_loss = checkpoint['loss'] \n",
    "    print(f\"Current best val loss : {best_valid_loss}\")\n",
    "else:\n",
    "    print(\"Training started, no checkpoint found.\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    start_time = time.time()\n",
    "    train_loss = train_fn(model, train_data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device)\n",
    "    valid_loss = evaluate_fn(model, valid_data_loader, criterion, device)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': best_valid_loss}, f\"{CHECKPOINT_DIR}/model_checkpoint.pt\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1}\")\n",
    "        no_improvement_epochs = 0\n",
    "    else:\n",
    "        no_improvement_epochs += 1\n",
    "        \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Valid Loss: {valid_loss:.3f} | Valid PPL: {np.exp(valid_loss):7.3f}')\n",
    "    # Auto stop\n",
    "    if no_improvement_epochs >= patience:\n",
    "        print(f\"No improvement in val loss for {patience} consecutive epochs. Stopping training.\")\n",
    "        break\n",
    "            \n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 5.609 | Test PPL: 272.876 |\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(r\"model_checkpoints\\model_checkpoint.pt\")\n",
    "if 'model_state_dict' in checkpoint:\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  \n",
    "else:\n",
    "    model.load_state_dict(checkpoint) \n",
    "\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "print(f\"| Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):7.3f} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_vocab,\n",
    "    zh_vocab,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    "    max_output_length=max_length_g,\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            en_tokens =  [token.text for token in nlp.tokenizer(sentence)][:max_length_g]\n",
    "        else:\n",
    "            print('sentence is not an instance!')\n",
    "        en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "        ids = en_vocab.lookup_indices(en_tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        encoder_outputs, hidden = model.encoder(tensor)\n",
    "        inputs = zh_vocab.lookup_indices([sos_token])\n",
    "        attentions = torch.zeros(max_output_length, 1, len(ids))\n",
    "        for i in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, attention = model.decoder(\n",
    "                inputs_tensor, hidden, encoder_outputs\n",
    "            )\n",
    "            attentions[i] = attention\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            if predicted_token not in [0,1,2,3]:\n",
    "                inputs.append(predicted_token)\n",
    "            \n",
    "            if predicted_token == zh_vocab[eos_token]:\n",
    "                break\n",
    "        zh_tokens = zh_vocab.lookup_tokens(inputs[1:])\n",
    "    return ''.join(zh_tokens), en_tokens, attentions[: len(zh_tokens) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>', '<pad>', '<sos>', '<eos>', '的']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_vocab.lookup_tokens([0,1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(sentence, translation, attention):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    attention = attention.squeeze(1).numpy()\n",
    "    cax = ax.matshow(attention, cmap=\"bone\")\n",
    "    ax.set_xticks(ticks=np.arange(len(sentence)), labels=sentence, rotation=90, size=15)\n",
    "    translation = translation[1:]\n",
    "    ax.set_yticks(ticks=np.arange(len(translation)), labels=translation, size=15)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow challenge.',\n",
       " '几年前，在TED大会上， Peter Skillman 介绍了一个设计挑战 叫做“棉花糖挑战”')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = test_data[0]['translation'][\"en\"]\n",
    "expected_translation = test_data[0]['translation'][\"zh\"]\n",
    "\n",
    "sentence, expected_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这是， '"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation, sentence_tokens, attention = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_vocab,\n",
    "    zh_vocab,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")\n",
    "translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Several',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'here',\n",
       " 'at',\n",
       " 'TED',\n",
       " ',',\n",
       " 'Peter',\n",
       " 'Skillman',\n",
       " ' ',\n",
       " 'introduced',\n",
       " 'a',\n",
       " 'design',\n",
       " 'challenge',\n",
       " ' ',\n",
       " 'called',\n",
       " 'the',\n",
       " 'marshmallow',\n",
       " 'challenge',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\OneDrive\\Documents\\GitHub\\cs4248-neural-translation\\venvE3C\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 26159 (\\N{CJK UNIFIED IDEOGRAPH-662F}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\user\\OneDrive\\Documents\\GitHub\\cs4248-neural-translation\\venvE3C\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 65292 (\\N{FULLWIDTH COMMA}) missing from current font.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_attention(sentence_tokens, translation, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation, sentence_tokens, attention = translate_sentence(\n",
    "    sentence,\n",
    "    model,\n",
    "    en_vocab,\n",
    "    zh_vocab,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvE2C",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
